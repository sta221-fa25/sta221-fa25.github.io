[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 221: Regression Analysis",
    "section": "",
    "text": "Teaching team & office hours\n\n\n\n\nContact\nOffice hours\nLocation\n\n\n\n\nDr.¬†Alexander Fisher\naaf29@duke.edu\nTu/Fr: 2-3pm\nOld Chem 223B\n\n\nCathy Lee\npin.chian.lee@duke.edu\nTh: 1-3pm\nZoom (see Canvas for link)\n\n\nXukun Zhu\nxukun.zhu@duke.edu\nWe 4:30-6:30pm\nOld Chem 025\n\n\nKrish Bansal\nkrish.bansal@duke.edu\nMo: 1:30-2:30pm / We: 12-1pm\nOld Chem 203A\n\n\n\n\n\n\n\nMeetings\n\n\n\nLecture\nTu/Th 11:45am - 1:00pm\nOld Chemistry 116\n\n\nLab 01\nWe 1:25pm - 2:40pm\nPerkins LINK 087 (Classroom 3)\n\n\nLab 02\nWe 3:05pm - 4:20pm\nPerkins LINK 087 (Classroom 3)\n\n\n\nCourse website: sta221-fa25.github.io\n\n\nCourse description\nIn STA 221, students will learn how linear and logistic regression models are used to explore multivariable relationships, apply these methods to real data and learn the mathematical underpinnings of the models. Students will develop computing skills to implement a reproducible data analysis workflow and gain experience communicating statistical results. Throughout the semester, students will work on a team project where they will develop a research question, answer it using methods learned in the course, and share results through a written report and presentation.\nTopics include applications of linear and logistic regression, least squares estimation, maximum likelihood estimation, analysis of variance, model diagnostics, and model selection. Students will gain experience using the computing tools R and GitHub to analyze real-world data from a variety of fields.\n\n\nPrerequisites\nEither any STA 100-level course or STA 230, 231, or 240L and MATH 216, 218, or 221. The recommended co-requisite is STA 230, 231, or 240L.\n\n\n\n\n\n\n\n\nCourse material\nThere is no official textbook for the course; readings will be made available as they are assigned. We will use the statistical software package R both in-class, and on take-home assignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/. Additionally, students may access R and RStudio through Docker containers provided by Duke Office of Information Technology. Containers can be accessed at https://cmgr.oit.duke.edu/containers.\n\n\nCourse learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions.\n\n\n\nEvaluation\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope.\n\n\n\nA \\(&gt;= 93\\), A- \\(&lt; 93\\), B+ \\(&lt; 90\\), B \\(&lt; 87\\), B- \\(&lt; 83\\), C+ \\(&lt;80\\), C \\(&lt; 77\\), C- \\(&lt; 73\\), D+ \\(&lt; 70\\), D \\(&lt; 67\\), D- \\(&lt; 63\\), F \\(&lt; 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn pseudo-random class days, there will be a brief quiz on the previous lectures. If you score \\(&gt;60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\n\n\n\n\nPolicies\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\n\nLate work\nLate homework may be submitted within 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nOutside resources and generative AI statement\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class assignments. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well. Any code that is discovered to be recycled or created by AI and is not explicitly cited will be treated as plagiarism.\n\n\n\n\n\n\nWarning\n\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments.\n\n\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "project-data.html",
    "href": "project-data.html",
    "title": "Data resources",
    "section": "",
    "text": "Credit: list compiled by Prof.¬†Maria Tackett\n\nTidyTuesday\nFiveThirtyEight data\nData Is Plural\nR Data Sources for Regression Analysis\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "prepare/prepare-lec09.html",
    "href": "prepare/prepare-lec09.html",
    "title": "Prepare for Lecture 09: Inference for regression cont‚Äôd",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "prepare/prepare-lec05.html",
    "href": "prepare/prepare-lec05.html",
    "title": "Prepare: intro to inference",
    "section": "",
    "text": "Read through John Fox‚Äôs Regression Analysis notes posted to Canvas under ‚ÄúFiles‚Äù &gt; ‚Äúreadings‚Äù &gt; intro_inference."
  },
  {
    "objectID": "prepare/prepare-lec03.html",
    "href": "prepare/prepare-lec03.html",
    "title": "Prepare: model assessment",
    "section": "",
    "text": "üìñ Read Model assessment\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#logistics",
    "href": "notes/slides/lec01-welcome.html#logistics",
    "title": "Welcome to STA 221",
    "section": "Logistics",
    "text": "Logistics\nContact\n\nalexander.fisher@duke.edu\nOffice hours: Tu/Fr: 2:00-3:00p in Old Chem 223B\n\nCourse website\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-linear-regression",
    "href": "notes/slides/lec01-welcome.html#why-linear-regression",
    "title": "Welcome to STA 221",
    "section": "Why linear regression?",
    "text": "Why linear regression?\n\nGenetics\n\nMbatchou et al.¬†(2021)\nGenome wide association studies (GWAS)\nWhich single nucleotide polymorphisms in the genome are associated with a specific disease?\n\nAstrophysics\n\nFerrarese and Merritt (2000)\nIs black hole mass related to bulge velocity and/or luminosity of a galaxy?\n\nEcology\n\nEstes et al.¬†(1998)\nAt what rate is the population of sea otters changing?\n\nFinance\n\nRuf and Wang (2021)\nCan correlations between price and volatility help us hedge in options trading?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#learning-objectives",
    "href": "notes/slides/lec01-welcome.html#learning-objectives",
    "title": "Welcome to STA 221",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "href": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "title": "Welcome to STA 221",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Can the tables and figures be exactly reproduced from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\n\n\nOriginally reported ‚Äúthe intervention, compared with usual care, resulted in a fewer number of mean COPD-related hospitalizations and emergency department visits at 6 months per participant.‚Äù\nThere were actually more COPD-related hospitalizations and emergency department visits in the intervention group compared to the control group\nMixed up the intervention vs.¬†control group using ‚Äú0/1‚Äù coding\n\n\n\n\n\nhttps://jamanetwork.com/journals/jama/fullarticle/2752474"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#toolkit",
    "href": "notes/slides/lec01-welcome.html#toolkit",
    "title": "Welcome to STA 221",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#assessments",
    "href": "notes/slides/lec01-welcome.html#assessments",
    "title": "Welcome to STA 221",
    "section": "Assessments",
    "text": "Assessments\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#community",
    "href": "notes/slides/lec01-welcome.html#community",
    "title": "Welcome to STA 221",
    "section": "Community",
    "text": "Community\nUphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#team-work-policy",
    "href": "notes/slides/lec01-welcome.html#team-work-policy",
    "title": "Welcome to STA 221",
    "section": "Team work policy",
    "text": "Team work policy\nThe final project and several labs will be completed in teams. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "href": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "title": "Welcome to STA 221",
    "section": "Sharing / reusing code",
    "text": "Sharing / reusing code\n\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class quizzes and exams. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nNarrative (non-code solutions) should always be entirely your own.\n\n\n\n\n\n\n\nWarning\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#late-policy",
    "href": "notes/slides/lec01-welcome.html#late-policy",
    "title": "Welcome to STA 221",
    "section": "Late policy",
    "text": "Late policy\n\nHomeworks and labs can be turned in within 48 hours of the deadline for grade penalty (5% off per day).\nExams and the final project cannot be turned in late and can only be excused under exceptional circumstances.\nThe Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nLast minute coding/rendering issues will not be granted extensions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#course-toolkit",
    "href": "notes/slides/lec01-welcome.html#course-toolkit",
    "title": "Welcome to STA 221",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\n\nResource\nDescription\n\n\n\n\ncourse website\ncourse notes, deadlines, assignments, office hours, syllabus\n\n\nCanvas\nclass recordings, solutions, announcements, Ed Discussion\n\n\ncourse organization\nassignments, collaboration\n\n\nRStudio containers*\nonline coding platform\n\n\n\n*You are welcome to install R and RStudio locally on your computer. If working locally you should make sure that your environment meets the following requirements:\n\nlatest R version\nlatest RStudio\nworking git installation\nability to create ssh keys (for GitHub authentication)\nAll R packages updated to their latest version from CRAN"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "href": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "title": "Welcome to STA 221",
    "section": "Communication and missing class",
    "text": "Communication and missing class\nIf you have questions about homework/lab exercises, debugging, or any question about course materials\n\ncome to office hours\nask on Ed Discussion\n\n\n\n\n\n\n\n\nWarning\n\n\nThe teaching team will not debug via email.\n\n\n\n\n\nWhen you miss a class:\n\nwatch the recorded lecture on Canvas\ncome to office hours / post on Ed Discussion / ask a friend about missed content"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#exercise",
    "href": "notes/slides/lec01-welcome.html#exercise",
    "title": "Welcome to STA 221",
    "section": "Exercise",
    "text": "Exercise\n\nbikeshare = readr::read_csv(\"https://sta221-fa25.github.io/data/bikeshare-2012.csv\")\n\n\n\n\n\nAlexander, Rohan. 2023. ‚ÄúTelling Stories with Data,‚Äù June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. ‚ÄúOpinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.‚Äù Journal of Statistics and Data Science Education 30 (3): 241‚Äì50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "notes/lec09-normal.html",
    "href": "notes/lec09-normal.html",
    "title": "The normal assumption",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "notes/lec09-normal.html#background-the-normal-distribution",
    "href": "notes/lec09-normal.html#background-the-normal-distribution",
    "title": "The normal assumption",
    "section": "Background: the normal distribution",
    "text": "Background: the normal distribution\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\nWe read the above mathematical statement: ‚Äúthe random variable X is normally distributed with mean mu and variance sigma-squared.‚Äù\n\nExampleCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata.frame(x = -8:12) |&gt;\n  ggplot(aes(x = x)) + \n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2)) +\n  theme_bw() +\n  labs(title = \"X ~ N(3, 4)\", y = \"density\")\n\n\n\n\n\nSampling from a normal in R\nThe normal distribution, also known as ‚ÄúGaussian distribution‚Äù is a distribution of a continuous random variable. The sample space of a normal random variable is \\(\\{- \\infty, + \\infty \\}\\) and is defined by two parameters: a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). The mean is known as the location parameter while the standard deviation is the scale.\nWe can sample N times from a normal with mean mu and standard deviation sigma using rnorm(n = N, mean = mu, sd = sigma).\n\nrnorm(n = 1000, mean = 10, sd = 1) |&gt;\n  hist()\n\n\n\n\n\n\n\n\n\n\nProperties\nA very useful property of Normal distribution is that it is stable. What this means is that linear combinations of normal random variables are themselves normal. In other words, if \\(X\\) and \\(Y\\) are normal random variables then \\(aX + bY\\) is normal for all \\(a\\) and \\(b\\). The two properties to remember when adding Gaussian random variables are:\n\nmean(X + Y) = mean(X) + mean(Y)\nvariance(X + Y) = variance(X) + variance(Y) when X and Y are independent\n\nWe can see this in an example.\nLet \\(X \\sim N(5, 9)\\) and let \\(Y \\sim N(-5, 1)\\)\nAccording to our rules above \\(X + Y \\sim N(0, 10)\\). Let‚Äôs check ourselves with code:\n\nset.seed(1)\nnormal_df = data.frame(X = rnorm(1000, mean = 5, sd = 3),\n                 Y = rnorm(1000, mean = -5, sd = 1))\n\n\n\nnormal_df = normal_df %&gt;%\n  mutate(Z = X + Y)\n\nnormal_df %&gt;%\n  ggplot(aes(x = Z)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  theme_bw() +\n  labs(y = \"Density\", title = \"Matching densities\") +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(10)),\n                color = \"darkblue\")\n\n\n\n\n\n\n\n\n\nExercise\n\n\nLet \\(Z \\sim N(0, 1)\\) and \\(X \\sim N(10, 16)\\). \\(aZ + b \\stackrel{d}{=} X\\) for some \\(a\\) and \\(b\\). What are \\(a\\) and \\(b\\)? Fill them in for the ? in the code below and the uncomment the code. Here, \\(\\stackrel{d}{=}\\) means ‚Äúequal in distribution‚Äù.\n\nset.seed(221)\nnormal_df = data.frame(z = rnorm(1000, mean = 0, sd = 1),\n                 x = rnorm(1000, mean = 10, sd = 4))\n\nsample_mean = normal_df %&gt;%\n  summarize(sample_mean = mean(x)) %&gt;%\n  pull()\n\nnormal_df %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  geom_histogram(aes(x = z, y = ..density..), fill = 'steelblue', alpha = 0.7) +\n  #geom_density(aes(x = z*? + ?), color = 'red') + \n  theme_bw() +\n  labs(x = \"\", y = \"\", title = \"Sampling from two normals\")\n\n\n\n\nAnother useful property is that the marginal distribution of a multivariate normal itself normal.\nLet \\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n\\sim\nMVN\\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix},\n\\;\n\\begin{bmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{bmatrix}\n\\right).\n\\]\nThen the marginal distributions are \\[\nX \\sim N(\\mu_X, \\sigma_X^2),\n\\qquad\nY \\sim N(\\mu_Y, \\sigma_Y^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: if you‚Äôd like to simulate from a multivariate normal, you can use the function rmvnorm() from the mvtnorm package.\n\nmvtnorm::rmvnorm(n = 3, mean = c(0, 10), sigma = \n                   matrix(c(2, .5, .5, 1), ncol = 2))\n\n            [,1]      [,2]\n[1,] -1.64375527  7.934314\n[2,]  2.37437357 10.848478\n[3,]  0.06844319 10.669071"
  },
  {
    "objectID": "notes/lec09-normal.html#computing-probabilities",
    "href": "notes/lec09-normal.html#computing-probabilities",
    "title": "The normal assumption",
    "section": "Computing probabilities",
    "text": "Computing probabilities\npnorm ‚Äúprobability normal‚Äù takes three arguments:\n\nq, mean and sd\n\nand pnorm(q = q, mean = mu, sd = sigma) answers the question:\nIf \\(X \\sim N(\\mu, \\sigma)\\), what is \\(p(X &lt; q)\\)?\nFor example, imagine that the resting heart rates in the classroom are normally distributed with mean 70 beats per minute (bpm) and standard deviation 5 bpm. What‚Äôs the probability a randomly selected individual has a heart rate less than 63 bpm?\nIn math: let \\(X\\) be the bpm of an individual in this class. Assume \\(X \\sim N(70, 25)\\). What is \\(p(X &lt; 63)\\)? Given heartbeats are normally distributed, randomly selecting an individual from the classroom is called ‚Äúdrawing from a normal distribution‚Äù.\nWe can compute this easily:\n\npnorm(63, mean = 70, sd = 5)\n\n[1] 0.08075666\n\n\n0.08 or about 8% chance. In picture, the probability is the proportion of area under the curve shaded:"
  },
  {
    "objectID": "notes/lec09-normal.html#regression-assumption-of-normality",
    "href": "notes/lec09-normal.html#regression-assumption-of-normality",
    "title": "The normal assumption",
    "section": "Regression assumption of normality",
    "text": "Regression assumption of normality\nRecall the main question we had before: what assumptions make \\(\\hat{\\beta}_{OLS}\\) representative of \\(\\beta\\)?\n\nAssumption 1: \\(E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}\\)\nAssumption 2: \\(\\text{var}[\\boldsymbol{\\varepsilon}] = \\sigma^2 \\boldsymbol{I}\\)\nAssumption 3 (new today): \\(\\boldsymbol{\\varepsilon}\\sim\\) multivariate normal\n\nTaken all together, we can write assumptions 1, 2, and 3 concisely:\n\\[\n\\boldsymbol{\\varepsilon}\\sim MVN(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I})\n\\] or equivalently,\n\\[\n\\varepsilon_i  \\stackrel{\\text{i.i.d.}}{\\sim} \\; N(0, \\sigma^2) \\text{ for each i}\n\\]\nThe implications of this assumption are:\n\\[\n\\boldsymbol{y}\\sim MVN_n(\\boldsymbol{X}\\beta, \\sigma^2 \\boldsymbol{I})\n\\]\nor equivalently,\n\\[\ny_i \\sim N(x_i^T \\beta, \\sigma^2).\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\nMoreover,\n\\[\n\\hat{\\beta} \\sim MVN(\\beta, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})\n\\] which implies that the marginal distribution of a single element of the \\(\\hat{\\beta}\\),\n\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 C_{jj})\n\\] where the matrix \\(C = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\) and \\(C_{jj}\\) is the \\(j\\)th diagonal element.\nBased on the properties of a normal described above,\n\\[\n\\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2 C_{jj}}} \\sim N(0, 1)\n\\]\nWhere this is going: checking our assumptions are valid, constructing confidence intervals, and performing hypothesis tests."
  },
  {
    "objectID": "notes/lec09-normal.html#data-generative-model",
    "href": "notes/lec09-normal.html#data-generative-model",
    "title": "The normal assumption",
    "section": "Data generative model",
    "text": "Data generative model\nAn important view of the equation above,\n\\[\ny_i \\sim N(x_i^T \\beta, \\sigma^2),\n\\] is that it can be used to generate data. For this reason, the statistical model is often called a ‚Äúdata generative model‚Äù.\nIt looks like this:\n\ncreate length n vector \\(x\\). For example, x &lt;- runif(n, 0, 10)\nspecify the true \\(\\beta\\), e.g.¬†beta0 &lt;- 1; beta1 &lt;- 2\nchoose some error \\(\\sigma^2\\) and generate \\(y_i\\) according to the data generative model: rnorm(n, mean = (beta0 + beta1 * x, sd = 2)).\n\nFrom here you can fit the data with lm(y ~ x) and examine the results."
  },
  {
    "objectID": "notes/lec07-intro-inference.html",
    "href": "notes/lec07-intro-inference.html",
    "title": "Introduction to inference",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT)\nlibrary(latex2exp)\nlibrary(patchwork)\n\npokemon &lt;- read_csv(\"https://sta221-fa25.github.io/data/pokemon_data.csv\") # complete, population data"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#the-data",
    "href": "notes/lec07-intro-inference.html#the-data",
    "title": "Introduction to inference",
    "section": "The data",
    "text": "The data\nAs of 2025, there are 1025 pokemon in existence.\nIn all statistical inference tasks, we only have a sample from the population. Let‚Äôs consider a random sample of the pokemon data, given below:\n\nset.seed(48) \npokemon_sample &lt;- pokemon |&gt;\n  slice_sample(n = 15) |&gt;\n  select(dexnum, name, height_m, weight_kg) |&gt;\n  arrange(dexnum)\n  \n\ndatatable(pokemon_sample, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"sample of 15 pokemon\")\n\n\n\n\n\nLet‚Äôs investigate the question: are heavier pokemon taller?\n\nplotlinear modelplot code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(height_m ~ weight_kg, data = pokemon_sample) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.692    0.0859        8.06 2.07e- 6\n2 weight_kg    0.00603  0.000369     16.3  4.90e-10\n\n\n\n\n\npokemon_sample |&gt;\n  ggplot(aes(x = weight_kg, y = height_m)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, color = 'steelblue') +\n  theme_bw() +\n  labs(title = \"Pokemon weight vs height\", y = \"height (m)\", x = \"weight (kg)\")"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#question",
    "href": "notes/lec07-intro-inference.html#question",
    "title": "Introduction to inference",
    "section": "Question",
    "text": "Question\nHow can we tell if our estimates \\(\\hat{\\beta}\\) are any good?\n\nrepeated samplingplotspopulation-level annotation\n\n\n\npokemon_hw = pokemon |&gt;\n  select(height_m, weight_kg)\n\nBETA_HAT &lt;- NULL\nfor(i in 1:1000) {\n  fit &lt;- pokemon_hw |&gt;\n    slice_sample(n = 15) |&gt; \n    lm(height_m ~ weight_kg, data = _) \n  BETA_HAT &lt;- rbind(BETA_HAT, fit$coefficients)\n}\n\nBETA_HAT &lt;- data.frame(BETA_HAT)\ncolnames(BETA_HAT) &lt;- c(\"beta0\", \"beta1\")\n\nglimpse(BETA_HAT)\n\nRows: 1,000\nColumns: 2\n$ beta0 &lt;dbl&gt; 0.9016025, 0.5209848, 0.6775509, 0.8639395, 0.4678607, 0.4351285‚Ä¶\n$ beta1 &lt;dbl&gt; 0.004823824, 0.010159875, 0.008377374, 0.012071556, 0.016425420,‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npopn_model &lt;- lm(height_m ~ weight_kg, data = pokemon)\npopn_model$coefficients\n\n(Intercept)   weight_kg \n0.775612595 0.006509202"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#framework",
    "href": "notes/lec07-intro-inference.html#framework",
    "title": "Introduction to inference",
    "section": "Framework",
    "text": "Framework\nOur objective is to infer properties about a population using data from an experiment or survey (in this case, a survey/sample of pokemon).\nPost-experiment: after collecting the data, \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are fixed and known.\nPre-experiment: before collecting the data, the data are unknown and random. \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\), which are functions of the data, are also unknown and random.\nIn all cases, the true population parameters, \\(\\beta_0, \\beta_1\\) are fixed but unknown.\nPre-experimental question: is the probability distribution of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) a meaningful representation of the population?\nAnswer: this depends on certain assumptions we make about population.\n\n\n\n\n\n\nAssumption 1\n\n\n\n\\(E[\\boldsymbol{\\varepsilon}|\\boldsymbol{x}] = \\boldsymbol{0}\\), or equivalently, \\(E[\\varepsilon_i|\\boldsymbol{x}] = 0\\) for all \\(i\\).\n\n\nImplications: in simple linear regression, assumption 1 implies that \\(E[y_i|\\boldsymbol{x}] = E[\\beta_0 + \\beta_1 x_i + \\epsilon_i|\\boldsymbol{x}] = E[\\beta_0 |\\boldsymbol{x}] + E[\\beta_1 x_i|\\boldsymbol{x}] + E[\\epsilon_i|\\boldsymbol{x}] = \\beta_0 + \\beta_1 x_i\\).\n\nExerciseSolution\n\n\nShow that assumption 1 implies that \\(E[\\hat{\\beta}|\\boldsymbol{X}] = \\beta\\).\n\n\n\\[\n\\begin{aligned}\nE[\\hat{\\beta}|\\boldsymbol{X}] &= E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E[\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{X}\\beta\\\\\n&= \\beta\n\\end{aligned}\n\\]\n\n\n\nSince \\(E[\\hat{\\beta}|\\boldsymbol{X}] = \\beta\\), we say \\(\\hat{\\beta}\\) is an unbiased estimator of \\(\\beta\\).\nNotice that this result (unbiasedness) does not depend independence of errors, normality, or constant variance.\nHowever, the result doesn‚Äôt tell us anything about how close \\(\\hat{\\beta}\\) will be to \\(\\beta\\). The estimator may be unbiased, but by itself, this doesn‚Äôt tell us much. See examples of ‚Äúunbiased‚Äù distributions of an estimator below."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html",
    "href": "notes/lec05-matrix-form-regression.html",
    "title": "Matrix algebra of regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "href": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "title": "Matrix algebra of regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nwrite down simple linear regression in matrix form\nunderstand the geometry of OLS regression\nbe able to explain the following vocabulary: design matrix, Hessian matrix, projection"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "href": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "title": "Matrix algebra of regression",
    "section": "Simple linear regression (p = 2)",
    "text": "Simple linear regression (p = 2)\nWe write down simple linear regression\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon},\n\\]\nwhere\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\\(\\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\nand \\(p = 2\\), i.e.¬†there‚Äôs an intercept and 1 slope. More explicitly:\n\\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\varepsilon},\n\\]\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n\nDefinition: design matrix\n\n\n\n\\(\\boldsymbol{X}\\) is called the ‚Äúdesign matrix‚Äù, ‚Äúcovariate matrix‚Äù, ‚Äúmodel matrix‚Äù or even sometimes the ‚Äúdata matrix‚Äù. It includes columns each predictors and an intercept (if applicable)."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "href": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "title": "Matrix algebra of regression",
    "section": "Linear algebra background",
    "text": "Linear algebra background\n\nIdentity Matrix\nThe identity matrix of size \\(n \\times n\\), denoted \\(\\mathbf{I}_n\\), is a square matrix with ones on the diagonal and zeros elsewhere:\n\\[\n\\mathbf{I}_n =\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}.\n\\]\nFor any vector \\(\\mathbf{v}\\),\n\\[\n\\mathbf{I}_n \\mathbf{v} = \\mathbf{v}.\n\\]\n\n\n\nMatrices as Linear Operators\nAn \\(m \\times n\\) matrix \\(\\mathbf{A}\\) represents a linear operator that maps vectors to vectors:\n\\[\n\\mathbf{A} : \\mathbb{R}^n \\to \\mathbb{R}^m, \\quad \\mathbf{v} \\mapsto \\mathbf{A}\\mathbf{v}.\n\\]\nMatrix multiplication distributes over addition:\n\\[\n\\mathbf{A}(\\mathbf{u} + \\mathbf{v}) = \\mathbf{A}\\mathbf{u} + \\mathbf{A}\\mathbf{v},\n\\]\nand scalar multiplication:\n\\[\n\\mathbf{A}(c \\mathbf{v}) = c (\\mathbf{A}\\mathbf{v}).\n\\]\n\n\n\nMatrix Inverses\nA square matrix \\(\\mathbf{A}\\) of size \\(n \\times n\\) has an inverse \\(\\mathbf{A}^{-1}\\) if\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n.\n\\]\nOnly full-rank matrices (rank \\(n\\)) are invertible.\nIf \\(\\det(\\mathbf{A}) = 0\\), then \\(\\mathbf{A}\\) is singular and has no inverse.\nTo invert a matrix in R, use solve(), for example:\n\n(A = matrix(c(1,0,3,4), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    0    4\n\n\n\nsolve(A)\n\n     [,1]  [,2]\n[1,]    1 -0.75\n[2,]    0  0.25\n\n\nCheck:\n\nsolve(A) %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\n\n\nSymmetric Matrices\nA square matrix \\(\\mathbf{A}\\) is symmetric if\n\\[\n\\mathbf{A} = \\mathbf{A}^T.\n\\]\nThat is, \\(\\mathbf{A}_{ij} = \\mathbf{A}_{ji}\\) for all \\(i,j\\).\nTo take a transpose in R, use t(A), for example:\n\n(A = matrix(c(1,2,3,4), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nt(A)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\n\n\n\nPositive Definite and Positive Semi-Definite Matrices\nA symmetric matrix \\(\\mathbf{A}\\) is\n\nPositive Definite (PD) if\n\\[\n\\mathbf{x}^T \\mathbf{A} \\mathbf{x} &gt; 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}.\n\\]\nPositive Semi-Definite (PSD) if\n\\[\n\\mathbf{x}^T \\mathbf{A} \\mathbf{x} \\geq 0 \\quad \\text{for all } \\mathbf{x}.\n\\]\n\nEquivalently, all eigenvalues of \\(\\mathbf{A}\\) are positive (the matrix is PD) or nonnegative (the matrix is PSD).\n\n\n\nVector Orthogonal to a Matrix\nA vector \\(\\mathbf{v} \\in \\mathbb{R}^n\\) is said to be orthogonal to a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times p}\\) if it is orthogonal to every column of \\(\\mathbf{A}\\).\nThat is,\n\\[\n\\mathbf{v}^T \\mathbf{a}_j = 0 \\quad \\text{for each column } \\mathbf{a}_j \\text{ of } \\mathbf{A}.\n\\]\nEquivalently,\n\\[\n\\mathbf{A}^T \\mathbf{v} = \\mathbf{0}.\n\\]\nIn this case, \\(\\mathbf{v}\\) lies in the orthogonal complement of the column space of \\(\\mathbf{A}\\), often written as\n\\[\n\\mathbf{v} \\in \\text{Col}(\\mathbf{A})^\\perp.\n\\]"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "href": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "title": "Matrix algebra of regression",
    "section": "Matrix calculus",
    "text": "Matrix calculus\nLet \\(\\mathbf{x} \\in \\mathbb{R}^n\\) be a column vector and \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) a matrix.\nGradients with respect to vectors are written as\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n\\]\n\n\n1. Derivative of an Inner Product\nConsider the scalar function\n\\[\nf(\\mathbf{x}) = \\mathbf{a}^T \\mathbf{x},\n\\]\nwhere \\(\\mathbf{a} \\in \\mathbb{R}^n\\).\n\nDerivative with respect to \\(\\mathbf{x}\\):\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}.\n\\]\nEquivalently,\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{a}) = \\mathbf{a}.\n\\]\n\n\n\n2. Derivative of a Quadratic Form\nConsider the quadratic form\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}.\n\\]\n\nIf \\(\\mathbf{A}\\) is not assumed symmetric:\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}.\n\\]\n\nIf \\(\\mathbf{A}\\) is symmetric (\\(\\mathbf{A} = \\mathbf{A}^T\\)):\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2 \\mathbf{A} \\mathbf{x}.\n\\]\nFor all else, see the matrix cookbook."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "href": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "title": "Matrix algebra of regression",
    "section": "OLS in Matrix form",
    "text": "OLS in Matrix form\nThe ordinary least squares (OLS) estimator is defined as\n\\[\n\\hat{\\beta}_{\\text{OLS}}\n= \\arg \\min_{\\beta} \\; \\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon},\n\\]\nwhere\n\\[\n\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\beta.\n\\]\nNotice this is equivalent to the sum of squares we saw before:\n\\[\n\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon} =\n\\begin{bmatrix}\n\\varepsilon_1 & \\varepsilon_2 & \\cdots & \\varepsilon_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n= \\sum_{i=1}^n \\varepsilon_i^2.\n\\]\nTo optimize, we need to take the derivative with respect to the vector \\(\\beta\\) and set it equal to zero. To begin, we differentiate,\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial{\\beta}}\n\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n(\\boldsymbol{y}- \\boldsymbol{X}\\beta)^T(\\boldsymbol{y}- \\boldsymbol{X}\\beta)\\\\\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n\\left(\n\\boldsymbol{y}^T \\boldsymbol{y}- \\boldsymbol{y}^T \\boldsymbol{X}\\beta - \\beta^T \\boldsymbol{X}^T \\boldsymbol{y}+ \\beta^T \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n\\left(\n\\boldsymbol{y}^T \\boldsymbol{y}\n-2 \\beta^T \\boldsymbol{X}^T \\boldsymbol{y}+ \\beta^T \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&=\n-2 \\boldsymbol{X}^T \\boldsymbol{y}+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\beta.\n\\end{aligned}\n\\]\nWhen we set the derivative equal to zero and solve,\n\\[\n\\begin{aligned}\n-2 \\boldsymbol{X}^T \\boldsymbol{y}+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\hat{\\beta} &= 0 \\implies\\\\\n\\hat{\\beta} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\end{aligned}\n\\]\n\nDid we find a minimum?\nQuestion: this may be some optima, but did we find the value that minimizes the sum of squared residuals?\nAnswer: check the Hessian matrix. The Hessian matrix is the square matrix of partial second derivatives.\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial{\\beta}^2} \\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}\n&=\n\\frac{\\partial}{\\partial{\\beta}} \\left(\n-2 \\boldsymbol{X}^T \\boldsymbol{y}\n+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&= 2 \\boldsymbol{X}^T \\boldsymbol{X}\n\\end{aligned}\n\\]\n\nIf the Hessian is positive definite, we have a minimum.\nIf the Hessian is negative definite, we have a maximum.\nIf the Hessian is neither, then we have a saddle point.\n\nExercise on homework 1: show that the Hessian matrix \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) is positive definite."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#geometry",
    "href": "notes/lec05-matrix-form-regression.html#geometry",
    "title": "Matrix algebra of regression",
    "section": "Geometry",
    "text": "Geometry\nSee handwritten notes."
  },
  {
    "objectID": "notes/lec03-slr.html",
    "href": "notes/lec03-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec03-slr.html#notation",
    "href": "notes/lec03-slr.html#notation",
    "title": "Simple linear regression",
    "section": "Notation",
    "text": "Notation\n\\(\\boldsymbol{y}\\): a vector of observations\n\\(\\boldsymbol{y}= [y_1, y_2, \\ldots, y_n]^T = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\\). We say ‚Äúy is of dimension n‚Äù.\n\\(\\hat{\\boldsymbol{y}}\\): vector of ‚Äúfitted‚Äù outcomes or ‚Äúpredicted‚Äù response variable.\n\\(\\boldsymbol{1}= \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{bmatrix}\\). \\(\\boldsymbol{1}\\) is of dimension of n.\n\n\n\n\n\n\nNote\n\n\n\nFor the handwritten in-class notes, we use the convention that a line underneath the symbol represents a vector."
  },
  {
    "objectID": "notes/lec03-slr.html#last-time",
    "href": "notes/lec03-slr.html#last-time",
    "title": "Simple linear regression",
    "section": "Last time",
    "text": "Last time\n\\(|cor(\\boldsymbol{x},\\boldsymbol{y})| = 1 \\iff \\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}\\) for some \\(\\beta_0\\), \\(\\beta_1\\). See Figure 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is more typical that \\(|cor(\\boldsymbol{x}, \\boldsymbol{y})| &lt; 1\\), in which case \\(\\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}+ \\boldsymbol{\\varepsilon}\\), where \\(\\boldsymbol{\\varepsilon}\\) is the error vector. See Figure 2 for an example. The observation by observation representation is given by the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i.\\]\n\n\n\n\n\n\n\n\n\nQuestion: what line, (i.e.¬†what (\\(\\beta_0, \\beta_1\\))) provides the ‚Äúbest fit‚Äù?\nAnswer: the set (\\(\\beta_0, \\beta_1\\)) that satisfy an objective function.\n\n\n\n\n\n\nDefinition: objective function\n\n\n\nAn objective function is some function we want to optimize.\nExample 1: least absolute value (LAV) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n |\\varepsilon_i|\n\\]\nExample 2: Ordinary least squares (OLS) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]"
  },
  {
    "objectID": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "href": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "title": "Simple linear regression",
    "section": "Ordinary least squares (OLS) regression line",
    "text": "Ordinary least squares (OLS) regression line\n\\(\\sum_{i=1}^n \\varepsilon_i^2\\) is called the ‚Äúresidual sum of squares‚Äù or RSS for short.\n\\[\n\\begin{aligned}\nRSS(\\beta_0, \\beta_1) &= \\sum_{i=1}^n \\varepsilon_i^2\\\\\n&= \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2\\\\\n&= (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))^T (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))\\\\\n&= ||(\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))||^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nDefinition: OLS estimates\n\n\n\nThe OLS values of \\(\\beta_0\\), \\(\\beta_1\\) are the values \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) that minimize \\(RSS(\\beta_0, \\beta_1)\\). Again, in math,\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]\n\n\nQuestion: How can we find the OLS line?\nAnswer: (1) Geometry (we‚Äôll do this later); (2) calculus\n\nComputing the OLS estimates using calculus\n\n\n\n\n\n\n\nlm(tmax ~ tmin, data = weather)\n\n\nCall:\nlm(formula = tmax ~ tmin, data = weather)\n\nCoefficients:\n(Intercept)         tmin  \n    61.1017       0.3729  \n\n\n\nNotice that the RSS is a quadratic (convex) function of \\(\\beta_0, \\beta_1\\).\nThe global minimum occurs where the derivative (gradient) equals zero.\n\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_0} RSS &=  -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i)) = 0\\\\\n\\frac{\\partial}{\\partial \\beta_1} RSS &=  -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i)) = 0\n\\end{aligned}\n\\]\nTherefore the OLS values \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) will satisfy the normal equations,\n\\[\n\\begin{align}\n\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{1}\\\\\n\\sum_{i=1}^n x_i \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{2}\n\\end{align}\n\\] Question: why are these called the normal equations?\nAnswer: Let \\(\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\) then,\n\\[\n\\begin{aligned}\n(1) &\\implies \\sum \\hat{\\varepsilon}_i  = \\hat{\\boldsymbol{\\varepsilon}}\\cdot \\boldsymbol{1}= 0\\\\\n(2) &\\implies \\sum x_i \\hat{\\varepsilon}_i = \\boldsymbol{x}^T \\hat{\\boldsymbol{\\varepsilon}}= 0.\n\\end{aligned}\n\\]\nIn words, the residual vector \\(\\hat{\\boldsymbol{\\varepsilon}}\\) is normal (orthogonal) to the vectors \\(\\boldsymbol{1}\\) and \\(\\boldsymbol{x}\\).\n\nExerciseSolution\n\n\nShow that the OLS regression line goes through \\(\\bar{x}, \\bar{y}\\). Reminder: \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n} \\sum y_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n(1) \\implies &\\sum y_i - \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i) = 0\\\\\n&\\sum y_i = \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\\\\n& n\\bar{y} = n \\hat{\\beta_0} + n \\hat{\\beta_1} \\bar{x}\\\\\n&\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x}.\n\\end{aligned}\n\\] Note the commonly used ‚Äútrick‚Äù:\n\\(\\sum y_i = n \\bar{y}\\).\n\n\n\nSolving the normal equations\n\\[\n\\begin{aligned}\n\\text{From (1): } &\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) = 0\\\\\n& \\bar{y}  = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\\\\\n&\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\] Plugging this result into (2):\n\\[\n\\begin{aligned}\n&\\sum x_i (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)) = 0\\\\\n&\\sum x_i (y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i) = 0\\\\\n&\\sum x_i (y_i - \\bar{y}) = \\hat{\\beta}_1 \\sum x_i (x_i - \\bar{x}) \\text{    }& (*)\n\\end{aligned}\n\\]\nNotice the trick:\n\\[\n\\begin{aligned}\n\\sum (x_i - \\bar{x}) (y_i - \\bar{y}) &= \\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x}(y_i - \\bar{y})\\\\\n&= \\sum x_i (y_i - \\bar{y})  - \\bar{x} (0)\\\\\n&= \\sum x_i (y_i - \\bar{y}).\n\\end{aligned}\n\\] Similarly,\n\\[\n\\begin{aligned}\n\\sum x_i (x_i - \\bar{x}) &= \\sum (x_i - \\bar{x})(x_i - \\bar{x})\\\\\n&= \\sum (x_i - \\bar{x})^2.\n\\end{aligned}\n\\]\nLet\n\\[\n\\begin{aligned}\nS_{xx} &= \\sum (x_i - \\bar{x})^2\\\\\nS_{yy} &= \\sum (y_i - \\bar{y})^2\\\\\nS_{xy} &= \\sum (x_i - \\bar{x}) (y_i - \\bar{y})\n\\end{aligned}\n\\] Then \\((*)\\) above says \\(S_{xy} = \\hat{\\beta}_1 S_{xx}\\), implying that the OLS values are\n\\[\n\\begin{aligned}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\\\\n\\hat{\\beta}_1 &= \\frac{S_{xy}}{S_{xx}}.\n\\end{aligned}\n\\]\nAn important take-away: the slope is closely related to the correlation. Notice\n\\[\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\nThe numerator looks like covariance, or correlation between \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). The denominator looks like \\(var(\\boldsymbol{x})\\). If we multiply by the number ‚Äú1‚Äù in a fancy way, the relationship becomes clear,\n\\[\n\\begin{aligned}\n\\frac{S_{yy}^{1/2}}{S_{yy}^{1/2}} \\cdot \\frac{S_{xy}}{S_{xx}^{1/2} S_{xx}^{1/2}}\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot \\frac{S_{xy}}{\\left(S_{xx} S_{yy}\\right)^{1/2}}\\\\\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot cor(\\boldsymbol{x},\\boldsymbol{y})\n\\end{aligned}\n\\]\nSummary: what do you need to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)?\n\n\\(\\bar{x}\\), \\(\\bar{y}\\)\n\\(S_{xx}, S_{yy}\\)\n\\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\)\n\nWe can write the fitted regression line in terms of these quantities:\n\\[\n\\begin{aligned}\n\\hat{y}_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y} -\\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y}  +\\hat{\\beta}_1 (x_i - \\bar{x})\\\\\n&= \\bar{y} + \\left(S_{yy}/S_{xx} \\right)^{1/2} \\cdot cor(\\boldsymbol{x}, \\boldsymbol{y}) \\cdot (x_i - \\bar{x})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/construction/anova.html",
    "href": "notes/construction/anova.html",
    "title": "Analysis of variance (ANOVA1)",
    "section": "",
    "text": "Show libraries used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(broom)"
  },
  {
    "objectID": "notes/construction/anova.html#definition",
    "href": "notes/construction/anova.html#definition",
    "title": "Analysis of variance (ANOVA1)",
    "section": "Definition",
    "text": "Definition\nANOVA refers to (1) procedures for fitting and testing linear models in which the independent variables are categorical and (2) partitioning the dependent-variable sum of squares into ‚Äúexplained‚Äù and ‚Äúunexplainbed‚Äù components."
  },
  {
    "objectID": "notes/construction/anova.html#one-way-anova-example",
    "href": "notes/construction/anova.html#one-way-anova-example",
    "title": "Analysis of variance (ANOVA1)",
    "section": "One-way ANOVA example",
    "text": "One-way ANOVA example\nOne-way ANOVA means that we have exactly 1 categorical dependent variable. As an example, consider the penguin data set. We‚Äôll take our outcome variable, \\(y\\), to be the body mass of the penguin in grams body_mass_g and the dependent variable \\(x\\) to be categorical species of penguin species. Notice there are 3 species of penguins in this data set: Adelie, Chinstrap and Gentoo.\n\npenguin_subset = penguins %&gt;%\n  select(body_mass_g, species) %&gt;%\n  drop_na()\n\npenguin_subset %&gt;%\n  count(species)\n\n# A tibble: 3 √ó 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\nglimpse(penguin_subset)\n\nRows: 342\nColumns: 2\n$ body_mass_g &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 3300‚Ä¶\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad‚Ä¶\n\n\nNext let‚Äôs fit the linear model and report the least squares estimates for each parameter:\n\nmodel = lm(body_mass_g ~ species, data = penguin_subset)\nmodel %&gt;%\n  tidy()\n\n# A tibble: 3 √ó 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        3701.       37.6    98.4   2.49e-251\n2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1\n3 speciesGentoo      1375.       56.1    24.5   5.42e- 77\n\n\nANOVA:\n\nanova(model) %&gt;%\n  tidy()\n\n# A tibble: 2 √ó 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.      344.  2.89e-82\n2 Residuals   339  72443483.   213698.       NA  NA       \n\n\nManual calculation:\n\n# penguin_subset %&gt;%\n#   group_by(species) %&gt;%\n#   mutate(ybar = mean(body_mass_g))\n\ny &lt;- penguin_subset$body_mass_g\n\nanova_df = penguin_subset %&gt;%\n  group_by(species) %&gt;%\n  summarize(ybar = mean(body_mass_g), n = n()) %&gt;%\n  mutate(y_total_bar = mean(y)) \n\nanova_df\n\n# A tibble: 3 √ó 4\n  species    ybar     n y_total_bar\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 Adelie    3701.   151       4202.\n2 Chinstrap 3733.    68       4202.\n3 Gentoo    5076.   123       4202.\n\nanova_df %&gt;%\n  summarize(sum(n * (ybar - y_total_bar)^2)) # sum sq x\n\n# A tibble: 1 √ó 1\n  `sum(n * (ybar - y_total_bar)^2)`\n                              &lt;dbl&gt;\n1                        146864214.\n\n# sum sq resid: \n\nleft_join(penguin_subset, anova_df, by = \"species\") %&gt;%\n  summarize(rss = sum(((body_mass_g - ybar)^2)))\n\n# A tibble: 1 √ó 1\n        rss\n      &lt;dbl&gt;\n1 72443483.\n\npf(343.6263, 2, 339, log.p = TRUE)\n\n[1] -2.892344e-82\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#introductions",
    "href": "labs/slides/lab0-slides.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta221-fa25.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "href": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nIntroduce lab assignment (5-10 minutes, longer today)\nWork on the lab assignment (you can find it on the course website). You will work with others but your submission must be your own for the first few labs.\nTypically you won‚Äôt finish labs in-class and they will be due 1 week after they are released."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#tips",
    "href": "labs/slides/lab0-slides.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nRead all instructions on the lab.\nOne work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when we can help you on the spot and leave the narrative writing until later.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "labs/slides/lab0-slides.html#beginnings",
    "href": "labs/slides/lab0-slides.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\n\nFind the lab instructions here\nFollow the instructions in the lab as I demo."
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "Lab 02: Linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due Tuesday, September 16 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab02.html#learning-goals",
    "href": "labs/lab02.html#learning-goals",
    "title": "Lab 02: Linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nContinue developing a reproducible workflow using RStudio and GitHub\nProduce visualizations and summary statistics to describe distributions\nFit, interpret, and evaluate linear regression models\nUse the matrix representation of the linear regression model to estimate coefficients\nExplore properties of the linear regression model"
  },
  {
    "objectID": "labs/lab02.html#exercise-1",
    "href": "labs/lab02.html#exercise-1",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with univariate exploratory data analysis.\n\nVisualize the distribution of the response variable total_cup_points and calculate summary statistics.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\nBased on this distribution, do you think the data set is representative of all coffee available to consumers? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure your data visualizations have clear and informative titles and axis labels."
  },
  {
    "objectID": "labs/lab02.html#exercise-2",
    "href": "labs/lab02.html#exercise-2",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let‚Äôs consider the relationship between how good a coffee smells and its overall quality.\n\nVisualize the relationship between aroma and total_cup_points.\nDoes there appear to be a relationship between a coffee‚Äôs aroma and its overall quality? If so, what is the shape and direction of the relationship?"
  },
  {
    "objectID": "labs/lab02.html#exercise-3",
    "href": "labs/lab02.html#exercise-3",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have seen the mathematical formulation for simple linear regression in class. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nfor some unknown regression coefficients \\((\\beta_0, \\beta_1)\\) and error terms \\(\\epsilon\\) that are centered at 0 and have variance \\(\\sigma^2_{\\epsilon}\\) . This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about regression, not be specific to the coffee data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat are the interpretations of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-4",
    "href": "labs/lab02.html#exercise-4",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nFit the model of the relationship between aroma and total_cup_points. Neatly display the output using 3 digits.\nInterpret the slope in the context of the data.\nWhat is the expected total_cup_points for coffees that receive the worst aroma score of 0? Is this a reliable estimate of the total_cup_points for these coffees? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-5",
    "href": "labs/lab02.html#exercise-5",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow let‚Äôs add flavor to the model, so we will use both flavor and aroma to understand variability in the overall quality of coffees. Use this model for the remainder of the lab.\nIn class we have seen how vectors and matrices can be used to represent the linear regression model:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}, \\boldsymbol{\\epsilon}\\) for this model. Your answer should have exact values given the coffee data set.\nCompute the estimated regression coefficients using the matrix form of the model. Show the code used to get the answer.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the model.matrix() function to get the design matrix. The code takes the general form:\n\nmodel.matrix(y ~ x, data = my_data)\n\nSee Lab 01 for other matrix operations in R.\n\n\n\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits.\nWrite the estimated regression equation.\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-6",
    "href": "labs/lab02.html#exercise-6",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nThe coefficient for aroma for the model fit in Exercise 5 is different than the coefficient from the model fit in Exercise 4. Briefly explain why these coefficients are different.\nWould you willingly drink a coffee represented by the intercept of the model in Exercise 5? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-7",
    "href": "labs/lab02.html#exercise-7",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCompute \\(\\mathbf{H}\\), the hat matrix corresponding to the model from Exercise 5. Then use \\(\\mathbf{H}\\) to compute the residuals for this model. Do not print out \\(\\mathbf{H}\\) or the residuals.\nCompute the mean and standard deviation of the residuals.\nRecall root mean square error RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}}\n\\]\nSimilar to other statistics we‚Äôve seen thus far, we can write the RMSE in matrix form. Compute RMSE for the model from Exercise 5 using matrix form. Show the code used to get the answer.\nHow do the standard deviation of the residuals and RMSE compare?\n\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Lab 02!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab00.html",
    "href": "labs/lab00.html",
    "title": "Lab 0: Getting Started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today‚Äôs lab tasks before leaving lab today."
  },
  {
    "objectID": "labs/lab00.html#rstudio",
    "href": "labs/lab00.html#rstudio",
    "title": "Lab 0: Getting Started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient graphical user interface (GUI).\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick ‚ÄúReserve STA 221‚Äù to reserve an RStudio container. Be sure you reserve the container labeled STA 221 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA221 to log into the Docker container. You should now see the RStudio environment."
  },
  {
    "objectID": "labs/lab00.html#git-and-github",
    "href": "labs/lab00.html#git-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username. tl;dr choose something that you would be proud to show a future employer.\n\n\n\nIf you already have a GitHub account, you can move on to the next step."
  },
  {
    "objectID": "labs/lab00.html#connect-rstudio-and-github",
    "href": "labs/lab00.html#connect-rstudio-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. An outline of the authentication steps is below; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Open your STA 221 RStudio container.\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta221)\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"Alexander Fisher\",\n  user.email = \"alexander.fisher@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio!"
  },
  {
    "objectID": "labs/lab00.html#getting-started",
    "href": "labs/lab00.html#getting-started",
    "title": "Lab 0: Getting Started",
    "section": "Getting started",
    "text": "Getting started\n\nClick here to create your individual lab-00 repo: https://classroom.github.com/a/PMZfNgLX\nClick to open your lab-00 repo.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ‚Üí New Project ‚Üí Version Control ‚Üí Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-00.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab00.html#update-the-quarto-document",
    "href": "labs/lab00.html#update-the-quarto-document",
    "title": "Lab 0: Getting Started",
    "section": "Update the Quarto document",
    "text": "Update the Quarto document\n\nTask 1: Change the author name at the top of the document to your name. Render the document. You will see your name at the top of the rendered PDF.\nTask 2: The plot shows the relationship between the daily temperature and number of bike rentals in Washington, D.C.‚Äôs Capital Bikeshare in 2012.\n\n\n\n\n\n\n\n\n\n\nWrite 1 - 2 observations from the plot. Render the document. You will see your response in the rendered PDF."
  },
  {
    "objectID": "labs/lab00.html#commit-and-push-changes-to-github",
    "href": "labs/lab00.html#commit-and-push-changes-to-github",
    "title": "Lab 0: Getting Started",
    "section": "Commit and push changes to GitHub",
    "text": "Commit and push changes to GitHub\n\nOnce you have made your final updates, go to the Git pane in your RStudio instance. This is a tab in the top right corner of the RStudio window.\nCheck the appropriate boxes on every file in the Git pane. All checked files will be sent to GitHub.\nNext, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box.\nClick Commit. Note that every commit needs to have a commit message associated with it.\nNow that you have made an update and committed this change, click Push to send the changes to GitHub.\nGo to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis: Theory and Applications",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nNotes\nAssignment\nProject\n\n\n\n\n1\nTue Aug 26\nwelcome\n\nüíª\n\n\n\n\n\nWed Aug 27\nlab: hello git and R\n\nüíª\nlab 0\n\n\n\n\nThu Aug 28\ncorrelation\nüìñ\nüóí\n\n\n\n\n2\nTue Sep 02\nsimple linear regression\n\nüóí\n\n\n\n\n\nWed Sep 03\nlab: computing and linear algebra review\n\n\nlab 1\n\n\n\n\nThu Sep 04\nmodel assessment\nüìñ\nüóí\n\n\n\n\n3\nTue Sep 09\nmatrix representation\nüìñ\nüóí üìù\nhw 1\n\n\n\n\nWed Sep 10\nlab: linear regression\n\n\nlab 2\n\n\n\n\nThu Sep 11\nmultiple linear regression\n\nüóí üìù\n\n\n\n\n4\nTue Sep 16\nintro to inference\nüìñ\nüóí\n\n\n\n\n\nWed Sep 17\nlab: multiple linear regression\n\n\nlab 3\n\n\n\n\nThu Sep 18\ninference continued\n\nüóí\nhw 2\n\n\n\n5\nTue Sep 23\nthe normal assumption\n\nüóí\n\n\n\n\n\nWed Sep 24\nlab: project workday\n\n\nproject 1\n\n\n\n\nThu Sep 25\nhypothesis tests\n\nüóí\n\n\n\n\n6\nTue Sep 30\nconfidence intervals\n\nüóíüìù\n\nresearch topics due\n\n\n\nWed Oct 01\nlab: interaction effects and exam practice\n\nüíª\nlab 4\n\n\n\n\nThu Oct 02\nexam review\n\n\n\n\n\n\n7\nTue Oct 07\nexam 1\n\n\n\n\n\n\n\nWed Oct 08\nNO LAB (exam)\n\n\n\n\n\n\n\nThu Oct 09\n\n\n\n\n\n\n\n8\nTue Oct 14\nNO CLASS\n\n\n\n\n\n\n\nWed Oct 15\n\n\n\n\nproject proposal due\n\n\n\nThu Oct 16\n\n\n\n\n\n\n\n9\nTue Oct 21\n\n\n\n\n\n\n\n\nWed Oct 22\n\n\n\n\n\n\n\n\nThu Oct 23\n\n\n\n\n\n\n\n10\nTue Oct 28\n\n\n\n\n\n\n\n\nWed Oct 29\n\n\n\n\n\n\n\n\nThu Oct 30\n\n\n\n\n\n\n\n11\nTue Nov 04\n\n\n\n\nEDA due\n\n\n\nWed Nov 05\n\n\n\n\n\n\n\n\nThu Nov 06\n\n\n\n\n\n\n\n12\nTue Nov 11\n\n\n\n\n\n\n\n\nWed Nov 12\nlab: project presentations\n\n\n\npresentation\n\n\n\nThu Nov 13\n\n\n\n\n\n\n\n13\nTue Nov 18\n\n\n\n\ndraft report due\n\n\n\nWed Nov 19\nlab: peer review\n\n\n\npeer review\n\n\n\nThu Nov 20\n\n\n\n\n\n\n\n14\nTue Nov 25\n\n\n\n\n\n\n\n\nWed Nov 26\nNO CLASS\n\n\n\n\n\n\n\nThu Nov 27\nNO CLASS\n\n\n\n\n\n\n15\nTue Dec 02\nexam review\n\n\n\n\n\n\n\nWed Dec 03\n\n\n\n\n\n\n\n\nThu Dec 04\nexam 2\n\n\n\n\n\n\nFinal day\nFri Dec 12\nproject due\n\n\n\nfinal report & repo due"
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.\n\n\nWe use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]\n\n\n\n\n\nExercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?\n\n\n\nLet\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the affine transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?\n\n\n\n\nShow that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]\n\n\n\nExercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#exercise-1",
    "href": "hw/hw01.html#exercise-1",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "We use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-2",
    "href": "hw/hw01.html#exercise-2",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?"
  },
  {
    "objectID": "hw/hw01.html#exercise-3",
    "href": "hw/hw01.html#exercise-3",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Let\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the affine transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?"
  },
  {
    "objectID": "hw/hw01.html#exercise-4",
    "href": "hw/hw01.html#exercise-4",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Show that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-5",
    "href": "hw/hw01.html#exercise-5",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#data",
    "href": "hw/hw01.html#data",
    "title": "Homework 01: simple linear regression",
    "section": "Data",
    "text": "Data\nThe datasets wi-icecover.csv and wi-air-temperature.csv contain information about ice cover and air temperature, respectively, at Lake Monona and Lake Mendota (both in Madison, Wisconsin) for days in 1886 through 2019. The data were obtained from the ntl_icecover and ntl_airtemp data frames in the lterdatasampler R package. They were originally collected by the US Long Term Ecological Research program (LTER) Network.\n\nicecover &lt;- read_csv(\"data/wi-icecover.csv\")\nairtemp &lt;- read_csv(\"data/wi-air-temperature.csv\")\n\nThe analysis will focus on the following variables:\n\nyear: year of observation\nlakeid: lake name\nice_duration: number of days between the freeze and ice breakup dates of each lake\nair_temp_avg: yearly average air temperature in Madison, WI (degrees Celsius)"
  },
  {
    "objectID": "hw/hw01.html#analysis-goal",
    "href": "hw/hw01.html#analysis-goal",
    "title": "Homework 01: simple linear regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goal of this analysis is to use linear regression explain variability in ice duration for lakes in Madison, WI based on air temperature. Because ice cover is impacted by various environmental factors, researchers are interested in examining the association between these two factors to better understand the changing climate."
  },
  {
    "objectID": "hw/hw01.html#exercise-6",
    "href": "hw/hw01.html#exercise-6",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nLet‚Äôs start by looking at the response variable ice_duration.\n\nVisualize the distribution of ice duration versus year with separate lines for each lake.\nThere are separate yearly measurements for each lake in the icecover data frame. In this analysis, we will combine the data from both lakes and use the average ice duration each year.\nComment on the analysis choice to use the average per year rather than the individual lake measurements. Some things to consider in your comments: Does the average accurately reflects the ice duration for these lakes in a given year year? Will there be information lost? How might that impact (or not) the analysis conclusions? Etc.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the ggplot2 reference for example code and plots."
  },
  {
    "objectID": "hw/hw01.html#exercise-7",
    "href": "hw/hw01.html#exercise-7",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNext, let‚Äôs combine the ice duration and air temperature data into a single analysis data frame.\n\nFill in the code below to create a new data frame, icecover_avg, of the average ice duration by year.\nThen join icecover_avg and airtemp to create a new data frame. The new data frame should have 134 observations.\n\nicecover_avg &lt;- icecover |&gt;\n  group_by(_____) |&gt;\n  summarise(_____) |&gt;\n  ungroup()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the new data frame with average ice duration and average air temperature for the remainder of the assignment.\n\n\n\nVisualize the relationship between the air temperature and average ice duration. Do you think a linear model is a reasonable choice to model the relationship between the two variables? Briefly explain.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw01.html#exercise-8",
    "href": "hw/hw01.html#exercise-8",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nWe will fit a model using the average air temperature to explain variability in ice duration. The model takes the form\n\\[\n\\boldsymbol{y}= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\n\nState the dimensions of \\(\\boldsymbol{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\varepsilon}\\) for this analysis. Your answer should have exact values given this data set.\nEstimate the regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) in R using the matrix representation. Show the code used to get the answer.\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits."
  },
  {
    "objectID": "hw/hw01.html#exercise-9",
    "href": "hw/hw01.html#exercise-9",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCalculate \\(R^2\\) for the model in the previous exercise and interpret it in the context of the data.\nCalculate \\(RMSE\\) for the model from the previous exercise and interpret it in the context of the data.\nComment on the model fit based on \\(R^2\\) and \\(RMSE\\)."
  },
  {
    "objectID": "hw/hw01.html#exercise-10",
    "href": "hw/hw01.html#exercise-10",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\na. Interpret the slope in the context of the data.\nb. The average air temperature in 2019, the most recent year in the data set, was 7.925 degrees Celsius. What was the predicted ice duration for 2019? What is the residual?"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in a Quarto document. Note: there is no GitHub repository for this assignment since there are no coding exercises.\nIn all exercises below, you may assume \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{cov}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\).\n\n\nIn lecture, we defined the hat matrix \\(\\boldsymbol{H}\\) as a projection matrix that projects \\(\\boldsymbol{y}\\) onto \\(Col(\\boldsymbol{X})\\) and discussed the properties of a projection matrix. In class we showed that \\(\\boldsymbol{H}\\) is symmetric and idempotent. Now we will focus on two other properties.\n\nShow that for any vector \\(\\boldsymbol{v}\\) in \\(Col(\\boldsymbol{X})\\). \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{v}\\).\nShow that any vector \\(\\boldsymbol{v}\\) orthogonal to \\(Col(\\boldsymbol{X})\\), \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{0}\\)\n\n\n\n\nDerive the expected value and covariance matrix of the residual vector, \\(\\hat{\\boldsymbol{\\varepsilon}}\\). In other words, derive the \\(E[\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X}]\\) and \\(\\text{var}(\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X})\\).\nHint: consider the hat matrix.\n\n\n\nShow that all eigenvalues of the hat matrix \\(\\boldsymbol{H}\\) are 0 or 1.\n\n\n\nLet \\(\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\epsilon\\) where \\(\\beta \\in \\mathbb{R}^p\\) and \\(p = 2\\).\n\nShow that \\((I-\\boldsymbol{H})\\) is orthogonal to \\(Col(\\boldsymbol{X})\\).\nShow \\(E\\left[\\frac{\\text{RSS}}{n-2}| \\boldsymbol{X}\\right]\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nHint: note the fact that \\(a = tr(a)\\) for all scalar numbers \\(a\\). Also note the cyclic property of trace: \\(tr(ABC) = tr(CAB) = tr(BCA)\\).\n\n\n\nShow that if \\(\\bar{x} &gt; 0\\), \\(\\text{cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &lt; 0\\) in simple linear regression.\n\n\n\nBecause the least squares estimator \\(\\hat{\\beta} = \\boldsymbol{A}\\boldsymbol{y}\\), for some matrix \\(\\boldsymbol{A}\\), we call \\(\\hat{\\beta}\\) a linear estimator.\nIf \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{var}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\), then show (a) that \\(E[\\hat{\\beta}] = \\beta\\) and (b) that \\(\\hat{\\beta}\\) has the smallest variance among all linear, unbiased estimators.\nHint for part (b): consider an alternative linear estimator \\(\\tilde{\\beta} = \\boldsymbol{B} \\boldsymbol{y}\\) and show that its variance is equal to the variance of \\(\\hat{\\beta}\\) plus some positive semi-definite matrix.\n\n\n\nDescribe, in your own words, the difference between \\(\\hat{\\beta}\\) and \\(\\beta\\) as well as the difference between \\(\\hat{y}\\) and \\(y\\). In your explanation, specifically identify whether each is random or fixed, known or unknown both before and after collecting the data."
  },
  {
    "objectID": "hw/hw02.html#exercise-1",
    "href": "hw/hw02.html#exercise-1",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "In lecture, we defined the hat matrix \\(\\boldsymbol{H}\\) as a projection matrix that projects \\(\\boldsymbol{y}\\) onto \\(Col(\\boldsymbol{X})\\) and discussed the properties of a projection matrix. In class we showed that \\(\\boldsymbol{H}\\) is symmetric and idempotent. Now we will focus on two other properties.\n\nShow that for any vector \\(\\boldsymbol{v}\\) in \\(Col(\\boldsymbol{X})\\). \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{v}\\).\nShow that any vector \\(\\boldsymbol{v}\\) orthogonal to \\(Col(\\boldsymbol{X})\\), \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{0}\\)"
  },
  {
    "objectID": "hw/hw02.html#exercise-2",
    "href": "hw/hw02.html#exercise-2",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Derive the expected value and covariance matrix of the residual vector, \\(\\hat{\\boldsymbol{\\varepsilon}}\\). In other words, derive the \\(E[\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X}]\\) and \\(\\text{var}(\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X})\\).\nHint: consider the hat matrix."
  },
  {
    "objectID": "hw/hw02.html#exercise-3",
    "href": "hw/hw02.html#exercise-3",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Show that all eigenvalues of the hat matrix \\(\\boldsymbol{H}\\) are 0 or 1."
  },
  {
    "objectID": "hw/hw02.html#exercise-4",
    "href": "hw/hw02.html#exercise-4",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Let \\(\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\epsilon\\) where \\(\\beta \\in \\mathbb{R}^p\\) and \\(p = 2\\).\n\nShow that \\((I-\\boldsymbol{H})\\) is orthogonal to \\(Col(\\boldsymbol{X})\\).\nShow \\(E\\left[\\frac{\\text{RSS}}{n-2}| \\boldsymbol{X}\\right]\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nHint: note the fact that \\(a = tr(a)\\) for all scalar numbers \\(a\\). Also note the cyclic property of trace: \\(tr(ABC) = tr(CAB) = tr(BCA)\\)."
  },
  {
    "objectID": "hw/hw02.html#exercise-5",
    "href": "hw/hw02.html#exercise-5",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Show that if \\(\\bar{x} &gt; 0\\), \\(\\text{cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &lt; 0\\) in simple linear regression."
  },
  {
    "objectID": "hw/hw02.html#exercise-6",
    "href": "hw/hw02.html#exercise-6",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Because the least squares estimator \\(\\hat{\\beta} = \\boldsymbol{A}\\boldsymbol{y}\\), for some matrix \\(\\boldsymbol{A}\\), we call \\(\\hat{\\beta}\\) a linear estimator.\nIf \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{var}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\), then show (a) that \\(E[\\hat{\\beta}] = \\beta\\) and (b) that \\(\\hat{\\beta}\\) has the smallest variance among all linear, unbiased estimators.\nHint for part (b): consider an alternative linear estimator \\(\\tilde{\\beta} = \\boldsymbol{B} \\boldsymbol{y}\\) and show that its variance is equal to the variance of \\(\\hat{\\beta}\\) plus some positive semi-definite matrix."
  },
  {
    "objectID": "hw/hw02.html#exercise-7",
    "href": "hw/hw02.html#exercise-7",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Describe, in your own words, the difference between \\(\\hat{\\beta}\\) and \\(\\beta\\) as well as the difference between \\(\\hat{y}\\) and \\(y\\). In your explanation, specifically identify whether each is random or fixed, known or unknown both before and after collecting the data."
  },
  {
    "objectID": "labs/lab-project.html",
    "href": "labs/lab-project.html",
    "title": "Lab: project planning",
    "section": "",
    "text": "The goal of today‚Äôs lab is to begin your project proposal.\n\nread the entire project description here or by clicking the tab ‚Äúproject‚Äù at the top of the website.\nnavigate to your GitHub repo project-team_name in the course organization and clone the repo.\nedit research-topics.qmd. Push the qmd and rendered pdf documents to GitHub by the deadline: Tuesday, September 30 at 5:00pm. There is no Gradescope submission. This exercise will be graded according to the following rubric:\n\n\n0pts: missing or severely incomplete research-topics\n1pt: somewhat incomplete research-topics\n2pts: each itemized point is thoroughly addressed for each topic\n\n\nAfter you complete this write-up, begin to plan your project proposal."
  },
  {
    "objectID": "labs/lab-project.html#todays-lab",
    "href": "labs/lab-project.html#todays-lab",
    "title": "Lab: project planning",
    "section": "",
    "text": "The goal of today‚Äôs lab is to begin your project proposal.\n\nread the entire project description here or by clicking the tab ‚Äúproject‚Äù at the top of the website.\nnavigate to your GitHub repo project-team_name in the course organization and clone the repo.\nedit research-topics.qmd. Push the qmd and rendered pdf documents to GitHub by the deadline: Tuesday, September 30 at 5:00pm. There is no Gradescope submission. This exercise will be graded according to the following rubric:\n\n\n0pts: missing or severely incomplete research-topics\n1pt: somewhat incomplete research-topics\n2pts: each itemized point is thoroughly addressed for each topic\n\n\nAfter you complete this write-up, begin to plan your project proposal."
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 9 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab01.html#learning-goals",
    "href": "labs/lab01.html#learning-goals",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nRecall some basic matrix operations and linear algebra rules\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to produce visualizations and summary statistics to describe distributions"
  },
  {
    "objectID": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta221-fa25 organization on GitHub.\nClick on the repo with the prefix lab-01. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the lab 0 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab01.html#r-and-r-studio",
    "href": "labs/lab01.html#r-and-r-studio",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file."
  },
  {
    "objectID": "labs/lab01.html#yaml",
    "href": "labs/lab01.html#yaml",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document."
  },
  {
    "objectID": "labs/lab01.html#committing-changes",
    "href": "labs/lab01.html#committing-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Committing changes",
    "text": "Committing changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "labs/lab01.html#push-changes",
    "href": "labs/lab01.html#push-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Push changes",
    "text": "Push changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab01.html#instructions",
    "href": "labs/lab01.html#instructions",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\nWrite all code and narrative in your Quarto file. Write all narrative in complete sentences. Throughout the assignment, you should periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all of your code in your PDF document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (|&gt;) and plus sign (+)."
  },
  {
    "objectID": "labs/lab01.html#exercise-1",
    "href": "labs/lab01.html#exercise-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 1",
    "text": "Exercise 1\nViewing a summary of the data is a useful starting point for data analysis, especially if the data set has a large number of observations (rows) or variables (columns). Run the code below to use the glimpse function to see a summary of the ikea data set.\nHow many observations are in the ikea data set? How many variables?\n\nglimpse(ikea)\n\n\n\n\n\n\n\nNote\n\n\n\nIn your `lab-01.qmd` document you‚Äôll see that we already added the code required for the exercise as well as a sentence where you can fill in the blanks to report the answer. Use this format for the remaining exercises.\nAlso note that the code chunk has a label: glimpse-data. Labeling your code chunks is not required, but it is good practice and highly encouraged."
  },
  {
    "objectID": "labs/lab01.html#exercise-2",
    "href": "labs/lab01.html#exercise-2",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe begin each regression analysis with exploratory data analysis (EDA) to help us ‚Äúget to know‚Äù the data and examine the variable distributions and relationships between variables. We do this by visualizing the data and calculating summary statistics to describe the variables in our data set. In this lab, we will focus on data visualizations.\nWhen we make visualizations, we want them to be clear and suitable to present to a professional audience. This means that, at a minimum, each visualization should have an informative title and informative axis labels.\nFill in the code below to visualize the distribution of price_usd, the price in US dollars.\n\nggplot(data = ikea, aes(x = _____)) +\n  geom_histogram() +\n    labs(x = \"_____\",\n       y = \"_____\", \n       title = \"_____\")"
  },
  {
    "objectID": "labs/lab01.html#exercise-3",
    "href": "labs/lab01.html#exercise-3",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the visualization to describe the distribution of price. In your narrative, include descriptions of the shape, approximate center, approximate spread, and any presence of outliers. Briefly explain why the median is more representative of the center of this distribution than the mean.\nNote: You may compute summary statistics to more precisely describe the center and spread.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-4",
    "href": "labs/lab01.html#exercise-4",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn this course, we‚Äôll be most interested in the relationship between two or more variables, so let‚Äôs begin by looking at the distribution of price by category. We‚Äôll focus on two categories, Sofas & armchairs and Bookcases & shelving units, since these may be types of furniture most useful to furnish an office.\nFill in the code below to create a new data frame called ikea_sub that only includes the furniture categories of interest. We‚Äôre assigning this subsetted data frame to an object with a new name, so we don‚Äôt overwrite the original data.\n\nikea_sub &lt;- ikea |&gt;\n  filter(_____ %in% c( \"_____\",\n                       \"_____\"))\n\nNow, run the code below to remove observations that have that have a missing value for at least one of width or price_usd.\n\nikea_sub &lt;- ikea_sub |&gt;\n  drop_na(width, price_usd)\n\nHow many observations are in the ikea_sub data frame? How many variables?\n\n\n\n\n\n\nImportant\n\n\n\nUse the ikea_sub data frame for the remainder of lab."
  },
  {
    "objectID": "labs/lab01.html#exercise-5",
    "href": "labs/lab01.html#exercise-5",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate a visualization of the relationship between the width and price of your items at Ikea in the two categories of interest. Include informative axis labels and an informative title. Use the visualization to describe the relationship between the two variables.\nThen, recreate your visualization, but now adding color based on furniture category. Comment on your observations from this visualization.\nNote: Show both visualizations in the response.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#instructions-1",
    "href": "labs/lab01.html#instructions-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\n\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nPut any relevant R code in the Quarto document. You may write the answers and show any associated work for conceptual exercises by hand or type them in your Quarto document using LaTex.\nLet\n\\[\nA = \\begin{bmatrix}\n1 & 2\\\\\n3 & 4\\\\\n5 & 6\\end{bmatrix},\n\\qquad\nB = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n0 & 1 & 2 & 3\n\\end{bmatrix},\n\\qquad\nC = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\end{bmatrix}\n\\qquad\n\\]\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12}& \\dots & x_{1p}\\\\\nx_{21} & x_{22}& \\dots & x_{2p}\\\\\n\\vdots & \\vdots& \\ddots & \\vdots\\\\\nx_{n1} & x_{n2}& \\dots & x_{np}\\\\\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-6",
    "href": "labs/lab01.html#exercise-6",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 6",
    "text": "Exercise 6\nWrite the dimensions of the following matrices:\n\n\\(A\\)\n\\(B\\)\n\\(A^\\top\\)\n\\(\\mathbf{X}\\)\n\\(\\mathbf{X}^\\top\\)"
  },
  {
    "objectID": "labs/lab01.html#exercise-7",
    "href": "labs/lab01.html#exercise-7",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 7",
    "text": "Exercise 7\ni. Which of the following is a proper matrix multiplication operation? Explain why.\n\n\\(A\\times C\\)\n\\(A\\times B\\)\n\\(A^\\top \\times B\\)\n\\(B \\times A\\)\n\\(B^\\top \\times C\\)\n\\(B\\times B\\)\n\nii. Perform the multiplication you chose in part (i)."
  },
  {
    "objectID": "labs/lab01.html#matrix-operations-in-r",
    "href": "labs/lab01.html#matrix-operations-in-r",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Matrix operations in R",
    "text": "Matrix operations in R\nR has built in matrix tools such as addition, multiplication, transpose, etc. We will now practice using these tools to review some matrix properties.\nWe first begin by creating matrices using matrix() function. We provide elements of our matrices as the data argument and specify how many rows our matrices have. byrow = TRUE allows us to fill the matrix by row.\n\nA &lt;- matrix(data = c(1, 2,\n                     3, 4,\n                     5, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nB &lt;- matrix(data = c(1, 1, 1, 1, \n                     0, 1, 2, 3), \n            nrow = 2,\n            byrow = TRUE)\n\nC &lt;-  matrix(data = c(1, 4,\n                      2, 5,\n                      3, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nYou can learn more about matrix() function by typing ?matrix in console."
  },
  {
    "objectID": "labs/lab01.html#exercise-8",
    "href": "labs/lab01.html#exercise-8",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 8",
    "text": "Exercise 8\ni. To perform addition or subtraction, we can simply use a + or - operators.\n\n# Add A and C\nA + C\n\n     [,1] [,2]\n[1,]    2    6\n[2,]    5    9\n[3,]    8   12\n\n\nUsing R, find \\(C + A\\). Is addition commutative (i.e.¬†does \\(A + C = C + A\\))? Show the work to support your response.\nii. In R, we have to use a special matrix multiplication operator, %*% .\n\n# multiply A and B\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    3    7   11   15\n[3,]    5   11   17   23\n\n\nDoes the output match your answer to Exercise 7 (ii)? What happens if you try to multiply \\(B\\times A\\) in R?\n\n\n\n\n\n\nWarning\n\n\n\nMatrix multiplication is not commutative! Matrix multiplication satisfies left and right distributivity: \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\), but the order here matters. \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} \\neq \\mathbf{C}\\mathbf{A} +\\mathbf{C} \\mathbf{B}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) \\neq \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\). Pay attention to the order and dimensions of matrices.\n\n\niii. In this class, we will work a lot with matrix transposes. You can transpose a matrix in R by applying t() function.\n\n# transpose A\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nFind \\(B^\\top \\times A^\\top\\) using R. How is your answer compare to the result of \\(A\\times B\\) you found in previous part?\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 6 - 8‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-9",
    "href": "labs/lab01.html#exercise-9",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 9",
    "text": "Exercise 9\nLet \\(\\mathbf a = \\begin{bmatrix}a_1 \\\\ \\vdots \\\\ a_n\\end{bmatrix}\\) and \\(\\mathbf b = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_n\\end{bmatrix}\\). Recall, \\[\\mathbf{a}^\\top \\mathbf{a} = \\sum_{i=1}^n a_i^2.\\]\nWrite \\((\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{a} - \\mathbf{b})\\) using summation notation."
  },
  {
    "objectID": "labs/lab01.html#more-linear-algebra",
    "href": "labs/lab01.html#more-linear-algebra",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "More Linear Algebra",
    "text": "More Linear Algebra\nRecall the definition of linear dependence:\nA sequence of vectors \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\) is said to be linearly dependent if there exists a series of scalars \\(a_1, a_2, \\dots, a_p\\), not all zero, such that\n\\[\na_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{0}\n\\] Further, matrix \\(\\mathbf{X}\\) has full column rank if all of its columns are linearly independent.\nFor example, the following matrix is not full rank,\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n1 & 1 & 2\n\\end{bmatrix}\n\\] since, letting letting \\(\\mathbf{x_1} = \\begin{bmatrix}1\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_2} = \\begin{bmatrix}2\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_3} = \\begin{bmatrix}3\\\\2 \\end{bmatrix}\\), and letting \\(a_1 = 1\\), \\(a_2 = 1\\), and \\(a_3 = -1\\), we have:\n\\[\na_1 \\mathbf{x_1} + a_2\\mathbf{x_2} + a_3\\mathbf{x_3} =  \\begin{bmatrix}1\\\\1 \\end{bmatrix} + \\begin{bmatrix}2\\\\1 \\end{bmatrix} - \\begin{bmatrix}3\\\\2 \\end{bmatrix} = \\mathbf{0}.\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-10",
    "href": "labs/lab01.html#exercise-10",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 10",
    "text": "Exercise 10\nFor each of the following matrices, state whether it is full rank. If not full rank, show why (find corresponding coefficients \\(a\\)‚Äôs).\n\n\\[\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n0 & 0 & 1\\\\\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n1 & 0 & 0 & 1\\\\\n1 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 23 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab03.html#exercise-1",
    "href": "labs/lab03.html#exercise-1",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet‚Äôs start with some exploratory data analysis. Visualize the distribution of the response variable mc_preschool and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab03.html#exercise-2",
    "href": "labs/lab03.html#exercise-2",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for preschool-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It‚Äôs your turn! Type the team‚Äôs response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab03.html#exercise-3",
    "href": "labs/lab03.html#exercise-3",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we‚Äôve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model ‚Äúas-is‚Äù may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\n\n\n\n\n\n\n\n\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab03.html#exercise-4",
    "href": "labs/lab03.html#exercise-4",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let‚Äôs conduct bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It‚Äôs your turn! Type the team‚Äôs response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab03.html#exercise-5",
    "href": "labs/lab03.html#exercise-5",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab03.html#exercise-6",
    "href": "labs/lab03.html#exercise-6",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm() to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It‚Äôs your turn! Type the team‚Äôs response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab03.html#exercise-7",
    "href": "labs/lab03.html#exercise-7",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let‚Äôs assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for preschool-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab03.html#exercise-8",
    "href": "labs/lab03.html#exercise-8",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab03.html#exercise-9",
    "href": "labs/lab03.html#exercise-9",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mc_preschool.\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team‚Äôs completed lab!"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Code\n\nRStudio containers\n\nCommunication\n\nEd Discussion\n\nCollaboration\n\ncourse GitHub organization\n\nAssignment turn-in\n\nGradescope"
  },
  {
    "objectID": "notes/lec02-correlation.html",
    "href": "notes/lec02-correlation.html",
    "title": "Correlation and intro to simple linear regresion",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(DT)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec02-correlation.html#learning-objectives",
    "href": "notes/lec02-correlation.html#learning-objectives",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the day you should be able to explain the following concepts:\n\ncovariance\ncorrelation\nlocation and scale invariance\nordinary least squares"
  },
  {
    "objectID": "notes/lec02-correlation.html#example",
    "href": "notes/lec02-correlation.html#example",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Example",
    "text": "Example\n\nThis data set contains Raleigh Durham International Airport weather data pulled from the NOAA web service between September 01 and September 30, 2021. The data were sourced from https://catalog.data.gov/ August 28, 2025.\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve recorded 30 observations of two measurements. We are interested in the association between these two measurements.\nWe‚Äôll call the minimum daily temperature measurement ‚Äúx‚Äù and the maximum daily temperature ‚Äúy‚Äù."
  },
  {
    "objectID": "notes/lec02-correlation.html#vocabulary",
    "href": "notes/lec02-correlation.html#vocabulary",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Vocabulary",
    "text": "Vocabulary\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\n\\(y\\)\nThe outcome variable. Also called ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. In prediction tasks, this is the variable we are interested in predicting.\n\n\n\\(x\\)\nThe predictor. Also called ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, or ‚Äúindependent variable‚Äù.\n\n\n\nHow are \\(x\\) and \\(y\\) associated?\n\nScatter plotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather %&gt;%\n  ggplot(aes(x = tmin, y = tmax)) +\n  geom_point() +\n  labs(title = \"Minimum and maxmimum temperature (F) at RDU in September, 2021\") +\n  theme_bw()\n\n\n\n\nNotice: visualized this way, each of the thirty data points, \\((x_i, y_i)\\), is an element of two-dimensional space.\n\nHow would you describe the association?\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\(cov(x, y) = \\frac{1}{n-1}\\sum_{i =1}^n (x_i - \\bar{x}) (y_i - \\bar{y})\\) \\(^*\\)\n\\(^*\\) Note that we divide by \\(n-1\\) when computing the sample covariance\n\n\\(\\bar{x} = \\frac{1}{n} \\sum x_i\\) (mean of x)\n\\(\\bar{y} = \\frac{1}{n} \\sum y_i\\) (mean of y)\n\n\n\n\nx = weather %&gt;%\n  select(tmin) %&gt;%\n  pull()\ny = weather %&gt;%\n  select(tmax) %&gt;%\n  pull()\n\ncov(x, y)\n\n[1] 18.68736\n\nsum((x - mean(x)) * (y - mean(y))) / (30 - 1)\n\n[1] 18.68736\n\n\n\nShould the association be the same if the thermometer recording measurements was consistently off by 2 degrees Farenheit?\n\n\n# compute covariance of true temperature measurement\nweather %&gt;%\n  mutate(temp_min_true = tmin + 2,\n         temp_max_true = tmax + 2) %&gt;%\n  summarize(cov(temp_min_true, temp_max_true)) %&gt;%\n  pull()\n\n[1] 18.68736\n\n\nCovariance is location invariant.\n\nWill the association stay the same if we recorded temperature in celsius?\n\n\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cov(temp_min_c, temp_max_c)) %&gt;%\n  pull()\n\n[1] 5.767703\n\n\nCovariance is not scale invariant. That is, covariance does depend on the scale of the measurements."
  },
  {
    "objectID": "notes/lec02-correlation.html#correlation",
    "href": "notes/lec02-correlation.html#correlation",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nDefinition: correlation\n\n\n\n\\(cor(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\left(\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2\\right)^{1/2}}\\)\nMore concisely, we may write\n\\(cor(x, y) = \\frac{S_{xy}}{S_{xx}^{1/2} S_{yy}^{1/2}}\\)\n\n\n\n# correlation in Farenheit vs Celsi\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cor_F = cor(tmin, tmax), \n            cor_C = cor(temp_min_c, temp_max_c))\n\n# A tibble: 1 √ó 2\n  cor_F cor_C\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.554 0.554\n\n\n\nExerciseSolution\n\n\nShow \\(cor(x, y) = cor(ax + b, cy + d)\\).\n\n\nLet \\[\n\\begin{aligned}\nx^* &= ax_i + b,\\\\\ny^* &= cx_i + d,\n\\end{aligned}\n\\]\nthen we want to prove that \\(cor(x, y) = cor(x^*, y^*)\\).\n\\[\n\\begin{aligned}\ncor(x^*, y^*) &= \\frac{\\sum(ax_i + b - a\\bar{x} - b)(cy_i + d - c\\bar{y} - d)}{\\sqrt{\n\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{ac \\sum(x_i -\\bar{x})(y_i - \\bar{y})}{\nac \\sqrt{\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{\nS_{xy}}{\\sqrt{S_{xx} S_{yy}}\n}\n\\end{aligned}\n\\]\n\n\n\nCorrelation is location and scale invariant!\nAdditional facts about correlation:\n\n\n\n\n\n\nFact 1\n\n\n\n\nCorrelation is not invariant to monotone transformations of the data\n\n\n\n\n\n\n\n\n\nDefinition: monotone\n\n\n\n\\(g\\) is a monotonic function iff \\(x \\leq y\\) implies \\(g(x) \\leq g(y)\\)\n\n\n\nExerciseSolution\n\n\nShow by example that correlation is not invariant to monotonic transformations.\n\n\n\nset.seed(221)\nx = c(1:10) \nlogx = log(x)\ny = rnorm(10)\ncor(x, y)\n\n[1] -0.1757986\n\ncor(logx, y)\n\n[1] -0.2686214\n\n\n\n\n\n\n\n\n\n\n\nFact 2\n\n\n\n\nCorrelation is not robust to outliers\n\n\n\n\nExample 1Example 2\n\n\n\nset.seed(221)\nx = c(1:5)\ny= x + rnorm(5)\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 0.9042223\n\n\n\n\n\nx2 = c(x, 0)\ny2 = c(y, 20)\nplot(x2, y2)\n\n\n\n\n\n\n\ncor(x2, y2)\n\n[1] -0.5195233\n\n\n\n\n\n\n\n\n\n\n\nFact 3\n\n\n\nCorrelation is bounded between -1 and 1.\n\n\n\nExerciseSolution\n\n\nShow that \\(|cor(x, y)| \\leq 1\\)\n\n\nCauchy-Schwarz inequality:\nLet \\(u\\) and \\(v\\) be vectors of dimension \\(n\\), then\n\\[\n|u^T v|^2 \\leq (u^Tu) (v^Tv).\n\\] To prove that \\(\\|cor(x,y)\\| \\leq 1\\), let \\(u = x - \\bar{x}\\) and let \\(v = y - \\bar{y}\\).\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the dimension of a vector inner product, e.g.¬†\\(u^Tv\\) is \\(1 \\times 1\\), in other words, it is a ‚Äúscalar‚Äù, a number.\n\n\n\n\n\n\n\n\n\n\n\nFact 4\n\n\n\n\ncorrelation is a measure of linear association\n\n\n\n\nExample 1Example 2\n\n\n\nx = -10:10\ny = x^2\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y) \n\n[1] -4.786989e-17\n\n\n\n\n\nx = -10:10\ny = x\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 1\n\n\n\n\n\nSince correlation is related to a linear relationship between the data, strong correlation implies that\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\nExercise\n\n\nCheck out the interactive regression web app here:\nhttps://seeing-theory.brown.edu/regression-analysis/index.html#section1"
  },
  {
    "objectID": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "href": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of simple linear regression",
    "text": "Nomenclature of simple linear regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{\\epsilon}\\) or \\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html",
    "href": "notes/lec04-model-assessment.html",
    "title": "Model assessment",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(DT) # shows the data table\nlibrary(patchwork) # arranging plots\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)\nlibrary(knitr)       # aesthetic tables\n \nlife_exp &lt;- read_csv(\n    \"https://sta221-fa25.github.io/data/life-expectancy-data.csv\") |&gt; \n  rename(life_exp = `Life_expectancy_at_birth`, \n         income_inequality = `Income_inequality_Gini_coefficient`) |&gt;\n  mutate(education = if_else(Education_Index &gt; median(Education_Index), \"High\", \"Low\"), \n         education = factor(education, levels = c(\"Low\", \"High\"))) |&gt;\n  select(Country, life_exp, Health_expenditure, income_inequality)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#learning-objectives",
    "href": "notes/lec04-model-assessment.html#learning-objectives",
    "title": "Model assessment",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nfit simple linear regression models in R\ninterpret coefficients in context\nanalyze the variance\ncheck model fit and interpret in context"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#example",
    "href": "notes/lec04-model-assessment.html#example",
    "title": "Model assessment",
    "section": "Example",
    "text": "Example\nThe data set comes from Zarulli et al. (2021) who analyze the effects of a country‚Äôs healthcare expenditures and other factors on the country‚Äôs life expectancy. The data are originally from the Human Development Database and World Health Organization.\nThere are 140 countries (observations) in the data set.\n\n\n\n\n\n\nGoal: Use the income inequality in a country to understand variability in the life expectancy.\n\n\nClick here for the original research paper.\n\nlife_exp: The average number of years that a newborn could expect to live, if he or she were to pass through life exposed to the sex- and age-specific death rates prevailing at the time of his or her birth, for a specific year, in a given country, territory, or geographic area (from the World Health Organization).\nincome_inequality: Measure of the deviation of the distribution of income among individuals or households within a country from a perfectly equal distribution. A value of 0 represents absolute equality, a value of 100 absolute inequality, based on ‚ÄúGini coefficient‚Äù (from Zarulli et al. (2021)).\nhealth_expenditure: Per capita current spending on healthcare goods and services, expressed in respective currency - international Purchasing Power Parity (PPP) dollar (from the World Health Organization)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "href": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "title": "Model assessment",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#univariate-eda",
    "href": "notes/lec04-model-assessment.html#univariate-eda",
    "title": "Model assessment",
    "section": "Univariate EDA",
    "text": "Univariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = life_exp, aes(x = life_exp))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Life expectancy (years)\", \n       y = \"Count\") +\n  theme_bw()\n\np2 &lt;- ggplot(data = life_exp, aes(x = income_inequality))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Income inequality\", \n       y = \"Count\") +\n  theme_bw()\n\np1 + p2 # uses patchwork package"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#bivariate-eda",
    "href": "notes/lec04-model-assessment.html#bivariate-eda",
    "title": "Model assessment",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of life expectancy vs income inequality\np1 &lt;- ggplot(life_exp, aes(x = income_inequality, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Income inequality\",\n    y = \"Life expectancy (years)\"\n  ) +\n  theme_bw()\n\n# Scatterplot of life expectancy vs health expenditure\np2 &lt;- ggplot(life_exp, aes(x = Health_expenditure, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Health expenditure\",\n    y = \"\"\n  ) +\n  theme_bw()\n\n# Display plots side by side; uses patchwork pacakge\np1 + p2"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "href": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "title": "Model assessment",
    "section": "Ordinary least squares regression in R",
    "text": "Ordinary least squares regression in R\n\nTemplate\nTo fit a model by OLS linear regression, we use the lm function. The arguments look as follows:\n\nlm(y ~ x, data = data_frame)\n\nIn words, we ‚Äúregress y on x‚Äù where ‚Äúy‚Äù and ‚Äúx‚Äù are column names in the data frame ‚Äúdata_frame‚Äù.\n\n\nFor our data\n\n# income inequality model\nmodel_ii = lm(life_exp ~ income_inequality, \n              data = life_exp)\n\n# health expenditure model\nmodel_he = lm(life_exp ~ Health_expenditure,\n              data = life_exp)\n\n\nQ: why is Health_expenditure capitalized, but income_inequality is not?\nA: That‚Äôs how they are named in the data! See the column names: names(life_exp).\n\n\n\nLook at results (income inequality model)\nRegular output:\n\nmodel_ii\n\n\nCall:\nlm(formula = life_exp ~ income_inequality, data = life_exp)\n\nCoefficients:\n      (Intercept)  income_inequality  \n          85.4202            -0.6973  \n\n\nTidy the output:\n\nmodel_ii |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         85.4      0.855       99.9 1.41e-130\n2 income_inequality   -0.697    0.0388     -18.0 6.17e- 38\n\n\nPut results in a more aesthetic table using kable from the knitr package:\n\nmodel_ii |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n85.420\n0.855\n99.877\n0\n\n\nincome_inequality\n-0.697\n0.039\n-17.961\n0\n\n\n\n\n\nThe fitted regression model:\n\\[\n\\hat{y_i} = 85.420 - 0.697 x_i\n\\]\n\n\nInterpretation\n\nThe intercept estimate \\(\\hat{\\beta}_0\\) is in units of the response variable. It is the expected value of the response variable when the predictor is set to 0.\nThe estimate of the slope coefficient, \\(\\hat{\\beta}_1\\) is measured in the units of the response variable per unit of the explanatory variable.\n\n\nExercise\n\n\nInterpret the coefficients above (for the income inequality model) in context.\n\n\n\n\n\nPrediction\nUse the predict function to calculate predictions for new observations\nSingle observation\n\nnew_ii &lt;- tibble(income_inequality = 50)\npredict(model_ii, new_ii)\n\n       1 \n50.55618 \n\n\nMultiple observations\n\nmore_new_ii &lt;- tibble(income_inequality = c(25,50, 100))\npredict(model_ii, more_new_ii)\n\n       1        2        3 \n67.98821 50.55618 15.69213 \n\n\nNote the range:\n\nrange(life_exp$income_inequality)\n\n[1]  5.4 44.2\n\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing the model to predict for values outside the range of the original data is extrapolation. Why do we want to avoid extrapolation?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#model-assessment",
    "href": "notes/lec04-model-assessment.html#model-assessment",
    "title": "Model assessment",
    "section": "Model assessment",
    "text": "Model assessment\nWe fit a model but is it any good?\n\nTwo statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n\nWhat indicates a good model fit? Higher or lower RMSE? Higher or lower \\(R^2\\)?\n\n\n\nRMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "href": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "title": "Model assessment",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response",
    "href": "notes/lec04-model-assessment.html#total-variability-response",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMin\nMedian\nMax\nMean\nStd.Dev\n\n\n\n\n51.6\n72.85\n84.1\n71.614\n8.075"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "href": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "title": "Model assessment",
    "section": "Partition sources of variability in life_exp",
    "text": "Partition sources of variability in life_exp"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response-1",
    "href": "notes/lec04-model-assessment.html#total-variability-response-1",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Total (SST)} = S_{yy} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1) \\cdot var(y)\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#explained-variability-model",
    "href": "notes/lec04-model-assessment.html#explained-variability-model",
    "title": "Model assessment",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "href": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "title": "Model assessment",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#sum-of-squares",
    "href": "notes/lec04-model-assessment.html#sum-of-squares",
    "title": "Model assessment",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{darkred}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{darkred}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]\n\n\nClick here to see why this equality holds."
  },
  {
    "objectID": "notes/lec04-model-assessment.html#r2",
    "href": "notes/lec04-model-assessment.html#r2",
    "title": "Model assessment",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#interpreting-r2",
    "href": "notes/lec04-model-assessment.html#interpreting-r2",
    "title": "Model assessment",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "href": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "title": "Model assessment",
    "section": "Using R to look at these quantities",
    "text": "Using R to look at these quantities\n\nAugmented data frame\nUse the augment() function from the broom package to add columns for predicted values, residuals, and other observation-level model statistics\n\nlife_exp_aug &lt;- augment(model_ii)\nlife_exp_aug\n\n# A tibble: 140 √ó 8\n   life_exp income_inequality .fitted .resid    .hat .sigma   .cooksd .std.resid\n      &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1     63.8              28.2    65.8 -1.96  0.0125    4.45 0.00125       -0.444\n 2     78.2              12.2    76.9  1.29  0.0116    4.45 0.000498       0.292\n 3     59.9              32.4    62.8 -2.93  0.0193    4.44 0.00437       -0.667\n 4     76.2              14      75.7  0.542 0.00972   4.45 0.0000739      0.123\n 5     74.6               8.6    79.4 -4.82  0.0168    4.43 0.0102        -1.10 \n 6     83                 8.3    79.6  3.37  0.0173    4.44 0.00515        0.766\n 7     81.3               7.4    80.3  1.04  0.0189    4.45 0.000540       0.237\n 8     72.5              10.1    78.4 -5.88  0.0144    4.42 0.0130        -1.33 \n 9     71.8              27.6    66.2  5.62  0.0118    4.43 0.00972        1.28 \n10     74                 6.5    80.9 -6.89  0.0207    4.41 0.0260        -1.57 \n# ‚Ñπ 130 more rows\n\n\n\n\nFinding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.40\n\n\n\n\nFinding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.700\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(model_ii)$r.squared\n\n[1] 0.7003831"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html",
    "href": "notes/lec06-multiple-regression.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\nlibrary(DT)"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "href": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "title": "Multiple linear regression",
    "section": "Fitting a regression model",
    "text": "Fitting a regression model\nTo ‚Äúfit‚Äù a (multiple) linear regression model means finding \\(\\hat{\\beta}\\) that defines the ‚Äúbest‚Äù hyperplane. Here, ‚Äúbest‚Äù means that \\(\\hat{\\beta}\\) is the optimal solution of some objective function."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#full-data-set",
    "href": "notes/lec06-multiple-regression.html#full-data-set",
    "title": "Multiple linear regression",
    "section": "Full data set",
    "text": "Full data set\n\ndatatable(penguins, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"penguins\")\n\n\n\n\n\n\nExerciseSolution\n\n\nWrite down the mathematical formula for linear regression where penguin bodymass is the outcome variable and flipper length, bill length, and sex of the penguin are the covariates. Note the dimension of each symbol in your model.\n\n\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon}\n\\]\nAs usual,\n\n\\(\\boldsymbol{y}, \\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\nbut here \\(p = 4\\) (3 covariates + intercept).\n\\[\n\\boldsymbol{X}=\n\\begin{bmatrix}\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{1} & \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots\n\\end{bmatrix}\n\\]\n\n\n\nQuestion: Why is sex (a categorical predictor) represented as 1 column and not 2?\nAnswer: It would create a linearly dependent column in the matrix \\(\\boldsymbol{X}\\), then \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) would be rank deficient and could not be inverted.\nQuestion: Suppose we add the covariate ‚Äúisland‚Äù, what is the new dimension of \\(\\boldsymbol{X}\\) and \\(\\beta\\) then?\nAnswer: Island is categorical with three states, therefore we would need 2 predictors to represent it. Now \\(p = 6\\) and \\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times 6}\\) and \\(\\beta \\in \\mathbb{R}^6\\)."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "href": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "title": "Multiple linear regression",
    "section": "Fitting multiple regression in R (OLS)",
    "text": "Fitting multiple regression in R (OLS)\nThe formula for the least squares estimator \\(\\hat{\\beta}_{OLS}\\) is the same,\n\\[\n\\hat{\\beta}_{OLS} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\]\n\nlmmanualin-between\n\n\n\npenguin_fit &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island, data = penguins)\n\npenguin_fit\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    sex + island, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm     bill_length_mm            sexmale  \n        -3629.563             37.638              5.808            390.055  \n      islandDream    islandTorgersen  \n         -376.124           -288.380  \n\n\nQuestion: Why didn‚Äôt I have to specify an intercept?\nAnswer: lm includes intercept by default.\nQuestion: What if I don‚Äôt want an intercept?\nAnswer: Use + 0, like this: lm(y ~ x1 + x2 + ... + x_5 + 0, data = penguin). This will tell lm not to exclude an intercept term from the model.\n\n\n\nyX &lt;- penguins %&gt;%\n  select(c(\"body_mass_g\", \"bill_length_mm\", \"flipper_length_mm\",\n           \"sex\", \"island\")) %&gt;%\n  mutate(isMale = ifelse(sex == \"male\", 1, 0), ## create 1 dummy variable \n           isDream = ifelse(island == \"Dream\", 1, 0), ## create other 2 dummy variables\n         isTorgersen = ifelse(island == \"Torgersen\", 1, 0)) %&gt;%\n  select(-c(island, sex)) %&gt;% ## remove redundant columns \n  mutate(one = 1) %&gt;% # create vector of ones\n  drop_na() %&gt;% # drop NAs, just like the lm() function does by default\n  as.matrix() # turn into a matrix for matrix algebra in R\n\ny &lt;- yX[,1] # grab first column (which was the outcome variable)\nX &lt;- yX[,-1] # grab all but first column (X matrix)\n\nbetaHat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbetaHat\n\n                          [,1]\nbill_length_mm        5.808086\nflipper_length_mm    37.637788\nisMale              390.055414\nisDream            -376.124225\nisTorgersen        -288.380366\none               -3629.562552\n\n\n\n\nAn alternative, easy way to grab \\(X\\) quickly:\n\nX = model.matrix( body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island,\n              data = penguins)\nX %&gt;%\n  head(n = 10) # notice NAs are dropped by default, (see the row number skips)\n\n   (Intercept) flipper_length_mm bill_length_mm sexmale islandDream\n1            1               181           39.1       1           0\n2            1               186           39.5       0           0\n3            1               195           40.3       0           0\n5            1               193           36.7       0           0\n6            1               190           39.3       1           0\n7            1               181           38.9       0           0\n8            1               195           39.2       1           0\n13           1               182           41.1       0           0\n14           1               191           38.6       1           0\n15           1               198           34.6       1           0\n   islandTorgersen\n1                1\n2                1\n3                1\n5                1\n6                1\n7                1\n8                1\n13               1\n14               1\n15               1"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#interpretation",
    "href": "notes/lec06-multiple-regression.html#interpretation",
    "title": "Multiple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nSlopes\nWe interpret slopes \\(\\hat{\\beta}_j\\) as the expected change in the mean of \\(Y\\) when \\(X_j\\) increases by one unit, holding the value of all other predictor variables constant.\nFor example, for each additional mm of flipper length a given penguin has, we expect their body mass to increase by 37.6 grams holding all other covariates constant.\n\n\nIntercept\nThe intercept is the expected value of \\(Y\\) when all predictors are zero. Note: when there is a categorical predictor, this corresponds to some ‚Äúdefault‚Äù category."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#prediction",
    "href": "notes/lec06-multiple-regression.html#prediction",
    "title": "Multiple linear regression",
    "section": "Prediction",
    "text": "Prediction\nWe can still make predictions in R; now we require more covariates. For example:\n\nnew_penguin &lt;- tibble(\n  flipper_length_mm = 180, \n  bill_length_mm = 40,\n  sex = \"male\",\n  island = \"Dream\"\n)\n\ncat(\"The predicted body mass of the new penguin is:\\n\")\n\nThe predicted body mass of the new penguin is:\n\npredict(penguin_fit, new_penguin)\n\n       1 \n3391.494 \n\n\n\n\n\n\n\n\nWarning\n\n\n\nRegression shows association not causality."
  },
  {
    "objectID": "notes/lec08-inference-2.html",
    "href": "notes/lec08-inference-2.html",
    "title": "Inference part II",
    "section": "",
    "text": "This section re-states the definitions of variance and covariance as well as lists a few important properties.\nIn this section, let \\(y\\) and \\(z\\) be random variables and let \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{z}\\) be random vectors. Let \\(a\\) be a constant and let \\(\\boldsymbol{A}\\) be a constant matrix.\n\n\n\n\n\n\nDefinition: variance\n\n\n\n\\[\n\\text{var}(y) = E[(y - E[y])^2] = E[y^2] - \\left(E[y]\\right)^2\n\\]\nSimilarly,\n\\[\n\\text{var}(\\boldsymbol{y}) = E \\boldsymbol{y}\\boldsymbol{y}^T - E\\boldsymbol{y}E\\boldsymbol{y}^T\n\\]\nNotice that the ‚Äúvariance of a vector‚Äù is a square matrix where each entry is the covariance between each pair of elements of the random vector.\n\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\[\n\\text{cov}(y, z) = E[(y - E[y])(z - E[z])] = E[yz] - \\left(E[y]E[z]\\right)\n\\]\nSimilarly,\n\\[\n\\text{cov}(\\boldsymbol{y}, \\boldsymbol{z}) = E \\boldsymbol{y}\\boldsymbol{z}^T - E\\boldsymbol{y}E\\boldsymbol{z}^T\n\\]\nNotice that \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{y}) = \\text{var}(\\boldsymbol{y})\\)\n\n\nProperties\n\n\\(\\text{var}(ay) = a^2 \\text{var}(y)\\) and similarly, \\(\\text{var}(\\boldsymbol{A} \\boldsymbol{y}) = \\boldsymbol{A} \\text{var}(\\boldsymbol{y}) \\boldsymbol{A}^T\\)\nBilinearity: \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{az}) = a \\cdot \\text{cov}(\\boldsymbol{y}, \\boldsymbol{z})\\)\nvariance of a sum: \\(\\text{var}(y + z) = \\text{var}(y) + \\text{var}(z) + 2\\text{cov}(y, z)\\)\n\n\nExerciseSolution\n\n\nWhat is \\(\\text{var}(ay - bz)\\)? Hint: apply properties 2 and 3 together.\n\n\n\\(\\text{var}(ay - bz) = \\text{var}(ay) + \\text{var}(-bz) + 2 \\text{cov}(ay, -bz) = a^2 \\text{var}(y) + b^2\\text{var}(z) - 2ab\\text{cov}(y, z)\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#background",
    "href": "notes/lec08-inference-2.html#background",
    "title": "Inference part II",
    "section": "",
    "text": "This section re-states the definitions of variance and covariance as well as lists a few important properties.\nIn this section, let \\(y\\) and \\(z\\) be random variables and let \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{z}\\) be random vectors. Let \\(a\\) be a constant and let \\(\\boldsymbol{A}\\) be a constant matrix.\n\n\n\n\n\n\nDefinition: variance\n\n\n\n\\[\n\\text{var}(y) = E[(y - E[y])^2] = E[y^2] - \\left(E[y]\\right)^2\n\\]\nSimilarly,\n\\[\n\\text{var}(\\boldsymbol{y}) = E \\boldsymbol{y}\\boldsymbol{y}^T - E\\boldsymbol{y}E\\boldsymbol{y}^T\n\\]\nNotice that the ‚Äúvariance of a vector‚Äù is a square matrix where each entry is the covariance between each pair of elements of the random vector.\n\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\[\n\\text{cov}(y, z) = E[(y - E[y])(z - E[z])] = E[yz] - \\left(E[y]E[z]\\right)\n\\]\nSimilarly,\n\\[\n\\text{cov}(\\boldsymbol{y}, \\boldsymbol{z}) = E \\boldsymbol{y}\\boldsymbol{z}^T - E\\boldsymbol{y}E\\boldsymbol{z}^T\n\\]\nNotice that \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{y}) = \\text{var}(\\boldsymbol{y})\\)\n\n\nProperties\n\n\\(\\text{var}(ay) = a^2 \\text{var}(y)\\) and similarly, \\(\\text{var}(\\boldsymbol{A} \\boldsymbol{y}) = \\boldsymbol{A} \\text{var}(\\boldsymbol{y}) \\boldsymbol{A}^T\\)\nBilinearity: \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{az}) = a \\cdot \\text{cov}(\\boldsymbol{y}, \\boldsymbol{z})\\)\nvariance of a sum: \\(\\text{var}(y + z) = \\text{var}(y) + \\text{var}(z) + 2\\text{cov}(y, z)\\)\n\n\nExerciseSolution\n\n\nWhat is \\(\\text{var}(ay - bz)\\)? Hint: apply properties 2 and 3 together.\n\n\n\\(\\text{var}(ay - bz) = \\text{var}(ay) + \\text{var}(-bz) + 2 \\text{cov}(ay, -bz) = a^2 \\text{var}(y) + b^2\\text{var}(z) - 2ab\\text{cov}(y, z)\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#last-time",
    "href": "notes/lec08-inference-2.html#last-time",
    "title": "Inference part II",
    "section": "Last time",
    "text": "Last time\nLast time we asked the question: what assumptions are required so that \\(\\hat{\\beta}\\) is a good representation of \\(\\beta\\)?\n\n\n\n\n\n\nAssumption 1\n\n\n\n\\(E[\\boldsymbol{\\varepsilon}|\\boldsymbol{x}] = \\boldsymbol{0}\\), or equivalently, \\(E[\\varepsilon_i|\\boldsymbol{x}] = 0\\) for all \\(i\\).\n\n\n\nExerciseSolution\n\n\nShow that assumption 1 implies that \\(E[\\hat{\\beta}|\\boldsymbol{x}] = \\beta\\).\n\n\n\\[\n\\begin{aligned}\nE[\\hat{\\beta}|\\boldsymbol{X}] &= E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E[\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{X}\\beta\\\\\n&= \\beta\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nAssumption 2\n\n\n\nConstant Variance: \\(\\text{var}(\\varepsilon_i | \\boldsymbol{X}) = \\sigma^2\\) for all \\(i\\),\nand uncorrelated errors: \\(\\text{cov}(\\varepsilon_{i}, \\varepsilon_j) = 0\\) for all pairs \\(i, j\\) where \\(i \\neq j\\).\nEquivalently, the two statements above can be written together concisely in matrix form:\n\\[\n\\text{var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\boldsymbol{I}\n\\]\n\n\n\nExercise\n\n\nShow that assumption 2 implies that \\(\\text{var}(\\hat{\\beta}|\\boldsymbol{X}) = \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#interpreting-variance-of-hatbeta-in-simple-linear-regression",
    "href": "notes/lec08-inference-2.html#interpreting-variance-of-hatbeta-in-simple-linear-regression",
    "title": "Inference part II",
    "section": "Interpreting variance of \\(\\hat{\\beta}\\) in simple linear regression",
    "text": "Interpreting variance of \\(\\hat{\\beta}\\) in simple linear regression\n\n\\(\\hat{\\beta}_1\\)\nConsider simple linear regression, where we have 1 predictor variable \\(\\boldsymbol{x} = x_1, \\ldots, x_n\\) and \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\).\nLet‚Äôs compute the variance of \\(\\hat{\\beta}_1\\):\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\n&= \\text{var}\n\\left(\n\\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\big| \\boldsymbol{x}\n\\right)\\\\\n&=\n\\frac{1}{\\left(\\sum_{i=1}^n (x_i - \\bar{x})^2\\right)^2} \\cdot \\sum_{i=1}^n (x_i - \\bar{x})^2 \\text{var}(y_i)\\\\\n&= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\end{aligned}\n\\]\nThe second equality above follows from the fact that (1) \\(x_1, \\ldots, x_n\\) are constant and (2) the \\(y_i\\)‚Äôs are uncorrelated.\nWe can multiply by ‚Äú1‚Äù in a fancy way to rearrange slightly:\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x}) &=\n\\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\cdot \\frac{1/n}{1/n}\\\\\n&= \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})}\n\\end{aligned}\n\\]\nwhere the last equality follows from the definition of the variance.\n\n\n\n\n\n\nImportant\n\n\n\nThe important take-away point here is that the variance of \\(\\hat{\\beta}_1\\) depends on - the variance of the error, \\(\\sigma^2\\) - the number of samples, \\(n\\), and - the variance of \\(x_1, \\ldots, x_n\\)\n\n\n\n\n\\(\\hat{\\beta}_0\\)\nRecall that \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\). Therefore,\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x})\n&=\n\\text{var}(\\bar{y} - \\hat{\\beta}_1 \\bar{x}| \\boldsymbol{x})\\\\\n&=\n\\text{var}(\\bar{y} | \\boldsymbol{x}) + \\bar{x}^2 \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x}) - 2 \\bar{x} \\text{cov}(\\bar{y}, \\hat{\\beta}_1 | \\boldsymbol{x})\n\\\\\n&= \\text{var}\\left(\n\\frac{1}{n} \\sum y_i \\Big|\\boldsymbol{x}\n\\right) +\n\\bar{x}^2 \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})} - 0\\\\\n&= \\frac{1}{n} \\sigma^2  + \\bar{x}^2 \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})}\\\\\n&= \\frac{1}{n} \\sigma^2 \\left(\n1 + \\frac{\\bar{x}^2}{\\text{var}(\\boldsymbol{x})}\n\\right)\n\\end{aligned}\n\\]\nwhere the second inequality follows from the first exercise in these notes. The third equality follows from the fact that \\(\\text{cov}(\\bar{y}, \\hat{\\beta}_1 | \\boldsymbol{x})\\) is 0. To see this,\n\n\n\n\n\n\nclick here\n\n\n\n\n\nFor notational convenience, I‚Äôll drop the writing of the conditioning on \\(\\boldsymbol{x}\\).\n\\[\n\\begin{aligned}\n\\text{cov}(\\bar{y}, \\hat{\\beta}_1)\n&= \\text{cov}\\left(\n\\frac{1}{n}\\sum_{j=1}^n y_j,\\sum_{i=1}^n w_iy_i\n\\right)\n\\end{aligned}\n\\] where\n\\(w_i = \\frac{(x_i - \\bar{x})}{\\sum(x_k - \\bar{x})^2}\\). Note \\(\\sum_i w_i = 0\\).\nContinuing the proof,\n\\[\n\\text{cov}\\left(\n\\frac{1}{n}\\sum_{j=1}^n y_j,\\sum_{i=1}^n w_iy_i\n\\right) =\n\\frac{1}{n} \\sum_j \\sum_i w_i \\text{cov}(y_j, y_i)\n\\]\nNotice \\(\\text{cov}(y_i, y_j) = 0\\) for \\(i \\neq j\\). If \\(i = j\\), then \\(\\text{cov}(y_j, y_i) = \\text{var}(y_i) = \\sigma^2\\).\nThis let‚Äôs us simplify:\n\\[\n\\frac{1}{n} \\sigma^2 \\sum_i w_i = 0\n\\]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe important take-away point here is that \\(\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x}) \\geq \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\\)\nQuestion: under what circumstance does \\(\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x}) = \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\\)?"
  },
  {
    "objectID": "notes/lecture-notes-playground.html",
    "href": "notes/lecture-notes-playground.html",
    "title": "lecture-notes-playground",
    "section": "",
    "text": "library(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# helper to add a vector with a large diamond tip\nadd_vector &lt;- function(fig, x, y, z, color, name=NULL) {\n  fig %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines+markers\",\n      x = c(0, x),\n      y = c(0, y),\n      z = c(0, z),\n      line = list(width = 6, color = color),\n      marker = list(\n        size = c(1, 12),  # &lt;-- make diamond much larger\n        color = c(color, color),\n        symbol = c(\"circle\", \"diamond\")\n      ),\n      name = name,\n      showlegend = TRUE\n    )\n}\n\n# build plot with all six vectors\nfig &lt;- plot_ly() %&gt;%\n  add_vector(1, 1, 1, \"blue\", \"(1,1,1)\") %&gt;%\n  add_vector(2, 0, 0, \"green\", \"(2,0,0)\") %&gt;%\n  add_vector(0, 1, 0, \"red\", \"(0,1,0)\") %&gt;%\n  add_vector(1, 0, 0, \"purple\", \"(1,0,0)\") %&gt;%\n  add_vector(0, 2, 0, \"orange\", \"(0,2,0)\") %&gt;%\n  add_vector(0, 0, 2, \"brown\", \"(0,0,2)\") %&gt;%\n  layout(\n    scene = list(\n      xaxis = list(title = \"X\"),\n      yaxis = list(title = \"Y\"),\n      zaxis = list(title = \"Z\"),\n      camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8))\n    )\n  )\n\nfig\n\n\n\n\n\n\n{r} # # install.packages(\"plotly\") # library(plotly) #  # # --- small helpers ----------------------------------------------------------- #  # # cross product (no extra packages) # vcross &lt;- function(a, b) { #   c(a[2]*b[3] - a[3]*b[2], #     a[3]*b[1] - a[1]*b[3], #     a[1]*b[2] - a[2]*b[1]) # } #  # # Rodrigues' rotation matrix to rotate vector a onto b # rot_from_a_to_b &lt;- function(a, b) { #   a &lt;- a / sqrt(sum(a^2)) #   b &lt;- b / sqrt(sum(b^2)) #   v &lt;- vcross(a, b) #   s &lt;- sqrt(sum(v^2)) #   cth &lt;- sum(a * b) #   I3 &lt;- diag(3) #   if (s &lt; 1e-12) { #     # parallel or anti-parallel #     if (cth &gt; 0) return(I3)                 # already aligned #     # 180¬∞: rotate around any axis ‚üÇ to a; x-axis works since a = (0,0,¬±1) #     return(matrix(c(1,0,0, 0,-1,0, 0,0,-1), nrow=3, byrow=TRUE)) #   } #   K &lt;- matrix(c(  0,   -v[3],  v[2], #                  v[3],   0,   -v[1], #                 -v[2],  v[1],   0), nrow=3, byrow=TRUE) #   I3 + K + K %*% K * ((1 - cth) / (s^2)) # } #  # # Build a cone mesh at `tip` pointing along `dir`. # # The cone is defined in local coords as: tip at origin, base at z = -length. # cone_mesh &lt;- function(tip, dir, length = 0.15, radius = 0.06, n = 24) { #   u &lt;- as.numeric(dir) #   if (sqrt(sum(u^2)) &lt; 1e-12) stop(\"Direction for cone is zero-length.\") #   u &lt;- u / sqrt(sum(u^2)) #  #   # Local cone vertices: tip + circular base in plane z = -length #   theta &lt;- seq(0, 2*pi, length.out = n + 1)[- (n + 1)]  # n points, exclude duplicate #   tip_local  &lt;- c(0, 0, 0) #   base_local &lt;- rbind(radius * cos(theta), radius * sin(theta), -length) #  #   # Rotate from local +Z axis to u #   R &lt;- rot_from_a_to_b(c(0,0,1), u) #   tip_world  &lt;- R %*% tip_local #   base_world &lt;- R %*% base_local #  #   # Translate to desired tip position #   tip_world  &lt;- tip_world  + tip #   base_world &lt;- sweep(base_world, 2, tip, `+`) #  #   # Assemble vertices: tip first, then base ring #   verts &lt;- cbind(tip_world, base_world)  # 3 x (1+n) #   x &lt;- verts[1,]; y &lt;- verts[2,]; z &lt;- verts[3,] #  #   # Triangles for the side surface: (tip, base_i, base_{i+1}) #   # Indices for plotly mesh3d are 0-based. #   tip_idx  &lt;- 0 #   base_idx &lt;- 1:(n)  # 1-based in R #   i_idx &lt;- rep(tip_idx, n) #   j_idx &lt;- base_idx #   k_idx &lt;- c(base_idx[-1], 1) #  #   list( #     x = x, y = y, z = z, #     i = i_idx, j = j_idx, k = k_idx #   ) # } #  # # Add one vector (line) + cone head to a plotly figure # add_vector_with_cone &lt;- function(fig, x, y, z, color = \"blue\", #                                  shaft_width = 6, #                                  cone_len = 0.15, cone_radius = 0.06, cone_n = 24, #                                  name = NULL) { #   tip &lt;- c(x, y, z) #   dir &lt;- c(x, y, z) #  #   # line (shaft) #   fig &lt;- fig %&gt;% #     add_trace( #       type = \"scatter3d\", mode = \"lines\", #       x = c(0, x), y = c(0, y), z = c(0, z), #       line = list(width = shaft_width, color = color), #       name = name %||% sprintf(\"(%g,%g,%g)\", x, y, z), #       showlegend = TRUE #     ) #  #   # cone head #   m &lt;- cone_mesh(tip = tip, dir = dir, length = cone_len, radius = cone_radius, n = cone_n) #   fig %&gt;% #     add_trace( #       type = \"mesh3d\", #       x = m$x, y = m$y, z = m$z, #       i = m$i, j = m$j, k = m$k, #       color = color, #       opacity = 1, #       showscale = FALSE, #       lighting = list(ambient = 0.5, diffuse = 0.8, specular = 0.2), #       hoverinfo = \"skip\", #       name = name %||% sprintf(\"(%g,%g,%g) head\", x, y, z), #       showlegend = FALSE #     ) # } #  # `%||%` &lt;- function(a, b) if (is.null(a)) b else a #  # # --- demo: your six vectors with true cone heads ----------------------------- #  # fig &lt;- plot_ly() #  # fig &lt;- fig %&gt;% #   add_vector_with_cone(1, 1, 1, color = \"blue\",  name = \"(1,1,1)\") %&gt;% #   add_vector_with_cone(2, 0, 0, color = \"green\", name = \"(2,0,0)\") %&gt;% #   add_vector_with_cone(0, 1, 0, color = \"red\",   name = \"(0,1,0)\") %&gt;% #   add_vector_with_cone(1, 0, 0, color = \"purple\",name = \"(1,0,0)\") %&gt;% #   add_vector_with_cone(0, 2, 0, color = \"orange\",name = \"(0,2,0)\") %&gt;% #   add_vector_with_cone(0, 0, 2, color = \"brown\", name = \"(0,0,2)\") #  # fig &lt;- fig %&gt;% #   layout( #     scene = list( #       xaxis = list(title = \"X\"), #       yaxis = list(title = \"Y\"), #       zaxis = list(title = \"Z\"), #       camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8)) #     ) #   ) #  # fig #  #"
  },
  {
    "objectID": "prepare/prepare-lec02.html",
    "href": "prepare/prepare-lec02.html",
    "title": "Prepare: simple linear regression",
    "section": "",
    "text": "üìñ Read Simple linear regression, Section 4.1 - 4.6\nüìñ Read R for Data Science, Introduction: What you will learn\nüìñ Read GitHub for supporting, reusing, contributing, and failing safely\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "prepare/prepare-lec04.html",
    "href": "prepare/prepare-lec04.html",
    "title": "Prepare: matrix representation",
    "section": "",
    "text": "Review linear algebra concepts (as needed)\n\nMatrices and vectors: [slides][video]\nMatrix-Vector products: [slides][video]\nMatrix multiplication: [slides][video]\n\n\n\n\n\n\n\nNote\n\n\n\nAll linear algebra review materials from Math 218: Matrices and Vectors (Summer 2024) taught by Dr.¬†Brian Fitzpatrick at Duke University"
  },
  {
    "objectID": "prepare/prepare-lec08.html",
    "href": "prepare/prepare-lec08.html",
    "title": "Prepare for Lecture 08: Inference for regression",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "prepare/prepare-lec20.html",
    "href": "prepare/prepare-lec20.html",
    "title": "Prepare for Lecture 20: Logistic regression - Prediction",
    "section": "",
    "text": "üìñ Classification module in Google Machine Learning Crash Course"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Research topics due Tuesday September 30 at 5:00pm\nProject proposal due Wednesday October 15 at 5:00pm\nExploratory data analysis due Tuesday November 4\nProject presentations (in lab) Wednesday November 12\nDraft report due Tuesday November 18 at 5:00pm\nPeer review Wednesday November 19, in lab\nFinal written report + reproducible GitHub repository due December 12 at 5:00pm."
  },
  {
    "objectID": "project.html#timeline",
    "href": "project.html#timeline",
    "title": "Final project",
    "section": "",
    "text": "Research topics due Tuesday September 30 at 5:00pm\nProject proposal due Wednesday October 15 at 5:00pm\nExploratory data analysis due Tuesday November 4\nProject presentations (in lab) Wednesday November 12\nDraft report due Tuesday November 18 at 5:00pm\nPeer review Wednesday November 19, in lab\nFinal written report + reproducible GitHub repository due December 12 at 5:00pm."
  },
  {
    "objectID": "project.html#description",
    "href": "project.html#description",
    "title": "Final project",
    "section": "Description",
    "text": "Description\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible."
  },
  {
    "objectID": "project.html#deliverables",
    "href": "project.html#deliverables",
    "title": "Final project",
    "section": "Deliverables",
    "text": "Deliverables\nYou will work on the project with your lab groups. The primary deliverables for the project are\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na summary of your project highlights to share with the class\na GitHub repository containing all work from the project\n\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the primary deliverables.\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the primary deliverables."
  },
  {
    "objectID": "project.html#research-topics",
    "href": "project.html#research-topics",
    "title": "Final project",
    "section": "Research topics",
    "text": "Research topics\nThe goal of this milestone is to discuss topics and develop potential research questions your team is interested in investigating for the project. You are only developing ideas at this point; you do not need to have a data set identified right now.\nDevelop three potential research topics. Include the following for each topic:\n\nA brief description of the topic\nA statement about your motivation for investigating this topic\nThe potential audience(s), i.e., who might be most interested in this research?\nTwo or three potential research questions you could analyze about this topic. (Note: These are draft questions at this point. You will finalize the questions in the next stage of the project.)\nIdeas about the type of data you might use to answer this question or potential data sets you‚Äôre interested in using. (Note: The goal is to generate ideas at this point, so it is fine if you have not identified any particular data sets at this point.)\n\nWrite your responses in research-topics.qmd in your team‚Äôs project GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Tuesday, September 30 at 5:00pm. There is no Gradescope submission."
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is for your team to identify the data set you‚Äôre interested in analyzing to investigate one of your potential research topics. You will also do some preliminary exploration of the response variable and begin thinking about the modeling strategy. If you‚Äôre unsure where to find data, you can use the list of potential data sources here as a starting point.\n\n\n\n\n\n\nImportant\n\n\n\nYou must use the data set(s) in the proposal for the final project, unless instructed otherwise when given feedback.\n\n\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as ‚Äúname‚Äù, ‚ÄúID number‚Äù, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials."
  },
  {
    "objectID": "notes/lec10-hypothesis.html",
    "href": "notes/lec10-hypothesis.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT) # datatable viewing\nlibrary(patchwork)\nlibrary(knitr)\n\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\",\n  fig.align = \"center\"\n)\n\nset.seed(221)\n\nfootball &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/ncaa-football-exp.csv\") |&gt;\n  mutate(nonsense = runif(n(), 0, 10))"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#data-ncaa-football-expenditures",
    "href": "notes/lec10-hypothesis.html#data-ncaa-football-expenditures",
    "title": "Hypothesis testing",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday‚Äôs data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 (2019 - 2020 season) expenditures on football for institutions in the NCAA - Division 1 FBS (Football Bowl Subdivision). The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\nnonsense: a created variable (see above) which has nothing to do with expenditure"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#univariate-eda",
    "href": "notes/lec10-hypothesis.html#univariate-eda",
    "title": "Hypothesis testing",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#bivariate-eda",
    "href": "notes/lec10-hypothesis.html#bivariate-eda",
    "title": "Hypothesis testing",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#regression-model",
    "href": "notes/lec10-hypothesis.html#regression-model",
    "title": "Hypothesis testing",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type + nonsense, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#from-sample-to-population",
    "href": "notes/lec10-hypothesis.html#from-sample-to-population",
    "title": "Hypothesis testing",
    "section": "From sample to population",
    "text": "From sample to population\nFor every additional 1,000 students, we expect an institution‚Äôs total expenditures on football to increase by $780,000, on average, holding institution type constant.\n\nThis estimate is valid for the single sample of 127 higher education institutions in the 2019 - 2020 academic year.\nBut what if we‚Äôre not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?\nWhat if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#varhatboldsymbolbeta-for-ncaa-data",
    "href": "notes/lec10-hypothesis.html#varhatboldsymbolbeta-for-ncaa-data",
    "title": "Hypothesis testing",
    "section": "\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data",
    "text": "\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\nsigma_sq &lt;- glance(exp_fit)$sigma^2\n\nvar_beta &lt;- sigma_sq * solve(t(X) %*% X)\nvar_beta\n\n              (Intercept) enrollment_th typePublic     nonsense\n(Intercept)    12.4139465  -0.170593886 -5.4231006 -0.692309267\nenrollment_th  -0.1705939   0.012597357 -0.1315619  0.007350219\ntypePublic     -5.4231006  -0.131561941 10.1018139 -0.136025423\nnonsense       -0.6923093   0.007350219 -0.1360254  0.137611482"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#sehatboldsymbolbeta-for-ncaa-data",
    "href": "notes/lec10-hypothesis.html#sehatboldsymbolbeta-for-ncaa-data",
    "title": "Hypothesis testing",
    "section": "\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data",
    "text": "\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n\n\n\n\n\n\nsqrt(diag(var_beta))\n\n  (Intercept) enrollment_th    typePublic      nonsense \n    3.5233431     0.1122379     3.1783351     0.3709602"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#example",
    "href": "notes/lec10-hypothesis.html#example",
    "title": "Hypothesis testing",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#compute-p-value",
    "href": "notes/lec10-hypothesis.html#compute-p-value",
    "title": "Hypothesis testing",
    "section": "Compute p-value",
    "text": "Compute p-value\n\nn &lt;- nrow(football)\np &lt;- 4\n2 * (1 - pt(abs(0.803), n - p))\n\n[1] 0.4235237\n\n\nVisually,"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#t-vs.-n01",
    "href": "notes/lec10-hypothesis.html#t-vs.-n01",
    "title": "Hypothesis testing",
    "section": "t vs.¬†N(0,1)",
    "text": "t vs.¬†N(0,1)\n\n\n\n\n\n\n\n\nFigure¬†1: Standard normal vs.¬†t distributions"
  },
  {
    "objectID": "notes/lec11-ci.html",
    "href": "notes/lec11-ci.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT) # datatable viewing\nlibrary(patchwork)\nlibrary(knitr)\n\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\",\n  fig.align = \"center\"\n)\n\nset.seed(221)\n\nfootball &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/ncaa-football-exp.csv\") |&gt;\n  mutate(nonsense = runif(n(), 0, 10))"
  },
  {
    "objectID": "notes/lec11-ci.html#data-ncaa-football-expenditures",
    "href": "notes/lec11-ci.html#data-ncaa-football-expenditures",
    "title": "Confidence intervals",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nSame data as before (reminder):\nToday‚Äôs data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 (2019 - 2020 season) expenditures on football for institutions in the NCAA - Division 1 FBS (Football Bowl Subdivision). The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\nnonsense: a created variable (see above) which has nothing to do with expenditure"
  },
  {
    "objectID": "notes/lec11-ci.html#regression-model",
    "href": "notes/lec11-ci.html#regression-model",
    "title": "Confidence intervals",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type + nonsense, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423"
  },
  {
    "objectID": "notes/lec11-ci.html#confidence-interval-in-r",
    "href": "notes/lec11-ci.html#confidence-interval-in-r",
    "title": "Confidence intervals",
    "section": "Confidence interval in R",
    "text": "Confidence interval in R\nWe can compute the confidence intervals in R easily:\n\n# alpha = 0.05; CI = 95%\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n10.859\n24.807\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n0.574\n1.018\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n-19.812\n-7.229\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n-0.436\n1.032\n\n\n\n\n\nWe can verify manually. For example, for the intercept:\n\nn = nrow(football)\np = 4 # 4 entries in beta (dimension p)\n17.833 - (3.523 * qt(0.975, df = n - p))\n\n[1] 10.85944\n\n17.833 + (3.523 * qt(0.975, df = n - p))\n\n[1] 24.80656\n\n\nNotice that\n\nqt(0.975, df = n - p)\n\n[1] 1.979439\n\n\nis close to the number 2.\nWe could approximate the 95% CI:\n\n17.833 - (3.523 * 2)\n\n[1] 10.787\n\n17.833 + (3.523 * 2)\n\n[1] 24.879\n\n\nQuestion: when is this approximation valid?"
  },
  {
    "objectID": "notes/lec11-ci.html#prediction-interval",
    "href": "notes/lec11-ci.html#prediction-interval",
    "title": "Confidence intervals",
    "section": "Prediction interval",
    "text": "Prediction interval\nFor a public college with an enrollment of 50,000 and a nonsense variable of 9, what is the predicted 95% expenditure interval?\nPrediction:\n\nxstar_df &lt;- tibble(\n  enrollment_th = 50,\n  nonsense = 9,\n  type = \"Public\"\n)\npredict(exp_fit, newdata = xstar_df, interval = \"prediction\",\n        level = 0.95)\n\n     fit      lwr     upr\n1 46.811 22.54961 71.0724\n\n\n\nExerciseSolution\n\n\nCompute the CI manually.\n\n\n\\(\\hat{y}_*\\):\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\n\ny = football$total_exp_m\n\nbetaHat = solve(t(X) %*% X) %*% t(X) %*% y\n\nxstar = matrix(c(1, 50, 1, 9), ncol = 1)\nprediction = t(xstar) %*% betaHat\nprediction\n\n       [,1]\n[1,] 46.811\n\n\nConfidence interval:\n\nsigma_hat &lt;- glance(exp_fit)$sigma\nt &lt;- qt(0.975, df = n - p)\nse = sigma_hat * sqrt(1 + t(xstar) %*% solve(t(X) %*% X) %*% xstar)\n\nprediction - (se*t)\n\n         [,1]\n[1,] 22.54961\n\nprediction + (se*t)\n\n        [,1]\n[1,] 71.0724"
  },
  {
    "objectID": "labs/lab04.html",
    "href": "labs/lab04.html",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "",
    "text": "Important\n\n\n\nThis lab will not be submitted for grade."
  },
  {
    "objectID": "labs/lab04.html#exploratory-data-analysis",
    "href": "labs/lab04.html#exploratory-data-analysis",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\np1 &lt;- ggplot(data = tips, aes(x = Tip)) + \n  geom_histogram(color = \"white\", binwidth = 2) +\n  labs(x = \"Tips ($)\",\n       title = \"Tips at local restaurant\")\n\np2 &lt;- ggplot(data = tips, aes(x = Party)) + \n  geom_histogram(color = \"white\") +\n  labs(x = \"Party\",\n       title = \"Number of diners in party\") +\n  xlim(c(0, 7))\n\np3 &lt;- ggplot(data = tips, aes(x = Age)) + \n  geom_bar(color = \"white\") +\n  labs(x = \"\",\n       title = \"Age of Payer\") \n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\n\np4 &lt;- ggplot(data = tips, aes(x = Party, y = Tip)) + \n  geom_jitter() + \n  labs(x = \"Number of diners in party\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Party\")\n\np5 &lt;- ggplot(data = tips, aes(x = Age, y = Tip)) + \n  geom_boxplot() + \n  labs(x = \"Age of payer\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Age\")\n\np4 + p5\n\n\n\n\n\n\n\n\nWe will use the number of diners in the party and age of the payer to understand variability in the tips."
  },
  {
    "objectID": "labs/lab04.html#exercise-1",
    "href": "labs/lab04.html#exercise-1",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe will start with the main effects model that includes Age and Party.\n\nHow many indicator variables for Age can we create from the data?\nHow many indicator variables for Age will be in the regression model?\nAre the responses to parts a and b equal? If not, explain why not.\nWhich of the following is true for this model? Select all that apply.\n\nThe intercepts are the same for every level of Age.\nThe intercepts differ by Age.\nThe effect of Party is the same for every level of Age.\nThe effect of Party differs by Age."
  },
  {
    "objectID": "labs/lab04.html#exercise-2",
    "href": "labs/lab04.html#exercise-2",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider the main effects model that includes Age and Party.\n\nWhat is the dimension of the design matrix \\(\\mathbf{X}\\) for the main effects model?\nCalculate the coefficient estimates \\(\\hat{\\boldsymbol{\\beta}}\\) directly from the data (without using lm), but you can use stats::model.matrix() to grab the X matrix.\nWrite the equation of the estimated regression model.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-3",
    "href": "labs/lab04.html#exercise-3",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 3",
    "text": "Exercise 3\nConsider the main effects model that includes Age and Party. Get \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the data.\n\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(\\hat{\\sigma}_{\\epsilon}\\) .\nInterpret \\(\\hat{\\sigma}_\\epsilon\\) in the context of the data.\nCompute \\(Var(\\hat{\\boldsymbol{\\beta}})\\).\nYou wish to test whether there is a linear relationship between tips and the number of diners in the party, after adjusting for the age of the payer. Compute the test statistic.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-4",
    "href": "labs/lab04.html#exercise-4",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 4",
    "text": "Exercise 4\nConsider the main effects model that includes Age and Party. Get \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the data.\n\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(R^2\\). Interpret this value in the context of the data.\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(RMSE\\). Interpret this value in the context of the data.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-5",
    "href": "labs/lab04.html#exercise-5",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou decide to add an interaction effect between Age and Party to the model and fit a model of the following form:\n\\[\nTip_i = \\beta_0 + \\beta_1Party_i + \\beta_2SenCit_i + \\beta_3Yadult_i + \\beta_4Party_i \\times SenCit_i + \\beta_5 Party_i \\times Yadult_i + \\epsilon_i\n\\]\n\nWhich of the following is true for this model? Select all that apply.\n\nThe intercepts are the same for every level of Age.\nThe intercepts differ by Age.\nThe effect of Party is the same for every level of Age.\nThe effect of Party differs by Age.\n\nBy how much does the intercept for tables with young adult payers differ from tables with middle age payers? Write the answer in terms of the \\(\\beta\\)‚Äôs.\nWrite the equation of the model for tables in which the payer is a senior citizen.\nSuppose you wish to test the hypotheses: \\(H_0: \\beta_5 = 0 \\text{ vs. }H_a: \\beta_5 \\neq 0\\) . State what is being tested in terms of the effect of Party."
  },
  {
    "objectID": "labs/lab04.html#exercise-6",
    "href": "labs/lab04.html#exercise-6",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the lm() function to fit the model that includes Age and Party and the interaction between the two variables. Display the 90% confidence interval for the coefficients.\n\nThe standard error for AgeSenCit is 0.784. State what this value means in the context of the data.\nWrite code to show how the 90% confidence interval for AgeSenCit was computed.\nBased on the confidence interval, is there evidence that tables with senior citizen payers tip differently on average than tables with middle age payers?"
  },
  {
    "objectID": "labs/lab04.html#exercise-7",
    "href": "labs/lab04.html#exercise-7",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 7",
    "text": "Exercise 7\nThe following are general questions about regression. They are not specific to the tips data set.\n\nWhat does it mean for an estimator to be the ‚Äúleast-squares‚Äù estimator?\nConsider the derivation of the least-squares estimator:\n\\[\n\\begin{aligned}\n\\nabla_{\\beta}\\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} &= \\nabla_{\\boldsymbol{\\beta}}[(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})] \\\\[5pt]\n&=\\nabla_{\\boldsymbol{\\beta}}[\\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}] \\\\[5pt]\n&=\\nabla_{\\boldsymbol{\\beta}}[\\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}]\\\\[5pt]\n& = -2\\mathbf{X}^\\mathsf{T}\\mathbf{y} + 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} \\\\[5pt]\n&\\Rightarrow \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\n\\end{aligned}\n\\]\n\nExplain how \\(-\\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) is simplified to \\(-2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) when going from lines 2 to 3.\n\nExplain what rules were used to compute the gradient in line 4."
  },
  {
    "objectID": "labs/lab04.html#exercise-8",
    "href": "labs/lab04.html#exercise-8",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 8",
    "text": "Exercise 8\nBelow we show how SSR = \\(\\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\)\n\\[\n\\begin{aligned}\nSSR = \\mathbf{e}^\\mathsf{T}\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{\\beta}}) \\\\[5pt]\n& = \\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\mathbf{y}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[5pt]\n&= \\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}+\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\[5pt]\n& = \\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\n\\end{aligned}\n\\]\na. Explain how \\(-\\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) is simplified to \\(-2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) when going from lines 2 to 3.\nb. Explain how we know \\(\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} = \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) when going from lines 3 to 4."
  },
  {
    "objectID": "labs/lab04.html#footnotes",
    "href": "labs/lab04.html#footnotes",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. ‚ÄúThe Effects of Credit Cards on Tipping.‚Äù Project for Statistics 212-Statistics for the Sciences, St.¬†Olaf College.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#packages-in-these-notes",
    "href": "labs/slides/lab-interactions.html#packages-in-these-notes",
    "title": "Interaction effects",
    "section": "Packages in these notes",
    "text": "Packages in these notes\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidymodels)"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-terms",
    "href": "labs/slides/lab-interactions.html#interaction-terms",
    "title": "Interaction effects",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#example-penguins",
    "href": "labs/slides/lab-interactions.html#example-penguins",
    "title": "Interaction effects",
    "section": "Example: penguins",
    "text": "Example: penguins\nThe relationship between penguin bill length and body mass depends on the penguin‚Äôs island.\n\nplotcodediscuss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g, color = island)) +\n  geom_point() +\n  theme_bw() +\n  geom_smooth(method = 'lm', se = F) +\n  labs(x = \"Bill length (mm)\", \n       y = \"Body mass (g)\", \n       title = \"Island interaction effect\")\n\n\n\nThe lines are not parallel indicating there is a potential interaction effect. The slope for bill length differs based on the penguin‚Äôs island."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-term-in-model",
    "href": "labs/slides/lab-interactions.html#interaction-term-in-model",
    "title": "Interaction effects",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\npenguin_fit &lt;- lm(body_mass_g ~ bill_length_mm * island,\n      data = penguins)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1726.02\n292.08\n-5.91\n0\n\n\nbill_length_mm\n142.34\n6.42\n22.18\n0\n\n\nislandDream\n4478.69\n395.31\n11.33\n0\n\n\nislandTorgersen\n2870.56\n777.69\n3.69\n0\n\n\nbill_length_mm:islandDream\n-120.60\n8.77\n-13.75\n0\n\n\nbill_length_mm:islandTorgersen\n-76.57\n19.53\n-3.92\n0"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-term-in-model-1",
    "href": "labs/slides/lab-interactions.html#interaction-term-in-model-1",
    "title": "Interaction effects",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nExerciseSolution\n\n\n\n\nWrite the regression equation for penguins from island Torgersen\nWrite the regression equation for penguins from island Dream\nWrite the regression equation for penguins from island Biscoe\n\n\n\n\nPenguins equations for each island:\n\\[\n\\begin{aligned}\n\\hat{y}_{\\text{torgersen}} &= (-1726.02 + 2870.56)  + (142.34 - 76.57)x_1\\\\\n&= 1144.54 + 65.77x_1\\\\\n\\hat{y}_{\\text{dream}} &= (-1726.02 + 4478.69   )   + (142.34 - 120.60)x_1\\\\\n&= 2752.67 + 21.74x_1\\\\\n\\hat{y}_{\\text{biscoe}} &= -1726.02 + 142.34 x_1\n\\end{aligned}\n\\]\nwhere \\(\\hat{y}\\) is the predicted body mass for a given bill length \\(x_1\\)."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interpreting-interaction-terms",
    "href": "labs/slides/lab-interactions.html#interpreting-interaction-terms",
    "title": "Interaction effects",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\nWhat the interaction means:\n\nThe effect of bill length on body mass differs by -76.5 when the penguin comes from island Torgersen instead of Biscoe.\nThe effect of bill length on body mass differs by -120 when the penguin comes from island Dream instead of Biscoe."
  },
  {
    "objectID": "notes/lec10-hypothesis.html#manually-computing-the-standard-error",
    "href": "notes/lec10-hypothesis.html#manually-computing-the-standard-error",
    "title": "Hypothesis testing",
    "section": "Manually computing the standard error",
    "text": "Manually computing the standard error\n\n\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\nsigma_sq &lt;- glance(exp_fit)$sigma^2\n\nvar_beta &lt;- sigma_sq * solve(t(X) %*% X)\nvar_beta\n\n              (Intercept) enrollment_th typePublic     nonsense\n(Intercept)    12.4139465  -0.170593886 -5.4231006 -0.692309267\nenrollment_th  -0.1705939   0.012597357 -0.1315619  0.007350219\ntypePublic     -5.4231006  -0.131561941 10.1018139 -0.136025423\nnonsense       -0.6923093   0.007350219 -0.1360254  0.137611482\n\n\n\n\n\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n\n\n\n\n\n\nsqrt(diag(var_beta))\n\n  (Intercept) enrollment_th    typePublic      nonsense \n    3.5233431     0.1122379     3.1783351     0.3709602"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#p-value",
    "href": "notes/lec10-hypothesis.html#p-value",
    "title": "Hypothesis testing",
    "section": "p-value",
    "text": "p-value\nA p-value is: (1) a tail probability of a statistic under the null hypothesis, (2) the lowest value of \\(\\alpha\\) such that \\(H_0\\) is rejected, (3) \\(Pr(|t| &gt; t_{\\beta_j (obs)})\\).\nIn R we can compute it: \\(2 \\times\\) (1 - pt(abs(t), n - p))."
  }
]