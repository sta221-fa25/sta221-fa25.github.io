[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis: Theory and Applications",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nNotes\nAssignment\n\n\n\n\n1\nTue Aug 26\nwelcome\n\nüíª\n\n\n\n\nWed Aug 27\nlab: hello git and R\n\nüíª\nlab 0\n\n\n\nThu Aug 28\ncorrelation\nüìñ\nüóí\n\n\n\n2\nTue Sep 02\nsimple linear regression\n\nüóí\n\n\n\n\nWed Sep 03\nlab: computing and linear algebra review\n\n\nlab 1\n\n\n\nThu Sep 04\nmodel assessment\nüìñ\nüóí\n\n\n\n3\nTue Sep 09\nmatrix representation\nüìñ\n\nhw 1\n\n\n\nWed Sep 10\nlab: linear regression\n\n\nlab 2\n\n\n\nThu Sep 11\nmultiple linear regression\n\nüóí üìù\n\n\n\n4\nTue Sep 16\nintro to inference\nüìñ\nüóí\n\n\n\n\nWed Sep 17\nlab: multiple linear regression\n\n\nlab 3\n\n\n\nThu Sep 18\ninference continued\n\n\n\n\n\n5\nTue Sep 23\n\n\n\n\n\n\n\nWed Sep 24\n\n\n\n\n\n\n\nThu Sep 25\n\n\n\n\n\n\n6\nTue Sep 30\n\n\n\n\n\n\n\nWed Oct 01\n\n\n\n\n\n\n\nThu Oct 02\nexam review\n\n\n\n\n\n7\nTue Oct 07\nexam 1\n\n\n\n\n\n\nWed Oct 08\n\n\n\n\n\n\n\nThu Oct 09\n\n\n\n\n\n\n8\nTue Oct 14\nNO CLASS\n\n\n\n\n\n\nWed Oct 15\n\n\n\n\n\n\n\nThu Oct 16\n\n\n\n\n\n\n9\nTue Oct 21\n\n\n\n\n\n\n\nWed Oct 22\n\n\n\n\n\n\n\nThu Oct 23\n\n\n\n\n\n\n10\nTue Oct 28\n\n\n\n\n\n\n\nWed Oct 29\n\n\n\n\n\n\n\nThu Oct 30\n\n\n\n\n\n\n11\nTue Nov 04\n\n\n\n\n\n\n\nWed Nov 05\n\n\n\n\n\n\n\nThu Nov 06\n\n\n\n\n\n\n12\nTue Nov 11\n\n\n\n\n\n\n\nWed Nov 12\nlab: project presentations\n\n\n\n\n\n\nThu Nov 13\n\n\n\n\n\n\n13\nTue Nov 18\n\n\n\n\n\n\n\nWed Nov 19\n\n\n\n\n\n\n\nThu Nov 20\n\n\n\n\n\n\n14\nTue Nov 25\n\n\n\n\n\n\n\nWed Nov 26\nNO CLASS\n\n\n\n\n\n\nThu Nov 27\nNO CLASS\n\n\n\n\n\n15\nTue Dec 02\nexam review\n\n\n\n\n\n\nWed Dec 03\n\n\n\n\n\n\n\nThu Dec 04\nexam 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 221: Regression Analysis",
    "section": "",
    "text": "Teaching team & office hours\n\n\n\n\nContact\nOffice hours\nLocation\n\n\n\n\nDr.¬†Alexander Fisher\naaf29@duke.edu\nTu/Fr: 2-3pm\nOld Chem 223B\n\n\nCathy Lee\npin.chian.lee@duke.edu\nTh: 1-3pm\nZoom (see Canvas for link)\n\n\nXukun Zhu\nxukun.zhu@duke.edu\nWe 4:30-6:30pm\nOld Chem 025\n\n\nKrish Bansal\nkrish.bansal@duke.edu\nMo: 1:30-2:30pm / We: 12-1pm\nOld Chem 203A\n\n\n\n\n\n\n\nMeetings\n\n\n\nLecture\nTu/Th 11:45am - 1:00pm\nOld Chemistry 116\n\n\nLab 01\nWe 1:25pm - 2:40pm\nPerkins LINK 087 (Classroom 3)\n\n\nLab 02\nWe 3:05pm - 4:20pm\nPerkins LINK 087 (Classroom 3)\n\n\n\nCourse website: sta221-fa25.github.io\n\n\nCourse description\nIn STA 221, students will learn how linear and logistic regression models are used to explore multivariable relationships, apply these methods to real data and learn the mathematical underpinnings of the models. Students will develop computing skills to implement a reproducible data analysis workflow and gain experience communicating statistical results. Throughout the semester, students will work on a team project where they will develop a research question, answer it using methods learned in the course, and share results through a written report and presentation.\nTopics include applications of linear and logistic regression, least squares estimation, maximum likelihood estimation, analysis of variance, model diagnostics, and model selection. Students will gain experience using the computing tools R and GitHub to analyze real-world data from a variety of fields.\n\n\nPrerequisites\nEither any STA 100-level course or STA 230, 231, or 240L and MATH 216, 218, or 221. The recommended co-requisite is STA 230, 231, or 240L.\n\n\n\n\n\n\n\n\nCourse material\nThere is no official textbook for the course; readings will be made available as they are assigned. We will use the statistical software package R both in-class, and on take-home assignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/. Additionally, students may access R and RStudio through Docker containers provided by Duke Office of Information Technology. Containers can be accessed at https://cmgr.oit.duke.edu/containers.\n\n\nCourse learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions.\n\n\n\nEvaluation\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope.\n\n\n\nA \\(&gt;= 93\\), A- \\(&lt; 93\\), B+ \\(&lt; 90\\), B \\(&lt; 87\\), B- \\(&lt; 83\\), C+ \\(&lt;80\\), C \\(&lt; 77\\), C- \\(&lt; 73\\), D+ \\(&lt; 70\\), D \\(&lt; 67\\), D- \\(&lt; 63\\), F \\(&lt; 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn pseudo-random class days, there will be a brief quiz on the previous lectures. If you score \\(&gt;60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\n\n\n\n\nPolicies\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\n\nLate work\nLate homework may be submitted within 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nOutside resources and generative AI statement\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class assignments. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well. Any code that is discovered to be recycled or created by AI and is not explicitly cited will be treated as plagiarism.\n\n\n\n\n\n\nWarning\n\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments.\n\n\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Code\n\nRStudio containers\n\nCommunication\n\nEd Discussion\n\nCollaboration\n\ncourse GitHub organization\n\nAssignment turn-in\n\nGradescope"
  },
  {
    "objectID": "notes/anova.html",
    "href": "notes/anova.html",
    "title": "Analysis of variance (ANOVA1)",
    "section": "",
    "text": "Show libraries used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(broom)"
  },
  {
    "objectID": "notes/anova.html#definition",
    "href": "notes/anova.html#definition",
    "title": "Analysis of variance (ANOVA1)",
    "section": "Definition",
    "text": "Definition\nANOVA refers to (1) procedures for fitting and testing linear models in which the independent variables are categorical and (2) partitioning the dependent-variable sum of squares into ‚Äúexplained‚Äù and ‚Äúunexplainbed‚Äù components."
  },
  {
    "objectID": "notes/anova.html#one-way-anova",
    "href": "notes/anova.html#one-way-anova",
    "title": "Analysis of variance (ANOVA1)",
    "section": "One-way ANOVA",
    "text": "One-way ANOVA\nOne-way ANOVA means that we have exactly 1 categorical dependent variable.\n\nExample"
  },
  {
    "objectID": "notes/anova.html#one-way-anova-example",
    "href": "notes/anova.html#one-way-anova-example",
    "title": "Analysis of variance (ANOVA1)",
    "section": "One-way ANOVA example",
    "text": "One-way ANOVA example\nOne-way ANOVA means that we have exactly 1 categorical dependent variable. As an example, consider the penguin data set. We‚Äôll take our outcome variable, \\(y\\), to be the body mass of the penguin in grams body_mass_g and the dependent variable \\(x\\) to be categorical species of penguin species. Notice there are 3 species of penguins in this data set: Adelie, Chinstrap and Gentoo.\n\npenguin_subset = penguins %&gt;%\n  select(body_mass_g, species) %&gt;%\n  drop_na()\n\npenguin_subset %&gt;%\n  count(species)\n\n# A tibble: 3 √ó 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\nglimpse(penguin_subset)\n\nRows: 342\nColumns: 2\n$ body_mass_g &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 3300‚Ä¶\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad‚Ä¶\n\n\nNext let‚Äôs fit the linear model and report the least squares estimates for each parameter:\n\nmodel = lm(body_mass_g ~ species, data = penguin_subset)\nmodel %&gt;%\n  tidy()\n\n# A tibble: 3 √ó 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        3701.       37.6    98.4   2.49e-251\n2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1\n3 speciesGentoo      1375.       56.1    24.5   5.42e- 77\n\n\nANOVA:\n\nanova(model) %&gt;%\n  tidy()\n\n# A tibble: 2 √ó 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.      344.  2.89e-82\n2 Residuals   339  72443483.   213698.       NA  NA       \n\n\nManual calculation:\n\n# penguin_subset %&gt;%\n#   group_by(species) %&gt;%\n#   mutate(ybar = mean(body_mass_g))\n\ny &lt;- penguin_subset$body_mass_g\n\nanova_df = penguin_subset %&gt;%\n  group_by(species) %&gt;%\n  summarize(ybar = mean(body_mass_g), n = n()) %&gt;%\n  mutate(y_total_bar = mean(y)) \n\nanova_df\n\n# A tibble: 3 √ó 4\n  species    ybar     n y_total_bar\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 Adelie    3701.   151       4202.\n2 Chinstrap 3733.    68       4202.\n3 Gentoo    5076.   123       4202.\n\nanova_df %&gt;%\n  summarize(sum(n * (ybar - y_total_bar)^2)) # sum sq x\n\n# A tibble: 1 √ó 1\n  `sum(n * (ybar - y_total_bar)^2)`\n                              &lt;dbl&gt;\n1                        146864214.\n\n# sum sq resid: \n\nleft_join(penguin_subset, anova_df, by = \"species\") %&gt;%\n  summarize(rss = sum(((body_mass_g - ybar)^2)))\n\n# A tibble: 1 √ó 1\n        rss\n      &lt;dbl&gt;\n1 72443483.\n\npf(343.6263, 2, 339, log.p = TRUE)\n\n[1] -2.892344e-82\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "STA 221 Syllabus",
    "section": "",
    "text": "Lecture\nTue & Thu 10:05 - 11:20am\nOld Chemistry 116\n\n\nLab 01\nFri 8:30 - 9:45am\nPerkins Link #5\n\n\nLab 02\nFri 10:05 - 11:20am\nPerkins Link #5\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nRole\n\n\n\n\nProf.¬†Maria Tackett\nInstructor\n\n\nKat Husar\nHead TA\nLab 01L leader\n\n\nKelly Huang\nClassroom TA\n\n\nJanice Kim\nClassroom TA\n\n\nCathy Lee\nLab 02L leader\n\n\nAlan Wang\nLab 01 helper\n\n\n\nOffice hours times and locations on Canvas."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "STA 221 Syllabus",
    "section": "Course description",
    "text": "Course description\nIn STA 221, students will learn how linear and logistic regression models are used to explore multivariable relationships, apply these methods to real data and learn the mathematical underpinnings of the models. Students will develop computing skills to implement a reproducible data analysis workflow and gain experience communicating statistical results. Throughout the semester, students will work on a team project where they will develop a research question, answer it using methods learned in the course, and share results through a written report and presentation.\nTopics include applications of linear and logistic regression, least squares estimation, maximum likelihood estimation, analysis of variance, model diagnostics, and model selection. Students will gain experience using the computing tools R and GitHub to analyze real-world data from a variety of fields.\n\nPrerequisites\nEither any STA 100-level course or STA 230, 231, or 240L and MATH 216, 218, or 221. The recommended co-requisite is STA 230, 231, or 240L.\n\n\n\n\n\n\n\n\nCourse material\nThere is no official textbook for the course; readings will be made available as they are assigned. We will use the statistical software package R both in-class, and on take-home assignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/. Additionally, students may access R and RStudio through Docker containers provided by Duke Office of Information Technology. Containers can be accessed at https://cmgr.oit.duke.edu/containers.\n\n\nCourse learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions.\n\n\n\nEvaluation\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (30%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (40%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope.\n\n\n\nA \\(&gt;= 93\\), A- \\(&lt; 93\\), B+ \\(&lt; 90\\), B \\(&lt; 87\\), B- \\(&lt; 83\\), C+ \\(&lt;80\\), C \\(&lt; 77\\), C- \\(&lt; 73\\), D+ \\(&lt; 70\\), D \\(&lt; 67\\), D- \\(&lt; 63\\), F \\(&lt; 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn random class days, there will be a brief quiz on the previous lectures. If you score \\(&gt;60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\n\n\n\n\nPolicies\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\n\nLate work\nLate homework may be submitted within 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nTeam work policy\nThe final project and several labs will be completed in teams. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group.\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "STA 221 Syllabus",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions."
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "STA 221 Syllabus",
    "section": "Course materials",
    "text": "Course materials\nWhile there is no official textbook for the course; readings will be made available as they are assigned. We will use the statistical software R. Students will be able to access R through Docker containers provided by Duke Office of Information Technology. See the computing page for more information."
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "STA 221 Syllabus",
    "section": "Course community",
    "text": "Course community\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students‚Äô learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke‚Äôs Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don‚Äôt hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive. Please update your gender pronouns in Duke Hub. You can find instructions to do so here. You can learn more at the Center for Sexual and Gender Diversity‚Äôs website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, sta221-sp25.netlify.app.\nLinks to Zoom meetings may be found in Canvas. Periodic announcements will be sent via email and will also be available through Ed Discussion and Canvas Announcements. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nEmail\nIf you have questions about assignment extensions, accommodations, or any other matter not appropriate for the class discussion forum, please email me directly at maria.tackett@duke.edu. If you email me, please include ‚ÄúSTA 221‚Äù in the subject line. Barring extenuating circumstances, I will respond to STA 221 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "STA 221 Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. Your TAs and I will help you be providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask the TAs, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask us why. If you‚Äôre not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings and other preparation work.\nDo the homework and lab.The earlier you start, the better. It‚Äôs not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon‚Äôt procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don‚Äôt let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and work with a member of the teaching team to help you identify a good (re)starting point."
  },
  {
    "objectID": "syllabus.html#getting-help-in-the-course",
    "href": "syllabus.html#getting-help-in-the-course",
    "title": "STA 221 Syllabus",
    "section": "Getting help in the course",
    "text": "Getting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours1 to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum Ed Discussion. There is a chance another student has already asked a similar question, so please check the other posts in Ed Discussion before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\nCheck out the Support page for more resources."
  },
  {
    "objectID": "syllabus.html#what-to-expect-in-the-course",
    "href": "syllabus.html#what-to-expect-in-the-course",
    "title": "STA 221 Syllabus",
    "section": "What to expect in the course",
    "text": "What to expect in the course\n\nLectures and labs\nLectures and labs are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities during the lectures and labs. You are expected to prepare for class by completing assigned readings, attend all lecture and lab sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or any device with internet and a keyboard to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom will not be sufficient to accommodate everyone.\n\n\nTeams\nYou will be assigned to a team at the beginning of the semester. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of the group activities, labs and the final project. You will be asked to complete teamwork evaluations and self-reflections throughout the semester. Failure to adequately contribute to an assignment can result in a penalty to your score relative to the team‚Äôs overall mark.\nYou are expected to make use of the provided GitHub repository as the central collaborative platform. Commits to this repository will be used as one of several metrics of each team member‚Äôs relative contribution for each project."
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "STA 221 Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on six components: application exercises, homework, labs, exams, project, and teamwork.\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios, with a focus on the computation and communication. Most lab assignments will be completed in teams, and all team members are expected to contribute equally to the completion of each assignment. You are expected to use the team‚Äôs Git repository in the course‚Äôs GitHub organization as the central platform for collaboration. Commits to this repository will be used as a metric of each team member‚Äôs relative contribution for each lab, and there will be periodic peer evaluation on the team collaboration. Lab assignments will be completed using Quarto, correspond to an appropriate GitHub repository, and submitted for grading in Gradescope.\nThe lowest lab grade will be dropped at the end of the semester.\n\n\nHomework\nIn homework, you will apply what you‚Äôve learned during lecture and lab to complete data analysis tasks and explain the underlying mathematics. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and GitHub and submitted as a PDF in Gradescope.\nOne homework assignment will be dedicated to a statistics experience. The statistics experience is an opportunity to engage with statistics and data science outside of the classroom through podcasts, books, seminars, data analysis competitions, and other activities. As you complete these experiences, the goal is to consider how the material you‚Äôre learning in the course connects with society more broadly.\nThe lowest homework grade will be dropped at the end of the semester.\n\n\nExams\nThere will be two exams in this course. Each exam will include a closed-notes in-class component and an open-note take-home component. Through these exams you have the opportunity to demonstrate what you‚Äôve learned in the course thus far. The exams will focus on both conceptual understanding of the applied and mathematical content and application through analysis and computational tasks. The exams will be based on content in reading assignments, lectures, application exercises, homework, and lab assignments. More detail about the exams will be given during the semester.\n\n\nProject\nThe purpose of the final project is to apply what you‚Äôve learned to analyze an interesting data-driven research question. The project will be completed with your lab teams, and each team will present their work through a written report and presentation. More information about the project will be provided during the semester. You can learn more on the final project page.\n\n\nParticipation (Application exercises + teamwork)\n\nApplication exercises\nYou will get the most out of the course if you actively participate in class and when working with your team. Parts of some lectures will be dedicated to working on Application Exercises (AEs). AEs are submitted by pushing your work to the relevant GitHub repo.\nAEs will be graded based on making a good-faith effort to attempt all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture.\nSuccessful effort on at least 80% of AEs will result in full credit for AEs in the final course grade.\n\n\nTeamwork\nGiven the collaborative nature of statistics and data science work, teamwork will be a key part of this course. You will work in teams for in-class activities, lab assignments, and the final course project. There will be periodic peer and self-evaluations to reflect on the team‚Äôs collaboration. These evaluations will be counted as part of the participation grade."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "STA 221 Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n30%\n\n\nFinal project\n15%\n\n\nLabs\n10%\n\n\nExams (2 Midterms)\n40%\n\n\nParticipation (AEs + Teamwork)\n5%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60"
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "STA 221 Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard(DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors;and\nI will act if the Standard is compromised.\n\n\n\n\n\n\nAcademic honesty\nTL;DR: Don‚Äôt cheat!\n\nThe homework assignments must be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what‚Äôs the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to homework questions (including any code) with anyone other than myself and the teaching assistants.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\nFor the projects and team labs, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project or team labs across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g.¬†StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:2 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate‚Äîrather than hinder‚Äîlearning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask a member of the teaching team.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one‚Äôs own work, following proper citation of sources, adhering to guidance around group work projects,and more).Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback in a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline. Note that the lowest homework and lab assignment will be dropped to accommodate such circumstances.\n\nHomework and labs may be submitted up to 2 days late. There will be a 5% deduction for each 24-hour period the assignment is late.\nThe late work policy for exams will be provided with the exam instructions.\nThe late work policy for the project will be provided with the project instructions.\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email me at maria.tackett@duke.edu before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let me know if you need help contacting your academic dean.\n\n\nRegrade Requests\nRegrade requests must be submitted on Gradescope within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.\n\n\nAttendance policy\nEvery student is expected to attend and participate in lecture and labs. There may be times, however, when you cannot attend class. Lecture recordings are available upon request for students who have an excused absence. See the Lecture recording request policy for more detail. If you miss a lecture, make sure to review the material and complete the application exercise, if applicable, before the next lecture. Labs dedicated to completing the lab assignment and collaborating with your lab team. If you miss a lab session, make sure to communicate with your lab TA and teammates about how you can make up your contribution. If you know you‚Äôre going to miss a lab session and you‚Äôre feeling well enough to do so, notify your lab TA and teammates ahead of time.\nMore details on Trinity attendance policies are available here.\n\n\nLecture recording request\nLectures will be recorded on Panopto and will be made available to students with an excused absence upon request. Videos shared with such students will be available for a week after the lecture date. To request a particular lecture‚Äôs video, please fill out the form at the link below. Please submit the form within 24 hours of missing lecture to ensure you have sufficient time to watch the recording. Please also make sure that any official documentation, such as STINFs, Dean‚Äôs excuses, NOVAPs, and quarantine/removal from class notices from student health are also uploaded to the form.\nüîó https://forms.office.com/r/rvsgNcumRy\nAbout one week before each exam, the class recordings will be available to all students. These recordings will be available until the start of the exam."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "STA 221 Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you need accommodations for this class, you will need to register with the Student Disability Access Office (SDAO) and provide them with documentation related to your needs. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.\n\n\nReligious accommodations\nStudents are permitted by university policy to be absent from class to observe a religious holiday. Accordingly, Trinity College of Arts & Sciences and the Pratt School of Engineering have established procedures to be followed by students for notifying their instructors of an absence necessitated by the observance of a religious holiday. Please submit requests for religious accommodations at the beginning of the semester so that we can work to make suitable arrangements well ahead of time. You can find the policy and relevant notification form here: trinity.duke.edu/undergraduate/academic-policies/religious-holidays"
  },
  {
    "objectID": "syllabus.html#academic-and-wellness-support",
    "href": "syllabus.html#academic-and-wellness-support",
    "title": "STA 221 Syllabus",
    "section": "Academic and wellness support",
    "text": "Academic and wellness support\n\nAcademic Resource Center\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.\n\n\nCAPS\nDuke Counseling & Psychological Services (CAPS) helps Duke Students enhance strengths and develop abilities to successfully live, grow and learn in their personal and academic lives. CAPS recognizes that we are living in unprecedented times and that the changes, challenges and stressors brought on by the COVID-19 pandemic have impacted everyone, often in ways that are tax our well-being. CAPS offers many services to Duke undergraduate students, including brief individual and group counseling, couples counseling and more. CAPS staff also provides outreach to student groups, particularly programs supportive of at-risk populations, on a wide range of issues impacting them in various aspects of campus life. CAPS provides services to students via Telehealth. To initiate services, you can contact their front desk at 919-660-1000."
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "STA 221 Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nJanuary 8: Classes begin\nJanuary 20: Martin Luther King Jr.¬†Day holiday.\nJanuary 22: Drop/Add ends\nMarch 10 - 14: Spring break\nMarch 26: Last day to withdraw with ‚ÄúW‚Äù\nNovember 27 - 29: Thanksgiving recess\nApril 23: Classes end\nApril 24 - 27: Reading period\nApril 28 - May 3: Final exam period\n\nClick here for the full Duke academic calendar."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STA 221 Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOffice hours are times the teaching team set aside each week to meet with students. Click here to learn more about how to effectively use office hours.‚Ü©Ô∏é\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.‚Ü©Ô∏é‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/lab00.html",
    "href": "labs/lab00.html",
    "title": "Lab 0: Getting Started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today‚Äôs lab tasks before leaving lab today."
  },
  {
    "objectID": "labs/lab00.html#rstudio",
    "href": "labs/lab00.html#rstudio",
    "title": "Lab 0: Getting Started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient graphical user interface (GUI).\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick ‚ÄúReserve STA 221‚Äù to reserve an RStudio container. Be sure you reserve the container labeled STA 221 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA221 to log into the Docker container. You should now see the RStudio environment."
  },
  {
    "objectID": "labs/lab00.html#git-and-github",
    "href": "labs/lab00.html#git-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username. tl;dr choose something that you would be proud to show a future employer.\n\n\n\nIf you already have a GitHub account, you can move on to the next step."
  },
  {
    "objectID": "labs/lab00.html#connect-rstudio-and-github",
    "href": "labs/lab00.html#connect-rstudio-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. An outline of the authentication steps is below; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Open your STA 221 RStudio container.\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta221)\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"Alexander Fisher\",\n  user.email = \"alexander.fisher@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio!"
  },
  {
    "objectID": "labs/lab00.html#getting-started",
    "href": "labs/lab00.html#getting-started",
    "title": "Lab 0: Getting Started",
    "section": "Getting started",
    "text": "Getting started\n\nClick here to create your individual lab-00 repo: https://classroom.github.com/a/PMZfNgLX\nClick to open your lab-00 repo.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ‚Üí New Project ‚Üí Version Control ‚Üí Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-00.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab00.html#update-the-quarto-document",
    "href": "labs/lab00.html#update-the-quarto-document",
    "title": "Lab 0: Getting Started",
    "section": "Update the Quarto document",
    "text": "Update the Quarto document\n\nTask 1: Change the author name at the top of the document to your name. Render the document. You will see your name at the top of the rendered PDF.\nTask 2: The plot shows the relationship between the daily temperature and number of bike rentals in Washington, D.C.‚Äôs Capital Bikeshare in 2012.\n\n\n\n\n\n\n\n\n\n\nWrite 1 - 2 observations from the plot. Render the document. You will see your response in the rendered PDF."
  },
  {
    "objectID": "labs/lab00.html#commit-and-push-changes-to-github",
    "href": "labs/lab00.html#commit-and-push-changes-to-github",
    "title": "Lab 0: Getting Started",
    "section": "Commit and push changes to GitHub",
    "text": "Commit and push changes to GitHub\n\nOnce you have made your final updates, go to the Git pane in your RStudio instance. This is a tab in the top right corner of the RStudio window.\nCheck the appropriate boxes on every file in the Git pane. All checked files will be sent to GitHub.\nNext, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box.\nClick Commit. Note that every commit needs to have a commit message associated with it.\nNow that you have made an update and committed this change, click Push to send the changes to GitHub.\nGo to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "labs/lab0-slides.html#introductions",
    "href": "labs/lab0-slides.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta221-fa25.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "labs/lab0-slides.html#what-to-expect-in-labs",
    "href": "labs/lab0-slides.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nIntroduce lab assignment (5-10 minutes, longer today)\nWork on the lab assignment (you can find it on the course website). You will work with others but your submission must be your own for the first few labs.\nTypically you won‚Äôt finish labs in-class and they will be due 1 week after they are released."
  },
  {
    "objectID": "labs/lab0-slides.html#tips",
    "href": "labs/lab0-slides.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nRead all instructions on the lab.\nOne work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when we can help you on the spot and leave the narrative writing until later.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "labs/lab0-slides.html#beginnings",
    "href": "labs/lab0-slides.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\n\nFind the lab instructions here\nFollow the instructions in the lab as I demo."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#introductions",
    "href": "labs/slides/lab0-slides.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta221-fa25.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "href": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nIntroduce lab assignment (5-10 minutes, longer today)\nWork on the lab assignment (you can find it on the course website). You will work with others but your submission must be your own for the first few labs.\nTypically you won‚Äôt finish labs in-class and they will be due 1 week after they are released."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#tips",
    "href": "labs/slides/lab0-slides.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nRead all instructions on the lab.\nOne work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when we can help you on the spot and leave the narrative writing until later.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "labs/slides/lab0-slides.html#beginnings",
    "href": "labs/slides/lab0-slides.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\n\nFind the lab instructions here\nFollow the instructions in the lab as I demo."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#logistics",
    "href": "notes/slides/lec00-welcome.html#logistics",
    "title": "Welcome to STA 221",
    "section": "Logistics",
    "text": "Logistics\nContact\n\nalexander.fisher@duke.edu\nOffice hours: Mo/Th: 2:00-3:00p in Old Chem 223B\n\nCourse website\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html",
    "href": "notes/slides/lec00-welcome.html",
    "title": "Welcome to STA 221",
    "section": "",
    "text": "alexander.fisher@duke.edu\nOffice hours: Mo/Th: 2:00-3:00p in Old Chem 223B\n\n\n\n\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#why-linear-regression",
    "href": "notes/slides/lec00-welcome.html#why-linear-regression",
    "title": "Welcome to STA 221",
    "section": "Why linear regression?",
    "text": "Why linear regression?\n\nGenetics\n\nMbatchou et al.¬†(2021)\nGenome wide association studies (GWAS)\nWhich single nucleotide polymorphisms in the genome are associated with a specific disease?\n\nAstrophysics\n\nFerrarese and Merritt (2000)\nIs black hole mass related to bulge velocity and/or luminosity of a galaxy?\n\nEcology\n\nEstes et al.¬†(1998)\nAt what rate is the population of sea otters changing?\n\nFinance\n\nRuf and Wang (2021)\nCan correlations between price and volatility help us hedge in options trading?"
  },
  {
    "objectID": "notes/construction/anova.html",
    "href": "notes/construction/anova.html",
    "title": "Analysis of variance (ANOVA1)",
    "section": "",
    "text": "Show libraries used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(broom)"
  },
  {
    "objectID": "notes/construction/anova.html#definition",
    "href": "notes/construction/anova.html#definition",
    "title": "Analysis of variance (ANOVA1)",
    "section": "Definition",
    "text": "Definition\nANOVA refers to (1) procedures for fitting and testing linear models in which the independent variables are categorical and (2) partitioning the dependent-variable sum of squares into ‚Äúexplained‚Äù and ‚Äúunexplainbed‚Äù components."
  },
  {
    "objectID": "notes/construction/anova.html#one-way-anova-example",
    "href": "notes/construction/anova.html#one-way-anova-example",
    "title": "Analysis of variance (ANOVA1)",
    "section": "One-way ANOVA example",
    "text": "One-way ANOVA example\nOne-way ANOVA means that we have exactly 1 categorical dependent variable. As an example, consider the penguin data set. We‚Äôll take our outcome variable, \\(y\\), to be the body mass of the penguin in grams body_mass_g and the dependent variable \\(x\\) to be categorical species of penguin species. Notice there are 3 species of penguins in this data set: Adelie, Chinstrap and Gentoo.\n\npenguin_subset = penguins %&gt;%\n  select(body_mass_g, species) %&gt;%\n  drop_na()\n\npenguin_subset %&gt;%\n  count(species)\n\n# A tibble: 3 √ó 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\nglimpse(penguin_subset)\n\nRows: 342\nColumns: 2\n$ body_mass_g &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 3300‚Ä¶\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad‚Ä¶\n\n\nNext let‚Äôs fit the linear model and report the least squares estimates for each parameter:\n\nmodel = lm(body_mass_g ~ species, data = penguin_subset)\nmodel %&gt;%\n  tidy()\n\n# A tibble: 3 √ó 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        3701.       37.6    98.4   2.49e-251\n2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1\n3 speciesGentoo      1375.       56.1    24.5   5.42e- 77\n\n\nANOVA:\n\nanova(model) %&gt;%\n  tidy()\n\n# A tibble: 2 √ó 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.      344.  2.89e-82\n2 Residuals   339  72443483.   213698.       NA  NA       \n\n\nManual calculation:\n\n# penguin_subset %&gt;%\n#   group_by(species) %&gt;%\n#   mutate(ybar = mean(body_mass_g))\n\ny &lt;- penguin_subset$body_mass_g\n\nanova_df = penguin_subset %&gt;%\n  group_by(species) %&gt;%\n  summarize(ybar = mean(body_mass_g), n = n()) %&gt;%\n  mutate(y_total_bar = mean(y)) \n\nanova_df\n\n# A tibble: 3 √ó 4\n  species    ybar     n y_total_bar\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 Adelie    3701.   151       4202.\n2 Chinstrap 3733.    68       4202.\n3 Gentoo    5076.   123       4202.\n\nanova_df %&gt;%\n  summarize(sum(n * (ybar - y_total_bar)^2)) # sum sq x\n\n# A tibble: 1 √ó 1\n  `sum(n * (ybar - y_total_bar)^2)`\n                              &lt;dbl&gt;\n1                        146864214.\n\n# sum sq resid: \n\nleft_join(penguin_subset, anova_df, by = \"species\") %&gt;%\n  summarize(rss = sum(((body_mass_g - ybar)^2)))\n\n# A tibble: 1 √ó 1\n        rss\n      &lt;dbl&gt;\n1 72443483.\n\npf(343.6263, 2, 339, log.p = TRUE)\n\n[1] -2.892344e-82\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#learning-objectives",
    "href": "notes/slides/lec00-welcome.html#learning-objectives",
    "title": "Welcome to STA 221",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#assessments",
    "href": "notes/slides/lec00-welcome.html#assessments",
    "title": "Welcome to STA 221",
    "section": "Assessments",
    "text": "Assessments\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#community",
    "href": "notes/slides/lec00-welcome.html#community",
    "title": "Welcome to STA 221",
    "section": "Community",
    "text": "Community\nUphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#team-work-policy",
    "href": "notes/slides/lec00-welcome.html#team-work-policy",
    "title": "Welcome to STA 221",
    "section": "Team work policy",
    "text": "Team work policy\nThe final project and several labs will be completed in teams. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#sharing-reusing-code",
    "href": "notes/slides/lec00-welcome.html#sharing-reusing-code",
    "title": "Welcome to STA 221",
    "section": "Sharing / reusing code",
    "text": "Sharing / reusing code\n\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class quizzes and exams. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nNarrative (non-code solutions) should always be entirely your own.\n\n\n\n\n\n\n\nWarning\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#late-policy",
    "href": "notes/slides/lec00-welcome.html#late-policy",
    "title": "Welcome to STA 221",
    "section": "Late policy",
    "text": "Late policy\n\nHomeworks and labs can be turned in within 48 hours of the deadline for grade penalty (5% off per day).\nExams and the final project cannot be turned in late and can only be excused under exceptional circumstances.\nThe Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nLast minute coding/rendering issues will not be granted extensions."
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#course-toolkit",
    "href": "notes/slides/lec00-welcome.html#course-toolkit",
    "title": "Welcome to STA 221",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\n\nResource\nDescription\n\n\n\n\ncourse website\ncourse notes, deadlines, assignments, office hours, syllabus\n\n\nCanvas\nclass recordings, solutions, announcements, Ed Discussion\n\n\ncourse organization\nassignments, collaboration\n\n\nRStudio containers*\nonline coding platform\n\n\n\n*You are welcome to install R and RStudio locally on your computer. If working locally you should make sure that your environment meets the following requirements:\n\nlatest R version\nlatest RStudio\nworking git installation\nability to create ssh keys (for GitHub authentication)\nAll R packages updated to their latest version from CRAN"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#communication-and-missing-class",
    "href": "notes/slides/lec00-welcome.html#communication-and-missing-class",
    "title": "Welcome to STA 221",
    "section": "Communication and missing class",
    "text": "Communication and missing class\nIf you have questions about homework/lab exercises, debugging, or any question about course materials\n\ncome to office hours\nask on Ed Discussion\n\n\n\n\n\n\n\n\nWarning\n\n\nThe teaching team will not debug via email.\n\n\n\n\n\nWhen you miss a class:\n\nwatch the recorded lecture on Canvas\ncome to office hours / post on Ed Discussion / ask a friend about missed content"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#reproducibility-checklist",
    "href": "notes/slides/lec00-welcome.html#reproducibility-checklist",
    "title": "Welcome to STA 221",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Can the tables and figures be exactly reproduced from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#why-is-reproducibility-important",
    "href": "notes/slides/lec00-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy [@ostblom2022]\nFacilitates more effective collaboration [@ostblom2022]\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses [@alexander2023]\nPossible to identify and correct errors or biases in the analysis process [@alexander2023]"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#why-is-reproducibility-important-1",
    "href": "notes/slides/lec00-welcome.html#why-is-reproducibility-important-1",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\n\n\nOriginally reported ‚Äúthe intervention, compared with usual care, resulted in a fewer number of mean COPD-related hospitalizations and emergency department visits at 6 months per participant.‚Äù\nThere were actually more COPD-related hospitalizations and emergency department visits in the intervention group compared to the control group\nMixed up the intervention vs.¬†control group using ‚Äú0/1‚Äù coding\n\n\n\n\n\nhttps://jamanetwork.com/journals/jama/fullarticle/2752474"
  },
  {
    "objectID": "notes/slides/lec00-welcome.html#toolkit",
    "href": "notes/slides/lec00-welcome.html#toolkit",
    "title": "Welcome to STA 221",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#logistics",
    "href": "notes/slides/lec01-welcome.html#logistics",
    "title": "Welcome to STA 221",
    "section": "Logistics",
    "text": "Logistics\nContact\n\nalexander.fisher@duke.edu\nOffice hours: Tu/Fr: 2:00-3:00p in Old Chem 223B\n\nCourse website\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-linear-regression",
    "href": "notes/slides/lec01-welcome.html#why-linear-regression",
    "title": "Welcome to STA 221",
    "section": "Why linear regression?",
    "text": "Why linear regression?\n\nGenetics\n\nMbatchou et al.¬†(2021)\nGenome wide association studies (GWAS)\nWhich single nucleotide polymorphisms in the genome are associated with a specific disease?\n\nAstrophysics\n\nFerrarese and Merritt (2000)\nIs black hole mass related to bulge velocity and/or luminosity of a galaxy?\n\nEcology\n\nEstes et al.¬†(1998)\nAt what rate is the population of sea otters changing?\n\nFinance\n\nRuf and Wang (2021)\nCan correlations between price and volatility help us hedge in options trading?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#learning-objectives",
    "href": "notes/slides/lec01-welcome.html#learning-objectives",
    "title": "Welcome to STA 221",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "href": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "title": "Welcome to STA 221",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Can the tables and figures be exactly reproduced from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\n\n\nOriginally reported ‚Äúthe intervention, compared with usual care, resulted in a fewer number of mean COPD-related hospitalizations and emergency department visits at 6 months per participant.‚Äù\nThere were actually more COPD-related hospitalizations and emergency department visits in the intervention group compared to the control group\nMixed up the intervention vs.¬†control group using ‚Äú0/1‚Äù coding\n\n\n\n\n\nhttps://jamanetwork.com/journals/jama/fullarticle/2752474"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#toolkit",
    "href": "notes/slides/lec01-welcome.html#toolkit",
    "title": "Welcome to STA 221",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#assessments",
    "href": "notes/slides/lec01-welcome.html#assessments",
    "title": "Welcome to STA 221",
    "section": "Assessments",
    "text": "Assessments\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#community",
    "href": "notes/slides/lec01-welcome.html#community",
    "title": "Welcome to STA 221",
    "section": "Community",
    "text": "Community\nUphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#team-work-policy",
    "href": "notes/slides/lec01-welcome.html#team-work-policy",
    "title": "Welcome to STA 221",
    "section": "Team work policy",
    "text": "Team work policy\nThe final project and several labs will be completed in teams. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "href": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "title": "Welcome to STA 221",
    "section": "Sharing / reusing code",
    "text": "Sharing / reusing code\n\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class quizzes and exams. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nNarrative (non-code solutions) should always be entirely your own.\n\n\n\n\n\n\n\nWarning\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#late-policy",
    "href": "notes/slides/lec01-welcome.html#late-policy",
    "title": "Welcome to STA 221",
    "section": "Late policy",
    "text": "Late policy\n\nHomeworks and labs can be turned in within 48 hours of the deadline for grade penalty (5% off per day).\nExams and the final project cannot be turned in late and can only be excused under exceptional circumstances.\nThe Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nLast minute coding/rendering issues will not be granted extensions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#course-toolkit",
    "href": "notes/slides/lec01-welcome.html#course-toolkit",
    "title": "Welcome to STA 221",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\n\nResource\nDescription\n\n\n\n\ncourse website\ncourse notes, deadlines, assignments, office hours, syllabus\n\n\nCanvas\nclass recordings, solutions, announcements, Ed Discussion\n\n\ncourse organization\nassignments, collaboration\n\n\nRStudio containers*\nonline coding platform\n\n\n\n*You are welcome to install R and RStudio locally on your computer. If working locally you should make sure that your environment meets the following requirements:\n\nlatest R version\nlatest RStudio\nworking git installation\nability to create ssh keys (for GitHub authentication)\nAll R packages updated to their latest version from CRAN"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "href": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "title": "Welcome to STA 221",
    "section": "Communication and missing class",
    "text": "Communication and missing class\nIf you have questions about homework/lab exercises, debugging, or any question about course materials\n\ncome to office hours\nask on Ed Discussion\n\n\n\n\n\n\n\n\nWarning\n\n\nThe teaching team will not debug via email.\n\n\n\n\n\nWhen you miss a class:\n\nwatch the recorded lecture on Canvas\ncome to office hours / post on Ed Discussion / ask a friend about missed content"
  },
  {
    "objectID": "prepare/prepare-lec02.html",
    "href": "prepare/prepare-lec02.html",
    "title": "Prepare: simple linear regression",
    "section": "",
    "text": "üìñ Read Simple linear regression, Section 4.1 - 4.6\nüìñ Read R for Data Science, Introduction: What you will learn\nüìñ Read GitHub for supporting, reusing, contributing, and failing safely\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#exercise",
    "href": "notes/slides/lec01-welcome.html#exercise",
    "title": "Welcome to STA 221",
    "section": "Exercise",
    "text": "Exercise\n\nbikeshare = readr::read_csv(\"https://sta221-fa25.github.io/data/bikeshare-2012.csv\")\n\n\n\n\n\nAlexander, Rohan. 2023. ‚ÄúTelling Stories with Data,‚Äù June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. ‚ÄúOpinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.‚Äù Journal of Statistics and Data Science Education 30 (3): 241‚Äì50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "notes/lec02-slr.html",
    "href": "notes/lec02-slr.html",
    "title": "Correlation and intro to simple linear regresion",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(DT)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec02-slr.html#nomenclature",
    "href": "notes/lec02-slr.html#nomenclature",
    "title": "Simple linear regression",
    "section": "",
    "text": "\\[\ny = \\beta_0 + \\beta_1 x + \\epsilon\n\\]\n\n\n\n\n\n\n\nSymbol\nMeaning\n\n\n\n\ny\n‚Äúresponse‚Äù, ‚Äúoutcome‚Äù, ‚Äúdependent variable‚Äù. In prediction tasks, this is what we are interested in predicting.\n\n\nx\n‚Äúpredictor‚Äù, ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, ‚Äúindependent variable‚Äù, when capitalized ‚ÄúX‚Äù is sometimes referred to as the ‚Äúdata matrix‚Äù\n\n\n\\(\\beta\\)\n‚Äúpopulation parameter‚Äù, ‚Äúconstants‚Äù, ‚Äúcoefficients‚Äù, \\(\\beta_0\\) is often called ‚Äúthe intercept‚Äù. These are fixed numbers.\n\n\n\\(\\epsilon\\)\nthe ‚Äúerror‚Äù. This quantity represents observational error, i.e.¬†the difference between our observation and the true population-level expected value"
  },
  {
    "objectID": "notes/lec02-slr.html#learning-objectives",
    "href": "notes/lec02-slr.html#learning-objectives",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the day you should be able to explain the following concepts:\n\ncovariance\ncorrelation\nlocation and scale invariance\nordinary least squares"
  },
  {
    "objectID": "notes/lec02-slr.html#example",
    "href": "notes/lec02-slr.html#example",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Example",
    "text": "Example\n\nThis data set contains Raleigh Durham International Airport weather data pulled from the NOAA web service between September 01 and September 30, 2021. The data were sourced from https://catalog.data.gov/ August 28, 2025.\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve recorded 30 observations of two measurements. We are interested in the association between these two measurements.\nWe‚Äôll call the minimum daily temperature measurement ‚Äúx‚Äù and the maximum daily temperature ‚Äúy‚Äù."
  },
  {
    "objectID": "notes/lec02-slr.html#vocabulary",
    "href": "notes/lec02-slr.html#vocabulary",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Vocabulary",
    "text": "Vocabulary\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\n\\(y\\)\nThe outcome variable. Also called ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. In prediction tasks, this is the variable we are interested in predicting.\n\n\n\\(x\\)\nThe predictor. Also called ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, or ‚Äúindependent variable‚Äù.\n\n\n\nHow are \\(x\\) and \\(y\\) associated?\n\nScatter plotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather %&gt;%\n  ggplot(aes(x = tmin, y = tmax)) +\n  geom_point() +\n  labs(title = \"Minimum and maxmimum temperature (F) at RDU in September, 2021\") +\n  theme_bw()\n\n\n\n\nNotice: visualized this way, each of the thirty data points, \\((x_i, y_i)\\), is an element of two-dimensional space.\n\nHow would you describe the association?\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\(cov(x, y) = \\frac{1}{n-1}\\sum_{i =1}^n (x_i - \\bar{x}) (y_i - \\bar{y})\\) \\(^*\\)\n\\(^*\\) Note that we divide by \\(n-1\\) when computing the sample covariance\n\n\\(\\bar{x} = \\frac{1}{n} \\sum x_i\\) (mean of x)\n\\(\\bar{y} = \\frac{1}{n} \\sum y_i\\) (mean of y)\n\n\n\n\nx = weather %&gt;%\n  select(tmin) %&gt;%\n  pull()\ny = weather %&gt;%\n  select(tmax) %&gt;%\n  pull()\n\ncov(x, y)\n\n[1] 18.68736\n\nsum((x - mean(x)) * (y - mean(y))) / (30 - 1)\n\n[1] 18.68736\n\n\n\nShould the association be the same if the thermometer recording measurements was consistently off by 2 degrees Farenheit?\n\n\n# compute covariance of true temperature measurement\nweather %&gt;%\n  mutate(temp_min_true = tmin + 2,\n         temp_max_true = tmax + 2) %&gt;%\n  summarize(cov(temp_min_true, temp_max_true)) %&gt;%\n  pull()\n\n[1] 18.68736\n\n\nCovariance is location invariant.\n\nWill the association stay the same if we recorded temperature in celsius?\n\n\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cov(temp_min_c, temp_max_c)) %&gt;%\n  pull()\n\n[1] 5.767703\n\n\nCovariance is not scale invariant. That is, covariance does depend on the scale of the measurements."
  },
  {
    "objectID": "notes/lec02-slr.html#correlation",
    "href": "notes/lec02-slr.html#correlation",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nDefinition: correlation\n\n\n\n\\(cor(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\left(\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2\\right)^{1/2}}\\)\nMore concisely, we may write\n\\(cor(x, y) = \\frac{S_{xy}}{S_{xx}^{1/2} S_{yy}^{1/2}}\\)\n\n\n\n# correlation in Farenheit vs Celsi\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cor_F = cor(tmin, tmax), \n            cor_C = cor(temp_min_c, temp_max_c))\n\n# A tibble: 1 √ó 2\n  cor_F cor_C\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.554 0.554\n\n\n\nExerciseSolution\n\n\nShow \\(cor(x, y) = cor(ax + b, cy + d)\\).\n\n\nLet \\[\n\\begin{aligned}\nx^* &= ax_i + b,\\\\\ny^* &= cx_i + d,\n\\end{aligned}\n\\]\nthen we want to prove that \\(cor(x, y) = cor(x^*, y^*)\\).\n\\[\n\\begin{aligned}\ncor(x^*, y^*) &= \\frac{\\sum(ax_i + b - a\\bar{x} - b)(cy_i + d - c\\bar{y} - d)}{\\sqrt{\n\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{ac \\sum(x_i -\\bar{x})(y_i - \\bar{y})}{\nac \\sqrt{\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{\nS_{xy}}{\\sqrt{S_{xx} S_{yy}}\n}\n\\end{aligned}\n\\]\n\n\n\nCorrelation is location and scale invariant!\nAdditional facts about correlation:\n\n\n\n\n\n\nFact 1\n\n\n\n\nCorrelation is not invariant to monotone transformations of the data\n\n\n\n\n\n\n\n\n\nDefinition: monotone\n\n\n\n\\(g\\) is a monotonic function iff \\(x \\leq y\\) implies \\(g(x) \\leq g(y)\\)\n\n\n\nExerciseSolution\n\n\nShow by example that correlation is not invariant to monotonic transformations.\n\n\n\nset.seed(221)\nx = c(1:10) \nlogx = log(x)\ny = rnorm(10)\ncor(x, y)\n\n[1] -0.1757986\n\ncor(logx, y)\n\n[1] -0.2686214\n\n\n\n\n\n\n\n\n\n\n\nFact 2\n\n\n\n\nCorrelation is not robust to outliers\n\n\n\n\nExample 1Example 2\n\n\n\nset.seed(221)\nx = c(1:5)\ny= x + rnorm(5)\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 0.9042223\n\n\n\n\n\nx2 = c(x, 0)\ny2 = c(y, 20)\nplot(x2, y2)\n\n\n\n\n\n\n\ncor(x2, y2)\n\n[1] -0.5195233\n\n\n\n\n\n\n\n\n\n\n\nFact 3\n\n\n\nCorrelation is bounded between -1 and 1.\n\n\n\nExerciseSolution\n\n\nShow that \\(|cor(x, y)| \\leq 1\\)\n\n\nCauchy-Schwarz inequality:\nLet \\(u\\) and \\(v\\) be vectors of dimension \\(n\\), then\n\\[\n|u^T v|^2 \\leq (u^Tu) (v^Tv).\n\\] To prove that \\(\\|cor(x,y)\\| \\leq 1\\), let \\(u = x - \\bar{x}\\) and let \\(v = y - \\bar{y}\\).\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the dimension of a vector inner product, e.g.¬†\\(u^Tv\\) is \\(1 \\times 1\\), in other words, it is a ‚Äúscalar‚Äù, a number.\n\n\n\n\n\n\n\n\n\n\n\nFact 4\n\n\n\n\ncorrelation is a measure of linear association\n\n\n\n\nExample 1Example 2\n\n\n\nx = -10:10\ny = x^2\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y) \n\n[1] -4.786989e-17\n\n\n\n\n\nx = -10:10\ny = x\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 1\n\n\n\n\n\nSince correlation is related to a linear relationship between the data, strong correlation implies that\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\nExercise\n\n\nCheck out the interactive regression web app here:\nhttps://seeing-theory.brown.edu/regression-analysis/index.html#section1"
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 9 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab01.html#learning-goals",
    "href": "labs/lab01.html#learning-goals",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nRecall some basic matrix operations and linear algebra rules\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to produce visualizations and summary statistics to describe distributions"
  },
  {
    "objectID": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta221-fa25 organization on GitHub.\nClick on the repo with the prefix lab-01. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the lab 0 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab01.html#r-and-r-studio",
    "href": "labs/lab01.html#r-and-r-studio",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file."
  },
  {
    "objectID": "labs/lab01.html#yaml",
    "href": "labs/lab01.html#yaml",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document."
  },
  {
    "objectID": "labs/lab01.html#committing-changes",
    "href": "labs/lab01.html#committing-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Committing changes",
    "text": "Committing changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "labs/lab01.html#push-changes",
    "href": "labs/lab01.html#push-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Push changes",
    "text": "Push changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab01.html#instructions",
    "href": "labs/lab01.html#instructions",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\nWrite all code and narrative in your Quarto file. Write all narrative in complete sentences. Throughout the assignment, you should periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all of your code in your PDF document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (|&gt;) and plus sign (+)."
  },
  {
    "objectID": "labs/lab01.html#exercise-1",
    "href": "labs/lab01.html#exercise-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 1",
    "text": "Exercise 1\nViewing a summary of the data is a useful starting point for data analysis, especially if the data set has a large number of observations (rows) or variables (columns). Run the code below to use the glimpse function to see a summary of the ikea data set.\nHow many observations are in the ikea data set? How many variables?\n\nglimpse(ikea)\n\n\n\n\n\n\n\nNote\n\n\n\nIn your `lab-01.qmd` document you‚Äôll see that we already added the code required for the exercise as well as a sentence where you can fill in the blanks to report the answer. Use this format for the remaining exercises.\nAlso note that the code chunk has a label: glimpse-data. Labeling your code chunks is not required, but it is good practice and highly encouraged."
  },
  {
    "objectID": "labs/lab01.html#exercise-2",
    "href": "labs/lab01.html#exercise-2",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe begin each regression analysis with exploratory data analysis (EDA) to help us ‚Äúget to know‚Äù the data and examine the variable distributions and relationships between variables. We do this by visualizing the data and calculating summary statistics to describe the variables in our data set. In this lab, we will focus on data visualizations.\nWhen we make visualizations, we want them to be clear and suitable to present to a professional audience. This means that, at a minimum, each visualization should have an informative title and informative axis labels.\nFill in the code below to visualize the distribution of price_usd, the price in US dollars.\n\nggplot(data = ikea, aes(x = _____)) +\n  geom_histogram() +\n    labs(x = \"_____\",\n       y = \"_____\", \n       title = \"_____\")"
  },
  {
    "objectID": "labs/lab01.html#exercise-3",
    "href": "labs/lab01.html#exercise-3",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the visualization to describe the distribution of price. In your narrative, include descriptions of the shape, approximate center, approximate spread, and any presence of outliers. Briefly explain why the median is more representative of the center of this distribution than the mean.\nNote: You may compute summary statistics to more precisely describe the center and spread.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-4",
    "href": "labs/lab01.html#exercise-4",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn this course, we‚Äôll be most interested in the relationship between two or more variables, so let‚Äôs begin by looking at the distribution of price by category. We‚Äôll focus on two categories, Sofas & armchairs and Bookcases & shelving units, since these may be types of furniture most useful to furnish an office.\nFill in the code below to create a new data frame called ikea_sub that only includes the furniture categories of interest. We‚Äôre assigning this subsetted data frame to an object with a new name, so we don‚Äôt overwrite the original data.\n\nikea_sub &lt;- ikea |&gt;\n  filter(_____ %in% c( \"_____\",\n                       \"_____\"))\n\nNow, run the code below to remove observations that have that have a missing value for at least one of width or price_usd.\n\nikea_sub &lt;- ikea_sub |&gt;\n  drop_na(width, price_usd)\n\nHow many observations are in the ikea_sub data frame? How many variables?\n\n\n\n\n\n\nImportant\n\n\n\nUse the ikea_sub data frame for the remainder of lab."
  },
  {
    "objectID": "labs/lab01.html#exercise-5",
    "href": "labs/lab01.html#exercise-5",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate a visualization of the relationship between the width and price of your items at Ikea in the two categories of interest. Include informative axis labels and an informative title. Use the visualization to describe the relationship between the two variables.\nThen, recreate your visualization, but now adding color based on furniture category. Comment on your observations from this visualization.\nNote: Show both visualizations in the response.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#instructions-1",
    "href": "labs/lab01.html#instructions-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\n\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nPut any relevant R code in the Quarto document. You may write the answers and show any associated work for conceptual exercises by hand or type them in your Quarto document using LaTex.\nLet\n\\[\nA = \\begin{bmatrix}\n1 & 2\\\\\n3 & 4\\\\\n5 & 6\\end{bmatrix},\n\\qquad\nB = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n0 & 1 & 2 & 3\n\\end{bmatrix},\n\\qquad\nC = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\end{bmatrix}\n\\qquad\n\\]\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12}& \\dots & x_{1p}\\\\\nx_{21} & x_{22}& \\dots & x_{2p}\\\\\n\\vdots & \\vdots& \\ddots & \\vdots\\\\\nx_{n1} & x_{n2}& \\dots & x_{np}\\\\\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-6",
    "href": "labs/lab01.html#exercise-6",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 6",
    "text": "Exercise 6\nWrite the dimensions of the following matrices:\n\n\\(A\\)\n\\(B\\)\n\\(A^\\top\\)\n\\(\\mathbf{X}\\)\n\\(\\mathbf{X}^\\top\\)"
  },
  {
    "objectID": "labs/lab01.html#exercise-7",
    "href": "labs/lab01.html#exercise-7",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 7",
    "text": "Exercise 7\ni. Which of the following is a proper matrix multiplication operation? Explain why.\n\n\\(A\\times C\\)\n\\(A\\times B\\)\n\\(A^\\top \\times B\\)\n\\(B \\times A\\)\n\\(B^\\top \\times C\\)\n\\(B\\times B\\)\n\nii. Perform the multiplication you chose in part (i)."
  },
  {
    "objectID": "labs/lab01.html#matrix-operations-in-r",
    "href": "labs/lab01.html#matrix-operations-in-r",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Matrix operations in R",
    "text": "Matrix operations in R\nR has built in matrix tools such as addition, multiplication, transpose, etc. We will now practice using these tools to review some matrix properties.\nWe first begin by creating matrices using matrix() function. We provide elements of our matrices as the data argument and specify how many rows our matrices have. byrow = TRUE allows us to fill the matrix by row.\n\nA &lt;- matrix(data = c(1, 2,\n                     3, 4,\n                     5, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nB &lt;- matrix(data = c(1, 1, 1, 1, \n                     0, 1, 2, 3), \n            nrow = 2,\n            byrow = TRUE)\n\nC &lt;-  matrix(data = c(1, 4,\n                      2, 5,\n                      3, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nYou can learn more about matrix() function by typing ?matrix in console."
  },
  {
    "objectID": "labs/lab01.html#exercise-8",
    "href": "labs/lab01.html#exercise-8",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 8",
    "text": "Exercise 8\ni. To perform addition or subtraction, we can simply use a + or - operators.\n\n# Add A and C\nA + C\n\n     [,1] [,2]\n[1,]    2    6\n[2,]    5    9\n[3,]    8   12\n\n\nUsing R, find \\(C + A\\). Is addition commutative (i.e.¬†does \\(A + C = C + A\\))? Show the work to support your response.\nii. In R, we have to use a special matrix multiplication operator, %*% .\n\n# multiply A and B\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    3    7   11   15\n[3,]    5   11   17   23\n\n\nDoes the output match your answer to Exercise 7 (ii)? What happens if you try to multiply \\(B\\times A\\) in R?\n\n\n\n\n\n\nWarning\n\n\n\nMatrix multiplication is not commutative! Matrix multiplication satisfies left and right distributivity: \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\), but the order here matters. \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} \\neq \\mathbf{C}\\mathbf{A} +\\mathbf{C} \\mathbf{B}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) \\neq \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\). Pay attention to the order and dimensions of matrices.\n\n\niii. In this class, we will work a lot with matrix transposes. You can transpose a matrix in R by applying t() function.\n\n# transpose A\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nFind \\(B^\\top \\times A^\\top\\) using R. How is your answer compare to the result of \\(A\\times B\\) you found in previous part?\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 6 - 8‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-9",
    "href": "labs/lab01.html#exercise-9",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 9",
    "text": "Exercise 9\nLet \\(\\mathbf a = \\begin{bmatrix}a_1 \\\\ \\vdots \\\\ a_n\\end{bmatrix}\\) and \\(\\mathbf b = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_n\\end{bmatrix}\\). Recall, \\[\\mathbf{a}^\\top \\mathbf{a} = \\sum_{i=1}^n a_i^2.\\]\nWrite \\((\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{a} - \\mathbf{b})\\) using summation notation."
  },
  {
    "objectID": "labs/lab01.html#more-linear-algebra",
    "href": "labs/lab01.html#more-linear-algebra",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "More Linear Algebra",
    "text": "More Linear Algebra\nRecall the definition of linear dependence:\nA sequence of vectors \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\) is said to be linearly dependent if there exists a series of scalars \\(a_1, a_2, \\dots, a_p\\), not all zero, such that\n\\[\na_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{0}\n\\] Further, matrix \\(\\mathbf{X}\\) has full column rank if all of its columns are linearly independent.\nFor example, the following matrix is not full rank,\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n1 & 1 & 2\n\\end{bmatrix}\n\\] since, letting letting \\(\\mathbf{x_1} = \\begin{bmatrix}1\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_2} = \\begin{bmatrix}2\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_3} = \\begin{bmatrix}3\\\\2 \\end{bmatrix}\\), and letting \\(a_1 = 1\\), \\(a_2 = 1\\), and \\(a_3 = -1\\), we have:\n\\[\na_1 \\mathbf{x_1} + a_2\\mathbf{x_2} + a_3\\mathbf{x_3} =  \\begin{bmatrix}1\\\\1 \\end{bmatrix} + \\begin{bmatrix}2\\\\1 \\end{bmatrix} - \\begin{bmatrix}3\\\\2 \\end{bmatrix} = \\mathbf{0}.\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-10",
    "href": "labs/lab01.html#exercise-10",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 10",
    "text": "Exercise 10\nFor each of the following matrices, state whether it is full rank. If not full rank, show why (find corresponding coefficients \\(a\\)‚Äôs).\n\n\\[\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n0 & 0 & 1\\\\\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n1 & 0 & 0 & 1\\\\\n1 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "notes/lec02-slr.html#fact-1",
    "href": "notes/lec02-slr.html#fact-1",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Fact 1",
    "text": "Fact 1\n\nCorrelation is not invariant to monotone transformations of the data"
  },
  {
    "objectID": "notes/lec02-slr.html#fact-2",
    "href": "notes/lec02-slr.html#fact-2",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Fact 2",
    "text": "Fact 2\n\nCorrelation is not robust to outliers"
  },
  {
    "objectID": "notes/lec02-slr.html#fact-3",
    "href": "notes/lec02-slr.html#fact-3",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Fact 3",
    "text": "Fact 3\nCorrelation is bounded between -1 and 1."
  },
  {
    "objectID": "notes/lec02-slr.html#fact-4",
    "href": "notes/lec02-slr.html#fact-4",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Fact 4",
    "text": "Fact 4\n\ncorrelation is a measure of linear association"
  },
  {
    "objectID": "notes/lec02-slr.html#nomenclature-of-regression",
    "href": "notes/lec02-slr.html#nomenclature-of-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of regression",
    "text": "Nomenclature of regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù\n\n\n\\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "notes/lec02-slr.html#nomenclature-of-simple-linear-regression",
    "href": "notes/lec02-slr.html#nomenclature-of-simple-linear-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of simple linear regression",
    "text": "Nomenclature of simple linear regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{\\epsilon}\\) or \\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "prepare/prepare-lec03.html",
    "href": "prepare/prepare-lec03.html",
    "title": "Prepare: model assessment",
    "section": "",
    "text": "üìñ Read Model assessment\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "prepare/prepare-lec20.html",
    "href": "prepare/prepare-lec20.html",
    "title": "Prepare for Lecture 20: Logistic regression - Prediction",
    "section": "",
    "text": "üìñ Classification module in Google Machine Learning Crash Course"
  },
  {
    "objectID": "prepare/prepare-lec08.html",
    "href": "prepare/prepare-lec08.html",
    "title": "Prepare for Lecture 08: Inference for regression",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "prepare/prepare-lec04.html",
    "href": "prepare/prepare-lec04.html",
    "title": "Prepare: matrix representation",
    "section": "",
    "text": "Review linear algebra concepts (as needed)\n\nMatrices and vectors: [slides][video]\nMatrix-Vector products: [slides][video]\nMatrix multiplication: [slides][video]\n\n\n\n\n\n\n\nNote\n\n\n\nAll linear algebra review materials from Math 218: Matrices and Vectors (Summer 2024) taught by Dr.¬†Brian Fitzpatrick at Duke University"
  },
  {
    "objectID": "prepare/prepare-lec05.html",
    "href": "prepare/prepare-lec05.html",
    "title": "Prepare: intro to inference",
    "section": "",
    "text": "Read through John Fox‚Äôs Regression Analysis notes posted to Canvas under ‚ÄúFiles‚Äù &gt; ‚Äúreadings‚Äù &gt; intro_inference."
  },
  {
    "objectID": "prepare/prepare-lec09.html",
    "href": "prepare/prepare-lec09.html",
    "title": "Prepare for Lecture 09: Inference for regression cont‚Äôd",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "notes/lec04-model-assessment.html",
    "href": "notes/lec04-model-assessment.html",
    "title": "Model assessment",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(DT) # shows the data table\nlibrary(patchwork) # arranging plots\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)\nlibrary(knitr)       # aesthetic tables\n \nlife_exp &lt;- read_csv(\n    \"https://sta221-fa25.github.io/data/life-expectancy-data.csv\") |&gt; \n  rename(life_exp = `Life_expectancy_at_birth`, \n         income_inequality = `Income_inequality_Gini_coefficient`) |&gt;\n  mutate(education = if_else(Education_Index &gt; median(Education_Index), \"High\", \"Low\"), \n         education = factor(education, levels = c(\"Low\", \"High\"))) |&gt;\n  select(Country, life_exp, Health_expenditure, income_inequality)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#section",
    "href": "notes/lec04-model-assessment.html#section",
    "title": "Model assessment",
    "section": "",
    "text": "Click here to see the original paper."
  },
  {
    "objectID": "notes/lec04-model-assessment.html#learning-objectives",
    "href": "notes/lec04-model-assessment.html#learning-objectives",
    "title": "Model assessment",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nfit simple linear regression models in R\ninterpret coefficients in context\nanalyze the variance\ncheck model fit and interpret in context"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#example",
    "href": "notes/lec04-model-assessment.html#example",
    "title": "Model assessment",
    "section": "Example",
    "text": "Example\nThe data set comes from Zarulli et al. (2021) who analyze the effects of a country‚Äôs healthcare expenditures and other factors on the country‚Äôs life expectancy. The data are originally from the Human Development Database and World Health Organization.\nThere are 140 countries (observations) in the data set.\n\n\n\n\n\n\nGoal: Use the income inequality in a country to understand variability in the life expectancy.\n\n\nClick here for the original research paper.\n\nlife_exp: The average number of years that a newborn could expect to live, if he or she were to pass through life exposed to the sex- and age-specific death rates prevailing at the time of his or her birth, for a specific year, in a given country, territory, or geographic area (from the World Health Organization).\nincome_inequality: Measure of the deviation of the distribution of income among individuals or households within a country from a perfectly equal distribution. A value of 0 represents absolute equality, a value of 100 absolute inequality, based on ‚ÄúGini coefficient‚Äù (from Zarulli et al. (2021)).\nhealth_expenditure: Per capita current spending on healthcare goods and services, expressed in respective currency - international Purchasing Power Parity (PPP) dollar (from the World Health Organization)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#interpretation",
    "href": "notes/lec04-model-assessment.html#interpretation",
    "title": "Model assessment",
    "section": "Interpretation",
    "text": "Interpretation\n\nThe intercept estimate \\(\\hat{\\beta}_0\\) is in units of the response variable. It is the expected value of the response variable when the predictor is set to 0.\nThe estimate of the slope coefficient, \\(\\hat{\\beta}_1\\) is measured in the units of the response variable per unit of the explanatory variable."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html",
    "href": "notes/slides/lec01-welcome.html",
    "title": "Welcome to STA 221",
    "section": "",
    "text": "alexander.fisher@duke.edu\nOffice hours: Mo/Th: 2:00-3:00p in Old Chem 223B\n\n\n\n\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#exploratory-data-analysis",
    "href": "notes/lec04-model-assessment.html#exploratory-data-analysis",
    "title": "Model assessment",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nPlotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of life expectancy vs income inequality\np1 &lt;- ggplot(life_exp, aes(x = income_inequality, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Income inequality\",\n    y = \"Life expectancy at birth\"\n  ) +\n  theme_minimal()\n\n# Scatterplot of life expectancy vs health expenditure\np2 &lt;- ggplot(life_exp, aes(x = Health_expenditure, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Health expenditure\",\n    y = \"\"\n  ) +\n  theme_minimal()\n\n# Display plots side by side; uses patchwork pacakge\np1 + p2"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "href": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "title": "Model assessment",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#univariate-eda",
    "href": "notes/lec04-model-assessment.html#univariate-eda",
    "title": "Model assessment",
    "section": "Univariate EDA",
    "text": "Univariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = life_exp, aes(x = life_exp))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Life expectancy (years)\", \n       y = \"Count\") +\n  theme_bw()\n\np2 &lt;- ggplot(data = life_exp, aes(x = income_inequality))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Income inequality\", \n       y = \"Count\") +\n  theme_bw()\n\np1 + p2 # uses patchwork package"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#bivariate-eda",
    "href": "notes/lec04-model-assessment.html#bivariate-eda",
    "title": "Model assessment",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of life expectancy vs income inequality\np1 &lt;- ggplot(life_exp, aes(x = income_inequality, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Income inequality\",\n    y = \"Life expectancy (years)\"\n  ) +\n  theme_bw()\n\n# Scatterplot of life expectancy vs health expenditure\np2 &lt;- ggplot(life_exp, aes(x = Health_expenditure, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Health expenditure\",\n    y = \"\"\n  ) +\n  theme_bw()\n\n# Display plots side by side; uses patchwork pacakge\np1 + p2"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#fit-with-ordinary-least-squares",
    "href": "notes/lec04-model-assessment.html#fit-with-ordinary-least-squares",
    "title": "Model assessment",
    "section": "Fit with ordinary least squares",
    "text": "Fit with ordinary least squares\n\nTemplate\nTo fit a model by OLS linear regression, we use the lm function. The arguments look as follows:\n\nlm(y ~ x, data = data_frame)\n\nIn words, we ‚Äúregress y on x‚Äù where ‚Äúy‚Äù and ‚Äúx‚Äù are column names in the data frame ‚Äúdata_frame‚Äù.\n\n\nFor our data\n\n# income inequality model\nmodel_ii = lm(life_exp ~ income_inequality, \n              data = life_exp)\n\n# health expenditure model\nmodel_he = lm(life_exp ~ Health_expenditure,\n              data = life_exp)\n\n\nQ: why is Health_expenditure capitalized, but income_inequality is not?\nA: That‚Äôs how they are in the data! See the column names: names(life_exp).\n\n\n\nLook at results (income inequality model)\nRegular output:\n\nmodel_ii\n\n\nCall:\nlm(formula = life_exp ~ income_inequality, data = life_exp)\n\nCoefficients:\n      (Intercept)  income_inequality  \n          85.4202            -0.6973  \n\n\nTidy the output:\n\nmodel_ii |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         85.4      0.855       99.9 1.41e-130\n2 income_inequality   -0.697    0.0388     -18.0 6.17e- 38\n\n\nPut results in a more aesthetic table using kable from the knitr package:\n\nmodel_ii |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n85.420\n0.855\n99.877\n0\n\n\nincome_inequality\n-0.697\n0.039\n-17.961\n0\n\n\n\n\n\nThe fitted regression model:\n\\[\n\\hat{y_i} = 85.420 - 0.697 x_i\n\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "href": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "title": "Model assessment",
    "section": "Ordinary least squares regression in R",
    "text": "Ordinary least squares regression in R\n\nTemplate\nTo fit a model by OLS linear regression, we use the lm function. The arguments look as follows:\n\nlm(y ~ x, data = data_frame)\n\nIn words, we ‚Äúregress y on x‚Äù where ‚Äúy‚Äù and ‚Äúx‚Äù are column names in the data frame ‚Äúdata_frame‚Äù.\n\n\nFor our data\n\n# income inequality model\nmodel_ii = lm(life_exp ~ income_inequality, \n              data = life_exp)\n\n# health expenditure model\nmodel_he = lm(life_exp ~ Health_expenditure,\n              data = life_exp)\n\n\nQ: why is Health_expenditure capitalized, but income_inequality is not?\nA: That‚Äôs how they are named in the data! See the column names: names(life_exp).\n\n\n\nLook at results (income inequality model)\nRegular output:\n\nmodel_ii\n\n\nCall:\nlm(formula = life_exp ~ income_inequality, data = life_exp)\n\nCoefficients:\n      (Intercept)  income_inequality  \n          85.4202            -0.6973  \n\n\nTidy the output:\n\nmodel_ii |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         85.4      0.855       99.9 1.41e-130\n2 income_inequality   -0.697    0.0388     -18.0 6.17e- 38\n\n\nPut results in a more aesthetic table using kable from the knitr package:\n\nmodel_ii |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n85.420\n0.855\n99.877\n0\n\n\nincome_inequality\n-0.697\n0.039\n-17.961\n0\n\n\n\n\n\nThe fitted regression model:\n\\[\n\\hat{y_i} = 85.420 - 0.697 x_i\n\\]\n\n\nInterpretation\n\nThe intercept estimate \\(\\hat{\\beta}_0\\) is in units of the response variable. It is the expected value of the response variable when the predictor is set to 0.\nThe estimate of the slope coefficient, \\(\\hat{\\beta}_1\\) is measured in the units of the response variable per unit of the explanatory variable.\n\n\nExercise\n\n\nInterpret the coefficients above (for the income inequality model) in context.\n\n\n\n\n\nPrediction\nUse the predict function to calculate predictions for new observations\nSingle observation\n\nnew_ii &lt;- tibble(income_inequality = 50)\npredict(model_ii, new_ii)\n\n       1 \n50.55618 \n\n\nMultiple observations\n\nmore_new_ii &lt;- tibble(income_inequality = c(25,50, 100))\npredict(model_ii, more_new_ii)\n\n       1        2        3 \n67.98821 50.55618 15.69213 \n\n\nNote the range:\n\nrange(life_exp$income_inequality)\n\n[1]  5.4 44.2\n\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing the model to predict for values outside the range of the original data is extrapolation. Why do we want to avoid extrapolation?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "href": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "title": "Model assessment",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response",
    "href": "notes/lec04-model-assessment.html#total-variability-response",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMin\nMedian\nMax\nMean\nStd.Dev\n\n\n\n\n51.6\n72.85\n84.1\n71.614\n8.075"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "href": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "title": "Model assessment",
    "section": "Partition sources of variability in life_exp",
    "text": "Partition sources of variability in life_exp"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response-1",
    "href": "notes/lec04-model-assessment.html#total-variability-response-1",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Total (SST)} = S_{yy} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1) \\cdot var(y)\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#explained-variability-model",
    "href": "notes/lec04-model-assessment.html#explained-variability-model",
    "title": "Model assessment",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "href": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "title": "Model assessment",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#sum-of-squares",
    "href": "notes/lec04-model-assessment.html#sum-of-squares",
    "title": "Model assessment",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{darkred}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{darkred}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]\n\n\nClick here to see why this equality holds."
  },
  {
    "objectID": "notes/lec04-model-assessment.html#r2",
    "href": "notes/lec04-model-assessment.html#r2",
    "title": "Model assessment",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#model-assessment",
    "href": "notes/lec04-model-assessment.html#model-assessment",
    "title": "Model assessment",
    "section": "Model assessment",
    "text": "Model assessment\nWe fit a model but is it any good?\n\nTwo statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n\nWhat indicates a good model fit? Higher or lower RMSE? Higher or lower \\(R^2\\)?\n\n\n\nRMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#interpreting-r2",
    "href": "notes/lec04-model-assessment.html#interpreting-r2",
    "title": "Model assessment",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "href": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "title": "Model assessment",
    "section": "Using R to look at these quantities",
    "text": "Using R to look at these quantities\n\nAugmented data frame\nUse the augment() function from the broom package to add columns for predicted values, residuals, and other observation-level model statistics\n\nlife_exp_aug &lt;- augment(model_ii)\nlife_exp_aug\n\n# A tibble: 140 √ó 8\n   life_exp income_inequality .fitted .resid    .hat .sigma   .cooksd .std.resid\n      &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1     63.8              28.2    65.8 -1.96  0.0125    4.45 0.00125       -0.444\n 2     78.2              12.2    76.9  1.29  0.0116    4.45 0.000498       0.292\n 3     59.9              32.4    62.8 -2.93  0.0193    4.44 0.00437       -0.667\n 4     76.2              14      75.7  0.542 0.00972   4.45 0.0000739      0.123\n 5     74.6               8.6    79.4 -4.82  0.0168    4.43 0.0102        -1.10 \n 6     83                 8.3    79.6  3.37  0.0173    4.44 0.00515        0.766\n 7     81.3               7.4    80.3  1.04  0.0189    4.45 0.000540       0.237\n 8     72.5              10.1    78.4 -5.88  0.0144    4.42 0.0130        -1.33 \n 9     71.8              27.6    66.2  5.62  0.0118    4.43 0.00972        1.28 \n10     74                 6.5    80.9 -6.89  0.0207    4.41 0.0260        -1.57 \n# ‚Ñπ 130 more rows\n\n\n\n\nFinding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.40\n\n\n\n\nFinding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.700\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(model_ii)$r.squared\n\n[1] 0.7003831"
  },
  {
    "objectID": "notes/lec02-correlation.html",
    "href": "notes/lec02-correlation.html",
    "title": "Correlation and intro to simple linear regresion",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(DT)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec02-correlation.html#learning-objectives",
    "href": "notes/lec02-correlation.html#learning-objectives",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the day you should be able to explain the following concepts:\n\ncovariance\ncorrelation\nlocation and scale invariance\nordinary least squares"
  },
  {
    "objectID": "notes/lec02-correlation.html#example",
    "href": "notes/lec02-correlation.html#example",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Example",
    "text": "Example\n\nThis data set contains Raleigh Durham International Airport weather data pulled from the NOAA web service between September 01 and September 30, 2021. The data were sourced from https://catalog.data.gov/ August 28, 2025.\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve recorded 30 observations of two measurements. We are interested in the association between these two measurements.\nWe‚Äôll call the minimum daily temperature measurement ‚Äúx‚Äù and the maximum daily temperature ‚Äúy‚Äù."
  },
  {
    "objectID": "notes/lec02-correlation.html#vocabulary",
    "href": "notes/lec02-correlation.html#vocabulary",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Vocabulary",
    "text": "Vocabulary\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\n\\(y\\)\nThe outcome variable. Also called ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. In prediction tasks, this is the variable we are interested in predicting.\n\n\n\\(x\\)\nThe predictor. Also called ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, or ‚Äúindependent variable‚Äù.\n\n\n\nHow are \\(x\\) and \\(y\\) associated?\n\nScatter plotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather %&gt;%\n  ggplot(aes(x = tmin, y = tmax)) +\n  geom_point() +\n  labs(title = \"Minimum and maxmimum temperature (F) at RDU in September, 2021\") +\n  theme_bw()\n\n\n\n\nNotice: visualized this way, each of the thirty data points, \\((x_i, y_i)\\), is an element of two-dimensional space.\n\nHow would you describe the association?\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\(cov(x, y) = \\frac{1}{n-1}\\sum_{i =1}^n (x_i - \\bar{x}) (y_i - \\bar{y})\\) \\(^*\\)\n\\(^*\\) Note that we divide by \\(n-1\\) when computing the sample covariance\n\n\\(\\bar{x} = \\frac{1}{n} \\sum x_i\\) (mean of x)\n\\(\\bar{y} = \\frac{1}{n} \\sum y_i\\) (mean of y)\n\n\n\n\nx = weather %&gt;%\n  select(tmin) %&gt;%\n  pull()\ny = weather %&gt;%\n  select(tmax) %&gt;%\n  pull()\n\ncov(x, y)\n\n[1] 18.68736\n\nsum((x - mean(x)) * (y - mean(y))) / (30 - 1)\n\n[1] 18.68736\n\n\n\nShould the association be the same if the thermometer recording measurements was consistently off by 2 degrees Farenheit?\n\n\n# compute covariance of true temperature measurement\nweather %&gt;%\n  mutate(temp_min_true = tmin + 2,\n         temp_max_true = tmax + 2) %&gt;%\n  summarize(cov(temp_min_true, temp_max_true)) %&gt;%\n  pull()\n\n[1] 18.68736\n\n\nCovariance is location invariant.\n\nWill the association stay the same if we recorded temperature in celsius?\n\n\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cov(temp_min_c, temp_max_c)) %&gt;%\n  pull()\n\n[1] 5.767703\n\n\nCovariance is not scale invariant. That is, covariance does depend on the scale of the measurements."
  },
  {
    "objectID": "notes/lec02-correlation.html#correlation",
    "href": "notes/lec02-correlation.html#correlation",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nDefinition: correlation\n\n\n\n\\(cor(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\left(\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2\\right)^{1/2}}\\)\nMore concisely, we may write\n\\(cor(x, y) = \\frac{S_{xy}}{S_{xx}^{1/2} S_{yy}^{1/2}}\\)\n\n\n\n# correlation in Farenheit vs Celsi\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cor_F = cor(tmin, tmax), \n            cor_C = cor(temp_min_c, temp_max_c))\n\n# A tibble: 1 √ó 2\n  cor_F cor_C\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.554 0.554\n\n\n\nExerciseSolution\n\n\nShow \\(cor(x, y) = cor(ax + b, cy + d)\\).\n\n\nLet \\[\n\\begin{aligned}\nx^* &= ax_i + b,\\\\\ny^* &= cx_i + d,\n\\end{aligned}\n\\]\nthen we want to prove that \\(cor(x, y) = cor(x^*, y^*)\\).\n\\[\n\\begin{aligned}\ncor(x^*, y^*) &= \\frac{\\sum(ax_i + b - a\\bar{x} - b)(cy_i + d - c\\bar{y} - d)}{\\sqrt{\n\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{ac \\sum(x_i -\\bar{x})(y_i - \\bar{y})}{\nac \\sqrt{\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{\nS_{xy}}{\\sqrt{S_{xx} S_{yy}}\n}\n\\end{aligned}\n\\]\n\n\n\nCorrelation is location and scale invariant!\nAdditional facts about correlation:\n\n\n\n\n\n\nFact 1\n\n\n\n\nCorrelation is not invariant to monotone transformations of the data\n\n\n\n\n\n\n\n\n\nDefinition: monotone\n\n\n\n\\(g\\) is a monotonic function iff \\(x \\leq y\\) implies \\(g(x) \\leq g(y)\\)\n\n\n\nExerciseSolution\n\n\nShow by example that correlation is not invariant to monotonic transformations.\n\n\n\nset.seed(221)\nx = c(1:10) \nlogx = log(x)\ny = rnorm(10)\ncor(x, y)\n\n[1] -0.1757986\n\ncor(logx, y)\n\n[1] -0.2686214\n\n\n\n\n\n\n\n\n\n\n\nFact 2\n\n\n\n\nCorrelation is not robust to outliers\n\n\n\n\nExample 1Example 2\n\n\n\nset.seed(221)\nx = c(1:5)\ny= x + rnorm(5)\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 0.9042223\n\n\n\n\n\nx2 = c(x, 0)\ny2 = c(y, 20)\nplot(x2, y2)\n\n\n\n\n\n\n\ncor(x2, y2)\n\n[1] -0.5195233\n\n\n\n\n\n\n\n\n\n\n\nFact 3\n\n\n\nCorrelation is bounded between -1 and 1.\n\n\n\nExerciseSolution\n\n\nShow that \\(|cor(x, y)| \\leq 1\\)\n\n\nCauchy-Schwarz inequality:\nLet \\(u\\) and \\(v\\) be vectors of dimension \\(n\\), then\n\\[\n|u^T v|^2 \\leq (u^Tu) (v^Tv).\n\\] To prove that \\(\\|cor(x,y)\\| \\leq 1\\), let \\(u = x - \\bar{x}\\) and let \\(v = y - \\bar{y}\\).\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the dimension of a vector inner product, e.g.¬†\\(u^Tv\\) is \\(1 \\times 1\\), in other words, it is a ‚Äúscalar‚Äù, a number.\n\n\n\n\n\n\n\n\n\n\n\nFact 4\n\n\n\n\ncorrelation is a measure of linear association\n\n\n\n\nExample 1Example 2\n\n\n\nx = -10:10\ny = x^2\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y) \n\n[1] -4.786989e-17\n\n\n\n\n\nx = -10:10\ny = x\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 1\n\n\n\n\n\nSince correlation is related to a linear relationship between the data, strong correlation implies that\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\nExercise\n\n\nCheck out the interactive regression web app here:\nhttps://seeing-theory.brown.edu/regression-analysis/index.html#section1"
  },
  {
    "objectID": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "href": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of simple linear regression",
    "text": "Nomenclature of simple linear regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{\\epsilon}\\) or \\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "notes/lec03-slr.html",
    "href": "notes/lec03-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec03-slr.html#learning-objectives",
    "href": "notes/lec03-slr.html#learning-objectives",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the day you should be able to explain the following concepts:\n\ncovariance\ncorrelation\nlocation and scale invariance\nordinary least squares"
  },
  {
    "objectID": "notes/lec03-slr.html#example",
    "href": "notes/lec03-slr.html#example",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Example",
    "text": "Example\n\nThis data set contains Raleigh Durham International Airport weather data pulled from the NOAA web service between September 01 and September 30, 2021. The data were sourced from https://catalog.data.gov/ August 28, 2025.\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve recorded 30 observations of two measurements. We are interested in the association between these two measurements.\nWe‚Äôll call the minimum daily temperature measurement ‚Äúx‚Äù and the maximum daily temperature ‚Äúy‚Äù."
  },
  {
    "objectID": "notes/lec03-slr.html#vocabulary",
    "href": "notes/lec03-slr.html#vocabulary",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Vocabulary",
    "text": "Vocabulary\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\n\\(y\\)\nThe outcome variable. Also called ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. In prediction tasks, this is the variable we are interested in predicting.\n\n\n\\(x\\)\nThe predictor. Also called ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, or ‚Äúindependent variable‚Äù.\n\n\n\nHow are \\(x\\) and \\(y\\) associated?\n\nScatter plotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather %&gt;%\n  ggplot(aes(x = tmin, y = tmax)) +\n  geom_point() +\n  labs(title = \"Minimum and maxmimum temperature (F) at RDU in September, 2021\") +\n  theme_bw()\n\n\n\n\nNotice: visualized this way, each of the thirty data points, \\((x_i, y_i)\\), is an element of two-dimensional space.\n\nHow would you describe the association?\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\(cov(x, y) = \\frac{1}{n-1}\\sum_{i =1}^n (x_i - \\bar{x}) (y_i - \\bar{y})\\) \\(^*\\)\n\\(^*\\) Note that we divide by \\(n-1\\) when computing the sample covariance\n\n\\(\\bar{x} = \\frac{1}{n} \\sum x_i\\) (mean of x)\n\\(\\bar{y} = \\frac{1}{n} \\sum y_i\\) (mean of y)\n\n\n\n\nx = weather %&gt;%\n  select(tmin) %&gt;%\n  pull()\ny = weather %&gt;%\n  select(tmax) %&gt;%\n  pull()\n\ncov(x, y)\n\n[1] 18.68736\n\nsum((x - mean(x)) * (y - mean(y))) / (30 - 1)\n\n[1] 18.68736\n\n\n\nShould the association be the same if the thermometer recording measurements was consistently off by 2 degrees Farenheit?\n\n\n# compute covariance of true temperature measurement\nweather %&gt;%\n  mutate(temp_min_true = tmin + 2,\n         temp_max_true = tmax + 2) %&gt;%\n  summarize(cov(temp_min_true, temp_max_true)) %&gt;%\n  pull()\n\n[1] 18.68736\n\n\nCovariance is location invariant.\n\nWill the association stay the same if we recorded temperature in celsius?\n\n\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cov(temp_min_c, temp_max_c)) %&gt;%\n  pull()\n\n[1] 5.767703\n\n\nCovariance is not scale invariant. That is, covariance does depend on the scale of the measurements."
  },
  {
    "objectID": "notes/lec03-slr.html#correlation",
    "href": "notes/lec03-slr.html#correlation",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nDefinition: correlation\n\n\n\n\\(cor(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\left(\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2\\right)^{1/2}}\\)\nMore concisely, we may write\n\\(cor(x, y) = \\frac{S_{xy}}{S_{xx}^{1/2} S_{yy}^{1/2}}\\)\n\n\n\n# correlation in Farenheit vs Celsi\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cor_F = cor(tmin, tmax), \n            cor_C = cor(temp_min_c, temp_max_c))\n\n# A tibble: 1 √ó 2\n  cor_F cor_C\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.554 0.554\n\n\n\nExerciseSolution\n\n\nShow \\(cor(x, y) = cor(ax + b, cy + d)\\).\n\n\nLet \\[\n\\begin{aligned}\nx^* &= ax_i + b,\\\\\ny^* &= cx_i + d,\n\\end{aligned}\n\\]\nthen we want to prove that \\(cor(x, y) = cor(x^*, y^*)\\).\n\\[\n\\begin{aligned}\ncor(x^*, y^*) &= \\frac{\\sum(ax_i + b - a\\bar{x} - b)(cy_i + d - c\\bar{y} - d)}{\\sqrt{\n\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{ac \\sum(x_i -\\bar{x})(y_i - \\bar{y})}{\nac \\sqrt{\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{\nS_{xy}}{\\sqrt{S_{xx} S_{yy}}\n}\n\\end{aligned}\n\\]\n\n\n\nCorrelation is location and scale invariant!\nAdditional facts about correlation:\n\n\n\n\n\n\nFact 1\n\n\n\n\nCorrelation is not invariant to monotone transformations of the data\n\n\n\n\n\n\n\n\n\nDefinition: monotone\n\n\n\n\\(g\\) is a monotonic function iff \\(x \\leq y\\) implies \\(g(x) \\leq g(y)\\)\n\n\n\nExerciseSolution\n\n\nShow by example that correlation is not invariant to monotonic transformations.\n\n\n\nset.seed(221)\nx = c(1:10) \nlogx = log(x)\ny = rnorm(10)\ncor(x, y)\n\n[1] -0.1757986\n\ncor(logx, y)\n\n[1] -0.2686214\n\n\n\n\n\n\n\n\n\n\n\nFact 2\n\n\n\n\nCorrelation is not robust to outliers\n\n\n\n\nExample 1Example 2\n\n\n\nset.seed(221)\nx = c(1:5)\ny= x + rnorm(5)\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 0.9042223\n\n\n\n\n\nx2 = c(x, 0)\ny2 = c(y, 20)\nplot(x2, y2)\n\n\n\n\n\n\n\ncor(x2, y2)\n\n[1] -0.5195233\n\n\n\n\n\n\n\n\n\n\n\nFact 3\n\n\n\nCorrelation is bounded between -1 and 1.\n\n\n\nExerciseSolution\n\n\nShow that \\(|cor(x, y)| \\leq 1\\)\n\n\nCauchy-Schwarz inequality:\nLet \\(u\\) and \\(v\\) be vectors of dimension \\(n\\), then\n\\[\n|u^T v|^2 \\leq (u^Tu) (v^Tv).\n\\] To prove that \\(\\|cor(x,y)\\| \\leq 1\\), let \\(u = x - \\bar{x}\\) and let \\(v = y - \\bar{y}\\).\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the dimension of a vector inner product, e.g.¬†\\(u^Tv\\) is \\(1 \\times 1\\), in other words, it is a ‚Äúscalar‚Äù, a number.\n\n\n\n\n\n\n\n\n\n\n\nFact 4\n\n\n\n\ncorrelation is a measure of linear association\n\n\n\n\nExample 1Example 2\n\n\n\nx = -10:10\ny = x^2\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y) \n\n[1] -4.786989e-17\n\n\n\n\n\nx = -10:10\ny = x\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 1\n\n\n\n\n\nSince correlation is related to a linear relationship between the data, strong correlation implies that\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\nExercise\n\n\nCheck out the interactive regression web app here:\nhttps://seeing-theory.brown.edu/regression-analysis/index.html#section1"
  },
  {
    "objectID": "notes/lec03-slr.html#nomenclature-of-simple-linear-regression",
    "href": "notes/lec03-slr.html#nomenclature-of-simple-linear-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of simple linear regression",
    "text": "Nomenclature of simple linear regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{\\epsilon}\\) or \\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "notes/lec03-slr.html#notation",
    "href": "notes/lec03-slr.html#notation",
    "title": "Simple linear regression",
    "section": "Notation",
    "text": "Notation\n\\(\\boldsymbol{y}\\): a vector of observations\n\\(\\boldsymbol{y}= [y_1, y_2, \\ldots, y_n]^T = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\\). We say ‚Äúy is of dimension n‚Äù.\n\\(\\hat{\\boldsymbol{y}}\\): vector of ‚Äúfitted‚Äù outcomes or ‚Äúpredicted‚Äù response variable.\n\\(\\boldsymbol{1}= \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{bmatrix}\\). \\(\\boldsymbol{1}\\) is of dimension of n.\n\n\n\n\n\n\nNote\n\n\n\nFor the handwritten in-class notes, we use the convention that a line underneath the symbol represents a vector."
  },
  {
    "objectID": "notes/lec03-slr.html#last-time",
    "href": "notes/lec03-slr.html#last-time",
    "title": "Simple linear regression",
    "section": "Last time",
    "text": "Last time\n\\(|cor(\\boldsymbol{x},\\boldsymbol{y})| = 1 \\iff \\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}\\) for some \\(\\beta_0\\), \\(\\beta_1\\). See Figure 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is more typical that \\(|cor(\\boldsymbol{x}, \\boldsymbol{y})| &lt; 1\\), in which case \\(\\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}+ \\boldsymbol{\\varepsilon}\\), where \\(\\boldsymbol{\\varepsilon}\\) is the error vector. See Figure 2 for an example. The observation by observation representation is given by the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i.\\]\n\n\n\n\n\n\n\n\n\nQuestion: what line, (i.e.¬†what (\\(\\beta_0, \\beta_1\\))) provides the ‚Äúbest fit‚Äù?\nAnswer: the set (\\(\\beta_0, \\beta_1\\)) that satisfy an objective function.\n\n\n\n\n\n\nDefinition: objective function\n\n\n\nAn objective function is some function we want to optimize.\nExample 1: least absolute value (LAV) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n |\\varepsilon_i|\n\\]\nExample 2: Ordinary least squares (OLS) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]"
  },
  {
    "objectID": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "href": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "title": "Simple linear regression",
    "section": "Ordinary least squares (OLS) regression line",
    "text": "Ordinary least squares (OLS) regression line\n\\(\\sum_{i=1}^n \\varepsilon_i^2\\) is called the ‚Äúresidual sum of squares‚Äù or RSS for short.\n\\[\n\\begin{aligned}\nRSS(\\beta_0, \\beta_1) &= \\sum_{i=1}^n \\varepsilon_i^2\\\\\n&= \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2\\\\\n&= (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))^T (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))\\\\\n&= ||(\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))||^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nDefinition: OLS estimates\n\n\n\nThe OLS values of \\(\\beta_0\\), \\(\\beta_1\\) are the values \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) that minimize \\(RSS(\\beta_0, \\beta_1)\\). Again, in math,\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]\n\n\nQuestion: How can we find the OLS line?\nAnswer: (1) Geometry (we‚Äôll do this later); (2) calculus\n\nComputing the OLS estimates using calculus\n\n\n\n\n\n\n\nlm(tmax ~ tmin, data = weather)\n\n\nCall:\nlm(formula = tmax ~ tmin, data = weather)\n\nCoefficients:\n(Intercept)         tmin  \n    61.1017       0.3729  \n\n\n\nNotice that the RSS is a quadratic (convex) function of \\(\\beta_0, \\beta_1\\).\nThe global minimum occurs where the derivative (gradient) equals zero.\n\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_0} RSS &=  -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i)) = 0\\\\\n\\frac{\\partial}{\\partial \\beta_1} RSS &=  -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i)) = 0\n\\end{aligned}\n\\]\nTherefore the OLS values \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) will satisfy the normal equations,\n\\[\n\\begin{align}\n\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{1}\\\\\n\\sum_{i=1}^n x_i \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{2}\n\\end{align}\n\\] Question: why are these called the normal equations?\nAnswer: Let \\(\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\) then,\n\\[\n\\begin{aligned}\n(1) &\\implies \\sum \\hat{\\varepsilon}_i  = \\hat{\\boldsymbol{\\varepsilon}}\\cdot \\boldsymbol{1}= 0\\\\\n(2) &\\implies \\sum x_i \\hat{\\varepsilon}_i = \\boldsymbol{x}^T \\hat{\\boldsymbol{\\varepsilon}}= 0.\n\\end{aligned}\n\\]\nIn words, the residual vector \\(\\hat{\\boldsymbol{\\varepsilon}}\\) is normal (orthogonal) to the vectors \\(\\boldsymbol{1}\\) and \\(\\boldsymbol{x}\\).\n\nExerciseSolution\n\n\nShow that the OLS regression line goes through \\(\\bar{x}, \\bar{y}\\). Reminder: \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n} \\sum y_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n(1) \\implies &\\sum y_i - \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i) = 0\\\\\n&\\sum y_i = \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\\\\n& n\\bar{y} = n \\hat{\\beta_0} + n \\hat{\\beta_1} \\bar{x}\\\\\n&\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x}.\n\\end{aligned}\n\\] Note the commonly used ‚Äútrick‚Äù:\n\\(\\sum y_i = n \\bar{y}\\).\n\n\n\nSolving the normal equations\n\\[\n\\begin{aligned}\n\\text{From (1): } &\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) = 0\\\\\n& \\bar{y}  = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\\\\\n&\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\] Plugging this result into (2):\n\\[\n\\begin{aligned}\n&\\sum x_i (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)) = 0\\\\\n&\\sum x_i (y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i) = 0\\\\\n&\\sum x_i (y_i - \\bar{y}) = \\hat{\\beta}_1 \\sum x_i (x_i - \\bar{x}) \\text{    }& (*)\n\\end{aligned}\n\\]\nNotice the trick:\n\\[\n\\begin{aligned}\n\\sum (x_i - \\bar{x}) (y_i - \\bar{y}) &= \\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x}(y_i - \\bar{y})\\\\\n&= \\sum x_i (y_i - \\bar{y})  - \\bar{x} (0)\\\\\n&= \\sum x_i (y_i - \\bar{y}).\n\\end{aligned}\n\\] Similarly,\n\\[\n\\begin{aligned}\n\\sum x_i (x_i - \\bar{x}) &= \\sum (x_i - \\bar{x})(x_i - \\bar{x})\\\\\n&= \\sum (x_i - \\bar{x})^2.\n\\end{aligned}\n\\]\nLet\n\\[\n\\begin{aligned}\nS_{xx} &= \\sum (x_i - \\bar{x})^2\\\\\nS_{yy} &= \\sum (y_i - \\bar{y})^2\\\\\nS_{xy} &= \\sum (x_i - \\bar{x}) (y_i - \\bar{y})\n\\end{aligned}\n\\] Then \\((*)\\) above says \\(S_{xy} = \\hat{\\beta}_1 S_{xx}\\), implying that the OLS values are\n\\[\n\\begin{aligned}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\\\\n\\hat{\\beta}_1 &= \\frac{S_{xy}}{S_{xx}}.\n\\end{aligned}\n\\]\nAn important take-away: the slope is closely related to the correlation. Notice\n\\[\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\nThe numerator looks like covariance, or correlation between \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). The denominator looks like \\(var(\\boldsymbol{x})\\). If we multiply by the number ‚Äú1‚Äù in a fancy way, the relationship becomes clear,\n\\[\n\\begin{aligned}\n\\frac{S_{yy}^{1/2}}{S_{yy}^{1/2}} \\cdot \\frac{S_{xy}}{S_{xx}^{1/2} S_{xx}^{1/2}}\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot \\frac{S_{xy}}{\\left(S_{xx} S_{yy}\\right)^{1/2}}\\\\\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot cor(\\boldsymbol{x},\\boldsymbol{y})\n\\end{aligned}\n\\]\nSummary: what do you need to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)?\n\n\\(\\bar{x}\\), \\(\\bar{y}\\)\n\\(S_{xx}, S_{yy}\\)\n\\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\)\n\nWe can write the fitted regression line in terms of these quantities:\n\\[\n\\begin{aligned}\n\\hat{y}_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y} -\\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y}  +\\hat{\\beta}_1 (x_i - \\bar{x})\\\\\n&= \\bar{y} + \\left(S_{yy}/S_{xx} \\right)^{1/2} \\cdot cor(\\boldsymbol{x}, \\boldsymbol{y}) \\cdot (x_i - \\bar{x})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "Lab 02: Linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due Tuesday, September 16 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab-02.html#learning-goals",
    "href": "labs/lab-02.html#learning-goals",
    "title": "Lab 02: Linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nContinue developing a reproducible workflow using RStudio and GitHub\nProduce visualizations and summary statistics to describe distributions\nFit, interpret, and evaluate linear regression models\nUse the matrix representation of the linear regression model to estimate coefficients\nExplore properties of the linear regression model"
  },
  {
    "objectID": "labs/lab-02.html#exercise-1",
    "href": "labs/lab-02.html#exercise-1",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with univariate exploratory data analysis.\n\nVisualize the distribution of the response variable total_cup_points and calculate summary statistics.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\nBased on this distribution, do you think the data set is representative of all coffee available to consumers? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure your data visualizations have clear and informative titles and axis labels."
  },
  {
    "objectID": "labs/lab-02.html#exercise-2",
    "href": "labs/lab-02.html#exercise-2",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let‚Äôs consider the relationship between how good a coffee smells and its overall quality.\n\nVisualize the relationship between aroma and total_cup_points.\nDoes there appear to be a relationship between a coffee‚Äôs aroma and its overall quality? If so, what is the shape and direction of the relationship?"
  },
  {
    "objectID": "labs/lab-02.html#exercise-3",
    "href": "labs/lab-02.html#exercise-3",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have seen the mathematical formulation for simple linear regression in class. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nfor some unknown regression coefficients for intercept and slope\\((\\beta_0, \\beta_1)\\) and error terms \\(\\epsilon\\) that are centered at 0 and have variance \\(\\sigma^2_{\\epsilon}\\) . This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about regression, not be specific to the coffee data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat are the interpretations of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-02.html#exercise-4",
    "href": "labs/lab-02.html#exercise-4",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nFit the model of the relationship between aroma and total_cup_points. Neatly display the output using 3 digits.\nInterpret the slope in the context of the data.\nWhat is the expected total_cup_points for coffees that receive the worst aroma score of 0? Is this a reliable estimate of the total_cup_points for these coffees? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab-02.html#exercise-5",
    "href": "labs/lab-02.html#exercise-5",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow let‚Äôs add flavor to the model, so we will use both flavor and aroma to understand variability in the overall quality of coffees. Use this model for the remainder of the lab.\nIn class we have seen how vectors and matrices can be used to represent the linear regression model:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}, \\boldsymbol{\\epsilon}\\) for this model. Your answer should have exact values given the coffee data set.\nCompute the estimated regression coefficients using the matrix form of the model. Show the code used to get the answer.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the model.matrix() function to get the design matrix. The code takes the general form:\n\nmodel.matrix(y ~ x, data = my_data)\n\nSee Lab 01 for other matrix operations in R.\n\n\n\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits.\nWrite the estimated regression equation.\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-02.html#exercise-6",
    "href": "labs/lab-02.html#exercise-6",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nThe coefficient for aroma for the model fit in Exercise 5 is different than the coefficient from the model fit in Exercise 4. Briefly explain why these coefficients are different.\nWould you willingly drink a coffee represented by the intercept of the model in Exercise 5? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab-02.html#exercise-7",
    "href": "labs/lab-02.html#exercise-7",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCompute \\(\\mathbf{H}\\), the hat matrix corresponding to the model from Exercise 5. Then use \\(\\mathbf{H}\\) to compute the residuals for this model. Do not print out \\(\\mathbf{H}\\) or the residuals.\nCompute the mean and standard deviation of the residuals.\nRecall root mean square error RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}}\n\\]\nSimilar to other statistics we‚Äôve seen thus far, we can write the RMSE in matrix form. Compute RMSE for the model from Exercise 5 using matrix form. Show the code used to get the answer.\nHow do the standard deviation of the residuals and RMSE compare?\n\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Lab 02!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "notes/lecture-notes-playground.html",
    "href": "notes/lecture-notes-playground.html",
    "title": "lecture-notes-playground",
    "section": "",
    "text": "library(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# helper to add a vector with a large diamond tip\nadd_vector &lt;- function(fig, x, y, z, color, name=NULL) {\n  fig %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines+markers\",\n      x = c(0, x),\n      y = c(0, y),\n      z = c(0, z),\n      line = list(width = 6, color = color),\n      marker = list(\n        size = c(1, 12),  # &lt;-- make diamond much larger\n        color = c(color, color),\n        symbol = c(\"circle\", \"diamond\")\n      ),\n      name = name,\n      showlegend = TRUE\n    )\n}\n\n# build plot with all six vectors\nfig &lt;- plot_ly() %&gt;%\n  add_vector(1, 1, 1, \"blue\", \"(1,1,1)\") %&gt;%\n  add_vector(2, 0, 0, \"green\", \"(2,0,0)\") %&gt;%\n  add_vector(0, 1, 0, \"red\", \"(0,1,0)\") %&gt;%\n  add_vector(1, 0, 0, \"purple\", \"(1,0,0)\") %&gt;%\n  add_vector(0, 2, 0, \"orange\", \"(0,2,0)\") %&gt;%\n  add_vector(0, 0, 2, \"brown\", \"(0,0,2)\") %&gt;%\n  layout(\n    scene = list(\n      xaxis = list(title = \"X\"),\n      yaxis = list(title = \"Y\"),\n      zaxis = list(title = \"Z\"),\n      camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8))\n    )\n  )\n\nfig\n\n\n\n\n\n\n{r} # # install.packages(\"plotly\") # library(plotly) #  # # --- small helpers ----------------------------------------------------------- #  # # cross product (no extra packages) # vcross &lt;- function(a, b) { #   c(a[2]*b[3] - a[3]*b[2], #     a[3]*b[1] - a[1]*b[3], #     a[1]*b[2] - a[2]*b[1]) # } #  # # Rodrigues' rotation matrix to rotate vector a onto b # rot_from_a_to_b &lt;- function(a, b) { #   a &lt;- a / sqrt(sum(a^2)) #   b &lt;- b / sqrt(sum(b^2)) #   v &lt;- vcross(a, b) #   s &lt;- sqrt(sum(v^2)) #   cth &lt;- sum(a * b) #   I3 &lt;- diag(3) #   if (s &lt; 1e-12) { #     # parallel or anti-parallel #     if (cth &gt; 0) return(I3)                 # already aligned #     # 180¬∞: rotate around any axis ‚üÇ to a; x-axis works since a = (0,0,¬±1) #     return(matrix(c(1,0,0, 0,-1,0, 0,0,-1), nrow=3, byrow=TRUE)) #   } #   K &lt;- matrix(c(  0,   -v[3],  v[2], #                  v[3],   0,   -v[1], #                 -v[2],  v[1],   0), nrow=3, byrow=TRUE) #   I3 + K + K %*% K * ((1 - cth) / (s^2)) # } #  # # Build a cone mesh at `tip` pointing along `dir`. # # The cone is defined in local coords as: tip at origin, base at z = -length. # cone_mesh &lt;- function(tip, dir, length = 0.15, radius = 0.06, n = 24) { #   u &lt;- as.numeric(dir) #   if (sqrt(sum(u^2)) &lt; 1e-12) stop(\"Direction for cone is zero-length.\") #   u &lt;- u / sqrt(sum(u^2)) #  #   # Local cone vertices: tip + circular base in plane z = -length #   theta &lt;- seq(0, 2*pi, length.out = n + 1)[- (n + 1)]  # n points, exclude duplicate #   tip_local  &lt;- c(0, 0, 0) #   base_local &lt;- rbind(radius * cos(theta), radius * sin(theta), -length) #  #   # Rotate from local +Z axis to u #   R &lt;- rot_from_a_to_b(c(0,0,1), u) #   tip_world  &lt;- R %*% tip_local #   base_world &lt;- R %*% base_local #  #   # Translate to desired tip position #   tip_world  &lt;- tip_world  + tip #   base_world &lt;- sweep(base_world, 2, tip, `+`) #  #   # Assemble vertices: tip first, then base ring #   verts &lt;- cbind(tip_world, base_world)  # 3 x (1+n) #   x &lt;- verts[1,]; y &lt;- verts[2,]; z &lt;- verts[3,] #  #   # Triangles for the side surface: (tip, base_i, base_{i+1}) #   # Indices for plotly mesh3d are 0-based. #   tip_idx  &lt;- 0 #   base_idx &lt;- 1:(n)  # 1-based in R #   i_idx &lt;- rep(tip_idx, n) #   j_idx &lt;- base_idx #   k_idx &lt;- c(base_idx[-1], 1) #  #   list( #     x = x, y = y, z = z, #     i = i_idx, j = j_idx, k = k_idx #   ) # } #  # # Add one vector (line) + cone head to a plotly figure # add_vector_with_cone &lt;- function(fig, x, y, z, color = \"blue\", #                                  shaft_width = 6, #                                  cone_len = 0.15, cone_radius = 0.06, cone_n = 24, #                                  name = NULL) { #   tip &lt;- c(x, y, z) #   dir &lt;- c(x, y, z) #  #   # line (shaft) #   fig &lt;- fig %&gt;% #     add_trace( #       type = \"scatter3d\", mode = \"lines\", #       x = c(0, x), y = c(0, y), z = c(0, z), #       line = list(width = shaft_width, color = color), #       name = name %||% sprintf(\"(%g,%g,%g)\", x, y, z), #       showlegend = TRUE #     ) #  #   # cone head #   m &lt;- cone_mesh(tip = tip, dir = dir, length = cone_len, radius = cone_radius, n = cone_n) #   fig %&gt;% #     add_trace( #       type = \"mesh3d\", #       x = m$x, y = m$y, z = m$z, #       i = m$i, j = m$j, k = m$k, #       color = color, #       opacity = 1, #       showscale = FALSE, #       lighting = list(ambient = 0.5, diffuse = 0.8, specular = 0.2), #       hoverinfo = \"skip\", #       name = name %||% sprintf(\"(%g,%g,%g) head\", x, y, z), #       showlegend = FALSE #     ) # } #  # `%||%` &lt;- function(a, b) if (is.null(a)) b else a #  # # --- demo: your six vectors with true cone heads ----------------------------- #  # fig &lt;- plot_ly() #  # fig &lt;- fig %&gt;% #   add_vector_with_cone(1, 1, 1, color = \"blue\",  name = \"(1,1,1)\") %&gt;% #   add_vector_with_cone(2, 0, 0, color = \"green\", name = \"(2,0,0)\") %&gt;% #   add_vector_with_cone(0, 1, 0, color = \"red\",   name = \"(0,1,0)\") %&gt;% #   add_vector_with_cone(1, 0, 0, color = \"purple\",name = \"(1,0,0)\") %&gt;% #   add_vector_with_cone(0, 2, 0, color = \"orange\",name = \"(0,2,0)\") %&gt;% #   add_vector_with_cone(0, 0, 2, color = \"brown\", name = \"(0,0,2)\") #  # fig &lt;- fig %&gt;% #   layout( #     scene = list( #       xaxis = list(title = \"X\"), #       yaxis = list(title = \"Y\"), #       zaxis = list(title = \"Z\"), #       camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8)) #     ) #   ) #  # fig #  #"
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "Lab 02: Linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due Tuesday, September 16 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab02.html#learning-goals",
    "href": "labs/lab02.html#learning-goals",
    "title": "Lab 02: Linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nContinue developing a reproducible workflow using RStudio and GitHub\nProduce visualizations and summary statistics to describe distributions\nFit, interpret, and evaluate linear regression models\nUse the matrix representation of the linear regression model to estimate coefficients\nExplore properties of the linear regression model"
  },
  {
    "objectID": "labs/lab02.html#exercise-1",
    "href": "labs/lab02.html#exercise-1",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with univariate exploratory data analysis.\n\nVisualize the distribution of the response variable total_cup_points and calculate summary statistics.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\nBased on this distribution, do you think the data set is representative of all coffee available to consumers? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure your data visualizations have clear and informative titles and axis labels."
  },
  {
    "objectID": "labs/lab02.html#exercise-2",
    "href": "labs/lab02.html#exercise-2",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let‚Äôs consider the relationship between how good a coffee smells and its overall quality.\n\nVisualize the relationship between aroma and total_cup_points.\nDoes there appear to be a relationship between a coffee‚Äôs aroma and its overall quality? If so, what is the shape and direction of the relationship?"
  },
  {
    "objectID": "labs/lab02.html#exercise-3",
    "href": "labs/lab02.html#exercise-3",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have seen the mathematical formulation for simple linear regression in class. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nfor some unknown regression coefficients \\((\\beta_0, \\beta_1)\\) and error terms \\(\\epsilon\\) that are centered at 0 and have variance \\(\\sigma^2_{\\epsilon}\\) . This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about regression, not be specific to the coffee data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat are the interpretations of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-4",
    "href": "labs/lab02.html#exercise-4",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nFit the model of the relationship between aroma and total_cup_points. Neatly display the output using 3 digits.\nInterpret the slope in the context of the data.\nWhat is the expected total_cup_points for coffees that receive the worst aroma score of 0? Is this a reliable estimate of the total_cup_points for these coffees? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-5",
    "href": "labs/lab02.html#exercise-5",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow let‚Äôs add flavor to the model, so we will use both flavor and aroma to understand variability in the overall quality of coffees. Use this model for the remainder of the lab.\nIn class we have seen how vectors and matrices can be used to represent the linear regression model:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}, \\boldsymbol{\\epsilon}\\) for this model. Your answer should have exact values given the coffee data set.\nCompute the estimated regression coefficients using the matrix form of the model. Show the code used to get the answer.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the model.matrix() function to get the design matrix. The code takes the general form:\n\nmodel.matrix(y ~ x, data = my_data)\n\nSee Lab 01 for other matrix operations in R.\n\n\n\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits.\nWrite the estimated regression equation.\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-6",
    "href": "labs/lab02.html#exercise-6",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nThe coefficient for aroma for the model fit in Exercise 5 is different than the coefficient from the model fit in Exercise 4. Briefly explain why these coefficients are different.\nWould you willingly drink a coffee represented by the intercept of the model in Exercise 5? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-7",
    "href": "labs/lab02.html#exercise-7",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCompute \\(\\mathbf{H}\\), the hat matrix corresponding to the model from Exercise 5. Then use \\(\\mathbf{H}\\) to compute the residuals for this model. Do not print out \\(\\mathbf{H}\\) or the residuals.\nCompute the mean and standard deviation of the residuals.\nRecall root mean square error RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}}\n\\]\nSimilar to other statistics we‚Äôve seen thus far, we can write the RMSE in matrix form. Compute RMSE for the model from Exercise 5 using matrix form. Show the code used to get the answer.\nHow do the standard deviation of the residuals and RMSE compare?\n\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Lab 02!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.\n\n\nWe use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]\n\n\n\n\n\nExercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?\n\n\n\nLet\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the linear transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?\n\n\n\n\nShow that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]\n\n\n\nExercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#exercise-6",
    "href": "hw/hw01.html#exercise-6",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nLet‚Äôs start by looking at the response variable ice_duration.\n\nVisualize the distribution of ice duration versus year with separate lines for each lake.\nThere are separate yearly measurements for each lake in the icecover data frame. In this analysis, we will combine the data from both lakes and use the average ice duration each year.\nComment on the analysis choice to use the average per year rather than the individual lake measurements. Some things to consider in your comments: Does the average accurately reflects the ice duration for these lakes in a given year year? Will there be information lost? How might that impact (or not) the analysis conclusions? Etc.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the ggplot2 reference for example code and plots."
  },
  {
    "objectID": "hw/hw01.html#exercise-7",
    "href": "hw/hw01.html#exercise-7",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNext, let‚Äôs combine the ice duration and air temperature data into a single analysis data frame.\n\nFill in the code below to create a new data frame, icecover_avg, of the average ice duration by year.\nThen join icecover_avg and airtemp to create a new data frame. The new data frame should have 134 observations.\n\nicecover_avg &lt;- icecover |&gt;\n  group_by(_____) |&gt;\n  summarise(_____) |&gt;\n  ungroup()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the new data frame with average ice duration and average air temperature for the remainder of the assignment.\n\n\n\nVisualize the relationship between the air temperature and average ice duration. Do you think a linear model is a reasonable choice to model the relationship between the two variables? Briefly explain.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw01.html#exercise-8",
    "href": "hw/hw01.html#exercise-8",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nWe will fit a model using the average air temperature to explain variability in ice duration. The model takes the form\n\\[\n\\boldsymbol{y}= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\n\nState the dimensions of \\(\\boldsymbol{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\varepsilon}\\) for this analysis. Your answer should have exact values given this data set.\nEstimate the regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) in R using the matrix representation. Show the code used to get the answer.\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits."
  },
  {
    "objectID": "hw/hw01.html#exercse-9",
    "href": "hw/hw01.html#exercse-9",
    "title": "Homework 01: simple linear regression",
    "section": "Exercse 9",
    "text": "Exercse 9"
  },
  {
    "objectID": "hw/hw01.html#exercise-10",
    "href": "hw/hw01.html#exercise-10",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\na. Interpret the slope in the context of the data.\nb. The average air temperature in 2019, the most recent year in the data set, was 7.925 degrees Celsius. What was the predicted ice duration for 2019? What is the residual?"
  },
  {
    "objectID": "hw/hw01.html#exercise-1",
    "href": "hw/hw01.html#exercise-1",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "We use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-2",
    "href": "hw/hw01.html#exercise-2",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?"
  },
  {
    "objectID": "hw/hw01.html#exercise-2-1",
    "href": "hw/hw01.html#exercise-2-1",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Let \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?\n\n\\(^1\\)Exercise adapted from John Fox‚Äôs Applied Regression Analysis."
  },
  {
    "objectID": "hw/hw01.html#exercise-3",
    "href": "hw/hw01.html#exercise-3",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Let\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the linear transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?"
  },
  {
    "objectID": "hw/hw01.html#exercise-4",
    "href": "hw/hw01.html#exercise-4",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Show that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-5",
    "href": "hw/hw01.html#exercise-5",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#data",
    "href": "hw/hw01.html#data",
    "title": "Homework 01: simple linear regression",
    "section": "Data",
    "text": "Data\nThe datasets wi-icecover.csv and wi-air-temperature.csv contain information about ice cover and air temperature, respectively, at Lake Monona and Lake Mendota (both in Madison, Wisconsin) for days in 1886 through 2019. The data were obtained from the ntl_icecover and ntl_airtemp data frames in the lterdatasampler R package. They were originally collected by the US Long Term Ecological Research program (LTER) Network.\n\nicecover &lt;- read_csv(\"data/wi-icecover.csv\")\nairtemp &lt;- read_csv(\"data/wi-air-temperature.csv\")\n\nThe analysis will focus on the following variables:\n\nyear: year of observation\nlakeid: lake name\nice_duration: number of days between the freeze and ice breakup dates of each lake\nair_temp_avg: yearly average air temperature in Madison, WI (degrees Celsius)"
  },
  {
    "objectID": "hw/hw01.html#analysis-goal",
    "href": "hw/hw01.html#analysis-goal",
    "title": "Homework 01: simple linear regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goal of this analysis is to use linear regression explain variability in ice duration for lakes in Madison, WI based on air temperature. Because ice cover is impacted by various environmental factors, researchers are interested in examining the association between these two factors to better understand the changing climate."
  },
  {
    "objectID": "hw/hw01.html#exercise-9",
    "href": "hw/hw01.html#exercise-9",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCalculate \\(R^2\\) for the model in the previous exercise and interpret it in the context of the data.\nCalculate \\(RMSE\\) for the model from the previous exercise and interpret it in the context of the data.\nComment on the model fit based on \\(R^2\\) and \\(RMSE\\)."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html",
    "href": "notes/lec06-multiple-regression.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\nlibrary(DT)"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#full-data-set",
    "href": "notes/lec06-multiple-regression.html#full-data-set",
    "title": "Multiple linear regression",
    "section": "Full data set",
    "text": "Full data set\n\ndatatable(penguins, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"penguins\")\n\n\n\n\n\n\nExerciseSolution\n\n\nWrite down the mathematical formula for linear regression where penguin bodymass is the outcome variable and flipper length, bill length, and sex of the penguin are the covariates. Note the dimension of each symbol in your model.\n\n\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon}\n\\]\nAs usual,\n\n\\(\\boldsymbol{y}, \\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\nbut here \\(p = 4\\) (3 covariates + intercept).\n\\[\n\\boldsymbol{X}=\n\\begin{bmatrix}\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{1} & \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots\n\\end{bmatrix}\n\\]\n\n\n\nQuestion: Why is sex (a categorical predictor) represented as 1 column and not 2?\nAnswer: It would create a linearly dependent column in the matrix \\(\\boldsymbol{X}\\), then \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) would be rank deficient and could not be inverted.\nQuestion: Suppose we add the covariate ‚Äúisland‚Äù, what is the new dimension of \\(\\boldsymbol{X}\\) and \\(\\beta\\) then?\nAnswer: Island is categorical with three states, therefore we would need 2 predictors to represent it. Now \\(p = 6\\) and \\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times 6}\\) and \\(\\beta \\in \\mathbb{R}^6\\)."
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, February 4 at 11:59pm. To be considered on time, the following must be done by the due date:\n\nFinal .qmd and .pdf files pushed to your team‚Äôs GitHub repo\nFinal .pdf file submitted on Gradescope"
  },
  {
    "objectID": "labs/lab-03.html#exercise-1",
    "href": "labs/lab-03.html#exercise-1",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet‚Äôs start with some exploratory data analysis. Visualize the distribution of the response variable mc_preschool and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab-03.html#exercise-2",
    "href": "labs/lab-03.html#exercise-2",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for preschool-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It‚Äôs your turn! Type the team‚Äôs response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab-03.html#exercise-3",
    "href": "labs/lab-03.html#exercise-3",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we‚Äôve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model ‚Äúas-is‚Äù may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\n\n\n\n\n\n\n\n\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab-03.html#exercise-4",
    "href": "labs/lab-03.html#exercise-4",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let‚Äôs conduct bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It‚Äôs your turn! Type the team‚Äôs response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab-03.html#exercise-5",
    "href": "labs/lab-03.html#exercise-5",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab-03.html#exercise-6",
    "href": "labs/lab-03.html#exercise-6",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm() to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It‚Äôs your turn! Type the team‚Äôs response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab-03.html#exercise-7",
    "href": "labs/lab-03.html#exercise-7",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let‚Äôs assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for preschool-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab-03.html#exercise-8",
    "href": "labs/lab-03.html#exercise-8",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab-03.html#exercise-9",
    "href": "labs/lab-03.html#exercise-9",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mc_preschool.\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team‚Äôs completed lab!"
  },
  {
    "objectID": "labs/lab-03.html#exercise-10",
    "href": "labs/lab-03.html#exercise-10",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nIf you haven‚Äôt already, make sure you have completed the team agreement (see the instructions in [Meet your team!])."
  },
  {
    "objectID": "labs/lab-03.html#footnotes",
    "href": "labs/lab-03.html#footnotes",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon‚Äôt trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!‚Ü©Ô∏é"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "href": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "title": "Multiple linear regression",
    "section": "Fitting a regression model",
    "text": "Fitting a regression model\nTo ‚Äúfit‚Äù a (multiple) linear regression model means finding \\(\\hat{\\beta}\\) that defines the ‚Äúbest‚Äù hyperplane. Here, ‚Äúbest‚Äù means that \\(\\hat{\\beta}\\) is the optimal solution of some objective function."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "href": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "title": "Multiple linear regression",
    "section": "Fitting multiple regression in R (OLS)",
    "text": "Fitting multiple regression in R (OLS)\nThe formula for the least squares estimator \\(\\hat{\\beta}_{OLS}\\) is the same,\n\\[\n\\hat{\\beta}_{OLS} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\]\n\nlmmanualin-between\n\n\n\npenguin_fit &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island, data = penguins)\n\npenguin_fit\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    sex + island, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm     bill_length_mm            sexmale  \n        -3629.563             37.638              5.808            390.055  \n      islandDream    islandTorgersen  \n         -376.124           -288.380  \n\n\nQuestion: Why didn‚Äôt I have to specify an intercept?\nAnswer: lm includes intercept by default.\nQuestion: What if I don‚Äôt want an intercept?\nAnswer: Use + 0, like this: lm(y ~ x1 + x2 + ... + x_5 + 0, data = penguin). This will tell lm not to exclude an intercept term from the model.\n\n\n\nyX &lt;- penguins %&gt;%\n  select(c(\"body_mass_g\", \"bill_length_mm\", \"flipper_length_mm\",\n           \"sex\", \"island\")) %&gt;%\n  mutate(isMale = ifelse(sex == \"male\", 1, 0), ## create 1 dummy variable \n           isDream = ifelse(island == \"Dream\", 1, 0), ## create other 2 dummy variables\n         isTorgersen = ifelse(island == \"Torgersen\", 1, 0)) %&gt;%\n  select(-c(island, sex)) %&gt;% ## remove redundant columns \n  mutate(one = 1) %&gt;% # create vector of ones\n  drop_na() %&gt;% # drop NAs, just like the lm() function does by default\n  as.matrix() # turn into a matrix for matrix algebra in R\n\ny &lt;- yX[,1] # grab first column (which was the outcome variable)\nX &lt;- yX[,-1] # grab all but first column (X matrix)\n\nbetaHat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbetaHat\n\n                          [,1]\nbill_length_mm        5.808086\nflipper_length_mm    37.637788\nisMale              390.055414\nisDream            -376.124225\nisTorgersen        -288.380366\none               -3629.562552\n\n\n\n\nAn alternative, easy way to grab \\(X\\) quickly:\n\nX = model.matrix( body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island,\n              data = penguins)\nX %&gt;%\n  head(n = 10) # notice NAs are dropped by default, (see the row number skips)\n\n   (Intercept) flipper_length_mm bill_length_mm sexmale islandDream\n1            1               181           39.1       1           0\n2            1               186           39.5       0           0\n3            1               195           40.3       0           0\n5            1               193           36.7       0           0\n6            1               190           39.3       1           0\n7            1               181           38.9       0           0\n8            1               195           39.2       1           0\n13           1               182           41.1       0           0\n14           1               191           38.6       1           0\n15           1               198           34.6       1           0\n   islandTorgersen\n1                1\n2                1\n3                1\n5                1\n6                1\n7                1\n8                1\n13               1\n14               1\n15               1"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#interpretation",
    "href": "notes/lec06-multiple-regression.html#interpretation",
    "title": "Multiple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\nWe interpret \\(\\hat{\\beta}_j\\) as the expected change in the mean of \\(Y\\) when \\(X_j\\) increases by one unit, holding the value of all other predictor variables constant.\nFor example, for each additional mm of flipper length a given penguin has, we expect their body mass to increase by 37.6 grams holding all other covariates constant."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#prediction",
    "href": "notes/lec06-multiple-regression.html#prediction",
    "title": "Multiple linear regression",
    "section": "Prediction",
    "text": "Prediction\nWe can still make predictions in R; now we require more covariates. For example:\n\nnew_penguin &lt;- tibble(\n  flipper_length_mm = 180, \n  bill_length_mm = 40,\n  sex = \"male\",\n  island = \"Dream\"\n)\n\ncat(\"The predicted body mass of the new penguin is:\\n\")\n\nThe predicted body mass of the new penguin is:\n\npredict(penguin_fit, new_penguin)\n\n       1 \n3391.494 \n\n\n\n\n\n\n\n\nWarning\n\n\n\nRegression shows association not causality."
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 23 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab03.html#exercise-1",
    "href": "labs/lab03.html#exercise-1",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet‚Äôs start with some exploratory data analysis. Visualize the distribution of the response variable mc_preschool and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab03.html#exercise-2",
    "href": "labs/lab03.html#exercise-2",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for preschool-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It‚Äôs your turn! Type the team‚Äôs response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab03.html#exercise-3",
    "href": "labs/lab03.html#exercise-3",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we‚Äôve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model ‚Äúas-is‚Äù may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\n\n\n\n\n\n\n\n\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab03.html#exercise-4",
    "href": "labs/lab03.html#exercise-4",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let‚Äôs conduct bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It‚Äôs your turn! Type the team‚Äôs response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab03.html#exercise-5",
    "href": "labs/lab03.html#exercise-5",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab03.html#exercise-6",
    "href": "labs/lab03.html#exercise-6",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm() to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It‚Äôs your turn! Type the team‚Äôs response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab03.html#exercise-7",
    "href": "labs/lab03.html#exercise-7",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let‚Äôs assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for preschool-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab03.html#exercise-8",
    "href": "labs/lab03.html#exercise-8",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab03.html#exercise-9",
    "href": "labs/lab03.html#exercise-9",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mc_preschool.\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team‚Äôs completed lab!"
  },
  {
    "objectID": "labs/lab03.html#exercise-10",
    "href": "labs/lab03.html#exercise-10",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nIf you haven‚Äôt already, make sure you have completed the team agreement (see the instructions in [Meet your team!])."
  },
  {
    "objectID": "labs/lab03.html#footnotes",
    "href": "labs/lab03.html#footnotes",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon‚Äôt trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!‚Ü©Ô∏é"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html",
    "href": "notes/lec05-matrix-form-regression.html",
    "title": "Matrix algebra of regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "href": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "title": "Matrix algebra of regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nwrite down simple linear regression in matrix form\nunderstand the geometry of OLS regression\nbe able to explain the following vocabulary: design matrix, Hessian matrix, projection"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "href": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "title": "Matrix algebra of regression",
    "section": "Simple linear regression (p = 2)",
    "text": "Simple linear regression (p = 2)\nWe write down simple linear regression\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon},\n\\]\nwhere\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\\(\\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\nand \\(p = 2\\), i.e.¬†there‚Äôs an intercept and 1 slope. More explicitly:\n\n\n\n\n\n\nDefinition: design matrix\n\n\n\n\\(\\boldsymbol{X}\\) is called the ‚Äúdesign matrix‚Äù, ‚Äúcovariate matrix‚Äù, ‚Äúmodel matrix‚Äù or even sometimes the ‚Äúdata matrix‚Äù. It includes columns each predictors and an intercept (if applicable).\n\n\nNote:\n\n\\(E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}\\) and this implies that \\(E[\\boldsymbol{y}] = E[\\boldsymbol{X}\\beta] + E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{X}\\beta\\).\n\\(var[\\boldsymbol{\\varepsilon}] = \\sigma^2_\\varepsilon \\boldsymbol{I}\\)"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "href": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "title": "Matrix algebra of regression",
    "section": "Linear algebra background",
    "text": "Linear algebra background"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "href": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "title": "Matrix algebra of regression",
    "section": "Matrix calculus",
    "text": "Matrix calculus"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "href": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "title": "Matrix algebra of regression",
    "section": "OLS in Matrix form",
    "text": "OLS in Matrix form\n\nDid we find a minimum?"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#geometry",
    "href": "notes/lec05-matrix-form-regression.html#geometry",
    "title": "Matrix algebra of regression",
    "section": "Geometry",
    "text": "Geometry"
  },
  {
    "objectID": "notes/lec07-intro-inference.html",
    "href": "notes/lec07-intro-inference.html",
    "title": "Introduction to inference",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT)\nlibrary(latex2exp)\nlibrary(patchwork)\n\npokemon &lt;- read_csv(\"~/Downloads/pokemon_data.csv\") # complete, population data"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#the-data",
    "href": "notes/lec07-intro-inference.html#the-data",
    "title": "Introduction to inference",
    "section": "The data",
    "text": "The data\nAs of 2025, there are 1025 pokemon in existence.\nIn all statistical inference tasks, we only have a sample from the population. Let‚Äôs consider a random sample of the pokemon data, given below:\n\nset.seed(48) \npokemon_sample &lt;- pokemon |&gt;\n  slice_sample(n = 15) |&gt;\n  select(dexnum, name, height_m, weight_kg) |&gt;\n  arrange(dexnum)\n  \n\ndatatable(pokemon_sample, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"sample of 15 pokemon\")\n\n\n\n\n\nLet‚Äôs investigate the question: do taller pokemon weigh more?\n\nplotlinear modelplot code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(weight_kg ~ height_m, data = pokemon_sample) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    -105.     17.8      -5.87 5.50e- 5\n2 height_m        158.      9.70     16.3  4.90e-10\n\n\n\n\n\npokemon_sample |&gt;\n  ggplot(aes(x = height_m, y = weight_kg)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, color = 'steelblue') +\n  theme_bw() +\n  labs(title = \"Pokemon weight vs height\", x = \"height (m)\", y = \"weight (kg)\")"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#question",
    "href": "notes/lec07-intro-inference.html#question",
    "title": "Introduction to inference",
    "section": "Question",
    "text": "Question\nHow can we tell if our estimates \\(\\hat{\\beta}\\) are any good?\n\nrepeated samplingplotspopulation-level annotation\n\n\n\npokemon_hw = pokemon |&gt;\n  select(height_m, weight_kg)\n\nBETA_HAT &lt;- NULL\nfor(i in 1:1000) {\n  fit &lt;- pokemon_hw |&gt;\n    slice_sample(n = 15) |&gt; \n    lm(height_m ~ weight_kg, data = _) \n  BETA_HAT &lt;- rbind(BETA_HAT, fit$coefficients)\n}\n\nBETA_HAT &lt;- data.frame(BETA_HAT)\ncolnames(BETA_HAT) &lt;- c(\"beta0\", \"beta1\")\n\nglimpse(BETA_HAT)\n\nRows: 1,000\nColumns: 2\n$ beta0 &lt;dbl&gt; 0.9016025, 0.5209848, 0.6775509, 0.8639395, 0.4678607, 0.4351285‚Ä¶\n$ beta1 &lt;dbl&gt; 0.004823824, 0.010159875, 0.008377374, 0.012071556, 0.016425420,‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npopn_model &lt;- lm(height_m ~ weight_kg, data = pokemon)\npopn_model$coefficients\n\n(Intercept)   weight_kg \n0.775612595 0.006509202"
  }
]