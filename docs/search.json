[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STA 221: Regression Analysis",
    "section": "",
    "text": "Teaching team & office hours\n\n\n\n\nContact\nOffice hours\nLocation\n\n\n\n\nDr.¬†Alexander Fisher\naaf29@duke.edu\nTu/Fr: 2-3pm\nOld Chem 223B\n\n\nCathy Lee\npin.chian.lee@duke.edu\nTh: 1-3pm\nZoom (see Canvas for link)\n\n\nXukun Zhu\nxukun.zhu@duke.edu\nWe 4:30-6:30pm\nOld Chem 025\n\n\nKrish Bansal\nkrish.bansal@duke.edu\nMo: 1:30-2:30pm / We: 12-1pm\nOld Chem 203A\n\n\n\n\n\n\n\nMeetings\n\n\n\nLecture\nTu/Th 11:45am - 1:00pm\nOld Chemistry 116\n\n\nLab 01\nWe 1:25pm - 2:40pm\nPerkins LINK 087 (Classroom 3)\n\n\nLab 02\nWe 3:05pm - 4:20pm\nPerkins LINK 087 (Classroom 3)\n\n\n\nCourse website: sta221-fa25.github.io\n\n\nCourse description\nIn STA 221, students will learn how linear and logistic regression models are used to explore multivariable relationships, apply these methods to real data and learn the mathematical underpinnings of the models. Students will develop computing skills to implement a reproducible data analysis workflow and gain experience communicating statistical results. Throughout the semester, students will work on a team project where they will develop a research question, answer it using methods learned in the course, and share results through a written report and presentation.\nTopics include applications of linear and logistic regression, least squares estimation, maximum likelihood estimation, analysis of variance, model diagnostics, and model selection. Students will gain experience using the computing tools R and GitHub to analyze real-world data from a variety of fields.\n\n\nPrerequisites\nEither any STA 100-level course or STA 230, 231, or 240L and MATH 216, 218, or 221. The recommended co-requisite is STA 230, 231, or 240L.\n\n\n\n\n\n\n\n\nCourse material\nThere is no official textbook for the course; readings will be made available as they are assigned. We will use the statistical software package R both in-class, and on take-home assignments in this course. R is freely available at http://www.r-project.org/. RStudio, the popular IDE for R, is freely available at https://posit.co/downloads/. Additionally, students may access R and RStudio through Docker containers provided by Duke Office of Information Technology. Containers can be accessed at https://cmgr.oit.duke.edu/containers.\n\n\nCourse learning objectives\nBy the end of the semester, you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions.\n\n\n\nEvaluation\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope.\n\n\n\nA \\(&gt;= 93\\), A- \\(&lt; 93\\), B+ \\(&lt; 90\\), B \\(&lt; 87\\), B- \\(&lt; 83\\), C+ \\(&lt;80\\), C \\(&lt; 77\\), C- \\(&lt; 73\\), D+ \\(&lt; 70\\), D \\(&lt; 67\\), D- \\(&lt; 63\\), F \\(&lt; 60\\)\n\n\n\n\n\n\nA note on quizzes\n\n\n\nOn pseudo-random class days, there will be a brief quiz on the previous lectures. If you score \\(&gt;60\\%\\) cumulatively on your final quiz grade, you will receive full participation credit. Your lowest two quizzes will also be dropped.\n\n\n\n\n\n\n\n\n\n\n\n\nPolicies\nAcademic integrity\nBy enrolling in this course, you commit to upholding Duke‚Äôs community standard reproduced as follows:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations of academic integrity will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action. For the Exams and Quizzes, students are required to work alone. For the Homework assignments, students may work with a study group but each student must write up and submit their own answers.\n\nLate work\nLate homework may be submitted within 48 hours of the assignment deadline. Late homework submitted within 24 hours (even 1 minute late) will receive a 5% late penalty. Late work submitted between 24 to 48 hours of the deadline will receive a 10% late penalty. Work submitted after 48 hours will not be accepted. Exams cannot be turned in late and can only be excused under exceptional circumstances. The Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nOutside resources and generative AI statement\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class assignments. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well. Any code that is discovered to be recycled or created by AI and is not explicitly cited will be treated as plagiarism.\n\n\n\n\n\n\nWarning\n\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments.\n\n\nErrors in grading\nErrors in grading must be brought to the attention of the TA or instructor during office hours within 1 week of receiving the grade."
  },
  {
    "objectID": "project-data.html",
    "href": "project-data.html",
    "title": "Data resources",
    "section": "",
    "text": "Credit: list compiled by Prof.¬†Maria Tackett\n\nTidyTuesday\nFiveThirtyEight data\nData Is Plural\nR Data Sources for Regression Analysis\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies"
  },
  {
    "objectID": "prepare/prepare-lec09.html",
    "href": "prepare/prepare-lec09.html",
    "title": "Prepare for Lecture 09: Inference for regression cont‚Äôd",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "prepare/prepare-lec05.html",
    "href": "prepare/prepare-lec05.html",
    "title": "Prepare: intro to inference",
    "section": "",
    "text": "Read through John Fox‚Äôs Regression Analysis notes posted to Canvas under ‚ÄúFiles‚Äù &gt; ‚Äúreadings‚Äù &gt; intro_inference."
  },
  {
    "objectID": "prepare/prepare-lec03.html",
    "href": "prepare/prepare-lec03.html",
    "title": "Prepare: model assessment",
    "section": "",
    "text": "üìñ Read Model assessment\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#logistics",
    "href": "notes/slides/lec01-welcome.html#logistics",
    "title": "Welcome to STA 221",
    "section": "Logistics",
    "text": "Logistics\nContact\n\nalexander.fisher@duke.edu\nOffice hours: Tu/Fr: 2:00-3:00p in Old Chem 223B\n\nCourse website\n\nhttps://sta221-fa25.github.io/\n\ncomplete office hours info\nsyllabus\ncourse schedule"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-linear-regression",
    "href": "notes/slides/lec01-welcome.html#why-linear-regression",
    "title": "Welcome to STA 221",
    "section": "Why linear regression?",
    "text": "Why linear regression?\n\nGenetics\n\nMbatchou et al.¬†(2021)\nGenome wide association studies (GWAS)\nWhich single nucleotide polymorphisms in the genome are associated with a specific disease?\n\nAstrophysics\n\nFerrarese and Merritt (2000)\nIs black hole mass related to bulge velocity and/or luminosity of a galaxy?\n\nEcology\n\nEstes et al.¬†(1998)\nAt what rate is the population of sea otters changing?\n\nFinance\n\nRuf and Wang (2021)\nCan correlations between price and volatility help us hedge in options trading?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#learning-objectives",
    "href": "notes/slides/lec01-welcome.html#learning-objectives",
    "title": "Welcome to STA 221",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of this course you will be able to‚Ä¶\n\nanalyze data to explore real-world multivariable relationships.\nfit, interpret, and draw conclusions from linear and logistic regression models.\nimplement a reproducible analysis workflow using R for analysis, Quarto to write reports and GitHub for version control and collaboration.\nexplain the mathematical foundations of linear and logistic regression.\neffectively communicate statistical results to a general audience.\nassess the ethical considerations and implications of analysis decisions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "href": "notes/slides/lec01-welcome.html#reproducibility-checklist",
    "title": "Welcome to STA 221",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n‚úîÔ∏è Can the tables and figures be exactly reproduced from the code and data?\n‚úîÔ∏è Does the code actually do what you think it does?\n‚úîÔ∏è In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n‚úîÔ∏è Can the code be used for other data?\n‚úîÔ∏è Can you extend the code to do other things?"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "href": "notes/slides/lec01-welcome.html#why-is-reproducibility-important-1",
    "title": "Welcome to STA 221",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\n\n\nOriginally reported ‚Äúthe intervention, compared with usual care, resulted in a fewer number of mean COPD-related hospitalizations and emergency department visits at 6 months per participant.‚Äù\nThere were actually more COPD-related hospitalizations and emergency department visits in the intervention group compared to the control group\nMixed up the intervention vs.¬†control group using ‚Äú0/1‚Äù coding\n\n\n\n\n\nhttps://jamanetwork.com/journals/jama/fullarticle/2752474"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#toolkit",
    "href": "notes/slides/lec01-welcome.html#toolkit",
    "title": "Welcome to STA 221",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#assessments",
    "href": "notes/slides/lec01-welcome.html#assessments",
    "title": "Welcome to STA 221",
    "section": "Assessments",
    "text": "Assessments\n\n\n\n\n\n\n\nAssignment\nDescription\n\n\n\n\nHomework (25%)\nIndividual take-home assignments, submitted to Gradescope.\n\n\nMidterms (45%)\nTwo exams with an in-class and take-home component.\n\n\nFinal project (15%)\nTeam-based final project.\n\n\nQuizzes (5%)\nIn-class pop-quizzes.\n\n\nLabs (10%)\nExercises assigned in lab, submitted to Gradescope."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#community",
    "href": "notes/slides/lec01-welcome.html#community",
    "title": "Welcome to STA 221",
    "section": "Community",
    "text": "Community\nUphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will automatically result in a 0 for the assignment and will be reported to the Office of Student Conduct for further action."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#team-work-policy",
    "href": "notes/slides/lec01-welcome.html#team-work-policy",
    "title": "Welcome to STA 221",
    "section": "Team work policy",
    "text": "Team work policy\nThe final project and several labs will be completed in teams. All group members are expected to participate equally. Commit history may be used to give individual team members different grades. Your grade may differ from the rest of your group."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "href": "notes/slides/lec01-welcome.html#sharing-reusing-code",
    "title": "Welcome to STA 221",
    "section": "Sharing / reusing code",
    "text": "Sharing / reusing code\n\nThe use of online resources (including generative AI, as well as static webpages like Stack-Overflow, etc.) is strictly prohibited on in-class quizzes and exams. For take home assignments, you may make use of online resources for coding portions on assignments. If you directly use code from a source (or use it as inspiration), you must explicitly cite where you obtained the code. If you used generative AI to create the code, you should include your prompt(s) in your citation as well.\nAny recycled code that is discovered and is not explicitly cited will be treated as plagiarism, regardless of source.\nNarrative (non-code solutions) should always be entirely your own.\n\n\n\n\n\n\n\nWarning\n\n\nExtensive use of AI on take-home assessments will likely set you up for poor performance on graded in-class assignments."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#late-policy",
    "href": "notes/slides/lec01-welcome.html#late-policy",
    "title": "Welcome to STA 221",
    "section": "Late policy",
    "text": "Late policy\n\nHomeworks and labs can be turned in within 48 hours of the deadline for grade penalty (5% off per day).\nExams and the final project cannot be turned in late and can only be excused under exceptional circumstances.\nThe Duke policy for illness requires a short-term illness report or a letter from the Dean; except in emergencies, all other absenteeism must be approved in advance (e.g., an athlete who must miss class may be excused by prior arrangement for specific days). For emergencies, email notification is needed at the first reasonable time.\nLast minute coding/rendering issues will not be granted extensions."
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#course-toolkit",
    "href": "notes/slides/lec01-welcome.html#course-toolkit",
    "title": "Welcome to STA 221",
    "section": "Course toolkit",
    "text": "Course toolkit\n\n\n\nResource\nDescription\n\n\n\n\ncourse website\ncourse notes, deadlines, assignments, office hours, syllabus\n\n\nCanvas\nclass recordings, solutions, announcements, Ed Discussion\n\n\ncourse organization\nassignments, collaboration\n\n\nRStudio containers*\nonline coding platform\n\n\n\n*You are welcome to install R and RStudio locally on your computer. If working locally you should make sure that your environment meets the following requirements:\n\nlatest R version\nlatest RStudio\nworking git installation\nability to create ssh keys (for GitHub authentication)\nAll R packages updated to their latest version from CRAN"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "href": "notes/slides/lec01-welcome.html#communication-and-missing-class",
    "title": "Welcome to STA 221",
    "section": "Communication and missing class",
    "text": "Communication and missing class\nIf you have questions about homework/lab exercises, debugging, or any question about course materials\n\ncome to office hours\nask on Ed Discussion\n\n\n\n\n\n\n\n\nWarning\n\n\nThe teaching team will not debug via email.\n\n\n\n\n\nWhen you miss a class:\n\nwatch the recorded lecture on Canvas\ncome to office hours / post on Ed Discussion / ask a friend about missed content"
  },
  {
    "objectID": "notes/slides/lec01-welcome.html#exercise",
    "href": "notes/slides/lec01-welcome.html#exercise",
    "title": "Welcome to STA 221",
    "section": "Exercise",
    "text": "Exercise\n\nbikeshare = readr::read_csv(\"https://sta221-fa25.github.io/data/bikeshare-2012.csv\")\n\n\n\n\n\nAlexander, Rohan. 2023. ‚ÄúTelling Stories with Data,‚Äù June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. ‚ÄúOpinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.‚Äù Journal of Statistics and Data Science Education 30 (3): 241‚Äì50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "notes/lec09-normal.html",
    "href": "notes/lec09-normal.html",
    "title": "The normal assumption",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "notes/lec09-normal.html#background-the-normal-distribution",
    "href": "notes/lec09-normal.html#background-the-normal-distribution",
    "title": "The normal assumption",
    "section": "Background: the normal distribution",
    "text": "Background: the normal distribution\n\\[\nX \\sim N(\\mu, \\sigma^2)\n\\]\nWe read the above mathematical statement: ‚Äúthe random variable X is normally distributed with mean mu and variance sigma-squared.‚Äù\n\nExampleCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata.frame(x = -8:12) |&gt;\n  ggplot(aes(x = x)) + \n  stat_function(fun = dnorm, args = list(mean = 3, sd = 2)) +\n  theme_bw() +\n  labs(title = \"X ~ N(3, 4)\", y = \"density\")\n\n\n\n\n\nSampling from a normal in R\nThe normal distribution, also known as ‚ÄúGaussian distribution‚Äù is a distribution of a continuous random variable. The sample space of a normal random variable is \\(\\{- \\infty, + \\infty \\}\\) and is defined by two parameters: a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\). The mean is known as the location parameter while the standard deviation is the scale.\nWe can sample N times from a normal with mean mu and standard deviation sigma using rnorm(n = N, mean = mu, sd = sigma).\n\nrnorm(n = 1000, mean = 10, sd = 1) |&gt;\n  hist()\n\n\n\n\n\n\n\n\n\n\nProperties\nA very useful property of Normal distribution is that it is stable. What this means is that linear combinations of normal random variables are themselves normal. In other words, if \\(X\\) and \\(Y\\) are normal random variables then \\(aX + bY\\) is normal for all \\(a\\) and \\(b\\). The two properties to remember when adding Gaussian random variables are:\n\nmean(X + Y) = mean(X) + mean(Y)\nvariance(X + Y) = variance(X) + variance(Y) when X and Y are independent\n\nWe can see this in an example.\nLet \\(X \\sim N(5, 9)\\) and let \\(Y \\sim N(-5, 1)\\)\nAccording to our rules above \\(X + Y \\sim N(0, 10)\\). Let‚Äôs check ourselves with code:\n\nset.seed(1)\nnormal_df = data.frame(X = rnorm(1000, mean = 5, sd = 3),\n                 Y = rnorm(1000, mean = -5, sd = 1))\n\n\n\nnormal_df = normal_df %&gt;%\n  mutate(Z = X + Y)\n\nnormal_df %&gt;%\n  ggplot(aes(x = Z)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  theme_bw() +\n  labs(y = \"Density\", title = \"Matching densities\") +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = sqrt(10)),\n                color = \"darkblue\")\n\n\n\n\n\n\n\n\n\nExercise\n\n\nLet \\(Z \\sim N(0, 1)\\) and \\(X \\sim N(10, 16)\\). \\(aZ + b \\stackrel{d}{=} X\\) for some \\(a\\) and \\(b\\). What are \\(a\\) and \\(b\\)? Fill them in for the ? in the code below and the uncomment the code. Here, \\(\\stackrel{d}{=}\\) means ‚Äúequal in distribution‚Äù.\n\nset.seed(221)\nnormal_df = data.frame(z = rnorm(1000, mean = 0, sd = 1),\n                 x = rnorm(1000, mean = 10, sd = 4))\n\nsample_mean = normal_df %&gt;%\n  summarize(sample_mean = mean(x)) %&gt;%\n  pull()\n\nnormal_df %&gt;%\n  ggplot(aes(x = x)) +\n  geom_histogram(aes(y = ..density..), fill = 'gold3', alpha = 0.5) +\n  geom_histogram(aes(x = z, y = ..density..), fill = 'steelblue', alpha = 0.7) +\n  #geom_density(aes(x = z*? + ?), color = 'red') + \n  theme_bw() +\n  labs(x = \"\", y = \"\", title = \"Sampling from two normals\")\n\n\n\n\nAnother useful property is that the marginal distribution of a multivariate normal itself normal.\nLet \\[\n\\begin{bmatrix}\nX \\\\\nY\n\\end{bmatrix}\n\\sim\nMVN\\left(\n\\begin{bmatrix}\n\\mu_X \\\\\n\\mu_Y\n\\end{bmatrix},\n\\;\n\\begin{bmatrix}\n\\sigma_X^2 & \\sigma_{XY} \\\\\n\\sigma_{XY} & \\sigma_Y^2\n\\end{bmatrix}\n\\right).\n\\]\nThen the marginal distributions are \\[\nX \\sim N(\\mu_X, \\sigma_X^2),\n\\qquad\nY \\sim N(\\mu_Y, \\sigma_Y^2).\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: if you‚Äôd like to simulate from a multivariate normal, you can use the function rmvnorm() from the mvtnorm package.\n\nmvtnorm::rmvnorm(n = 3, mean = c(0, 10), sigma = \n                   matrix(c(2, .5, .5, 1), ncol = 2))\n\n            [,1]      [,2]\n[1,] -1.64375527  7.934314\n[2,]  2.37437357 10.848478\n[3,]  0.06844319 10.669071"
  },
  {
    "objectID": "notes/lec09-normal.html#computing-probabilities",
    "href": "notes/lec09-normal.html#computing-probabilities",
    "title": "The normal assumption",
    "section": "Computing probabilities",
    "text": "Computing probabilities\npnorm ‚Äúprobability normal‚Äù takes three arguments:\n\nq, mean and sd\n\nand pnorm(q = q, mean = mu, sd = sigma) answers the question:\nIf \\(X \\sim N(\\mu, \\sigma)\\), what is \\(p(X &lt; q)\\)?\nFor example, imagine that the resting heart rates in the classroom are normally distributed with mean 70 beats per minute (bpm) and standard deviation 5 bpm. What‚Äôs the probability a randomly selected individual has a heart rate less than 63 bpm?\nIn math: let \\(X\\) be the bpm of an individual in this class. Assume \\(X \\sim N(70, 25)\\). What is \\(p(X &lt; 63)\\)? Given heartbeats are normally distributed, randomly selecting an individual from the classroom is called ‚Äúdrawing from a normal distribution‚Äù.\nWe can compute this easily:\n\npnorm(63, mean = 70, sd = 5)\n\n[1] 0.08075666\n\n\n0.08 or about 8% chance. In picture, the probability is the proportion of area under the curve shaded:"
  },
  {
    "objectID": "notes/lec09-normal.html#regression-assumption-of-normality",
    "href": "notes/lec09-normal.html#regression-assumption-of-normality",
    "title": "The normal assumption",
    "section": "Regression assumption of normality",
    "text": "Regression assumption of normality\nRecall the main question we had before: what assumptions make \\(\\hat{\\beta}_{OLS}\\) representative of \\(\\beta\\)?\n\nAssumption 1: \\(E[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}\\)\nAssumption 2: \\(\\text{var}[\\boldsymbol{\\varepsilon}] = \\sigma^2 \\boldsymbol{I}\\)\nAssumption 3 (new today): \\(\\boldsymbol{\\varepsilon}\\sim\\) multivariate normal\n\nTaken all together, we can write assumptions 1, 2, and 3 concisely:\n\\[\n\\boldsymbol{\\varepsilon}\\sim MVN(\\boldsymbol{0}, \\sigma^2 \\boldsymbol{I})\n\\] or equivalently,\n\\[\n\\varepsilon_i  \\stackrel{\\text{i.i.d.}}{\\sim} \\; N(0, \\sigma^2) \\text{ for each i}\n\\]\nThe implications of this assumption are:\n\\[\n\\boldsymbol{y}\\sim MVN_n(\\boldsymbol{X}\\beta, \\sigma^2 \\boldsymbol{I})\n\\]\nor equivalently,\n\\[\ny_i \\sim N(x_i^T \\beta, \\sigma^2).\n\\]\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\nMoreover,\n\\[\n\\hat{\\beta} \\sim MVN(\\beta, \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1})\n\\] which implies that the marginal distribution of a single element of the \\(\\hat{\\beta}\\),\n\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, \\sigma^2 C_{jj})\n\\] where the matrix \\(C = (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\) and \\(C_{jj}\\) is the \\(j\\)th diagonal element.\nBased on the properties of a normal described above,\n\\[\n\\frac{\\hat{\\beta}_j - \\beta_j}{\\sqrt{\\sigma^2 C_{jj}}} \\sim N(0, 1)\n\\]\nWhere this is going: checking our assumptions are valid, constructing confidence intervals, and performing hypothesis tests."
  },
  {
    "objectID": "notes/lec09-normal.html#data-generative-model",
    "href": "notes/lec09-normal.html#data-generative-model",
    "title": "The normal assumption",
    "section": "Data generative model",
    "text": "Data generative model\nAn important view of the equation above,\n\\[\ny_i \\sim N(x_i^T \\beta, \\sigma^2),\n\\] is that it can be used to generate data. For this reason, the statistical model is often called a ‚Äúdata generative model‚Äù.\nIt looks like this:\n\ncreate length n vector \\(x\\). For example, x &lt;- runif(n, 0, 10)\nspecify the true \\(\\beta\\), e.g.¬†beta0 &lt;- 1; beta1 &lt;- 2\nchoose some error \\(\\sigma^2\\) and generate \\(y_i\\) according to the data generative model: rnorm(n, mean = (beta0 + beta1 * x, sd = 2)).\n\nFrom here you can fit the data with lm(y ~ x) and examine the results."
  },
  {
    "objectID": "notes/lec07-intro-inference.html",
    "href": "notes/lec07-intro-inference.html",
    "title": "Introduction to inference",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT)\nlibrary(latex2exp)\nlibrary(patchwork)\n\npokemon &lt;- read_csv(\"https://sta221-fa25.github.io/data/pokemon_data.csv\") # complete, population data"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#the-data",
    "href": "notes/lec07-intro-inference.html#the-data",
    "title": "Introduction to inference",
    "section": "The data",
    "text": "The data\nAs of 2025, there are 1025 pokemon in existence.\nIn all statistical inference tasks, we only have a sample from the population. Let‚Äôs consider a random sample of the pokemon data, given below:\n\nset.seed(48) \npokemon_sample &lt;- pokemon |&gt;\n  slice_sample(n = 15) |&gt;\n  select(dexnum, name, height_m, weight_kg) |&gt;\n  arrange(dexnum)\n  \n\ndatatable(pokemon_sample, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"sample of 15 pokemon\")\n\n\n\n\n\nLet‚Äôs investigate the question: are heavier pokemon taller?\n\nplotlinear modelplot code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlm(height_m ~ weight_kg, data = pokemon_sample) |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  0.692    0.0859        8.06 2.07e- 6\n2 weight_kg    0.00603  0.000369     16.3  4.90e-10\n\n\n\n\n\npokemon_sample |&gt;\n  ggplot(aes(x = weight_kg, y = height_m)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE, color = 'steelblue') +\n  theme_bw() +\n  labs(title = \"Pokemon weight vs height\", y = \"height (m)\", x = \"weight (kg)\")"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#question",
    "href": "notes/lec07-intro-inference.html#question",
    "title": "Introduction to inference",
    "section": "Question",
    "text": "Question\nHow can we tell if our estimates \\(\\hat{\\beta}\\) are any good?\n\nrepeated samplingplotspopulation-level annotation\n\n\n\npokemon_hw = pokemon |&gt;\n  select(height_m, weight_kg)\n\nBETA_HAT &lt;- NULL\nfor(i in 1:1000) {\n  fit &lt;- pokemon_hw |&gt;\n    slice_sample(n = 15) |&gt; \n    lm(height_m ~ weight_kg, data = _) \n  BETA_HAT &lt;- rbind(BETA_HAT, fit$coefficients)\n}\n\nBETA_HAT &lt;- data.frame(BETA_HAT)\ncolnames(BETA_HAT) &lt;- c(\"beta0\", \"beta1\")\n\nglimpse(BETA_HAT)\n\nRows: 1,000\nColumns: 2\n$ beta0 &lt;dbl&gt; 0.9016025, 0.5209848, 0.6775509, 0.8639395, 0.4678607, 0.4351285‚Ä¶\n$ beta1 &lt;dbl&gt; 0.004823824, 0.010159875, 0.008377374, 0.012071556, 0.016425420,‚Ä¶\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npopn_model &lt;- lm(height_m ~ weight_kg, data = pokemon)\npopn_model$coefficients\n\n(Intercept)   weight_kg \n0.775612595 0.006509202"
  },
  {
    "objectID": "notes/lec07-intro-inference.html#framework",
    "href": "notes/lec07-intro-inference.html#framework",
    "title": "Introduction to inference",
    "section": "Framework",
    "text": "Framework\nOur objective is to infer properties about a population using data from an experiment or survey (in this case, a survey/sample of pokemon).\nPost-experiment: after collecting the data, \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) are fixed and known.\nPre-experiment: before collecting the data, the data are unknown and random. \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\), which are functions of the data, are also unknown and random.\nIn all cases, the true population parameters, \\(\\beta_0, \\beta_1\\) are fixed but unknown.\nPre-experimental question: is the probability distribution of \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) a meaningful representation of the population?\nAnswer: this depends on certain assumptions we make about population.\n\n\n\n\n\n\nAssumption 1\n\n\n\n\\(E[\\boldsymbol{\\varepsilon}|\\boldsymbol{x}] = \\boldsymbol{0}\\), or equivalently, \\(E[\\varepsilon_i|\\boldsymbol{x}] = 0\\) for all \\(i\\).\n\n\nImplications: in simple linear regression, assumption 1 implies that \\(E[y_i|\\boldsymbol{x}] = E[\\beta_0 + \\beta_1 x_i + \\epsilon_i|\\boldsymbol{x}] = E[\\beta_0 |\\boldsymbol{x}] + E[\\beta_1 x_i|\\boldsymbol{x}] + E[\\epsilon_i|\\boldsymbol{x}] = \\beta_0 + \\beta_1 x_i\\).\n\nExerciseSolution\n\n\nShow that assumption 1 implies that \\(E[\\hat{\\beta}|\\boldsymbol{X}] = \\beta\\).\n\n\n\\[\n\\begin{aligned}\nE[\\hat{\\beta}|\\boldsymbol{X}] &= E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E[\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{X}\\beta\\\\\n&= \\beta\n\\end{aligned}\n\\]\n\n\n\nSince \\(E[\\hat{\\beta}|\\boldsymbol{X}] = \\beta\\), we say \\(\\hat{\\beta}\\) is an unbiased estimator of \\(\\beta\\).\nNotice that this result (unbiasedness) does not depend independence of errors, normality, or constant variance.\nHowever, the result doesn‚Äôt tell us anything about how close \\(\\hat{\\beta}\\) will be to \\(\\beta\\). The estimator may be unbiased, but by itself, this doesn‚Äôt tell us much. See examples of ‚Äúunbiased‚Äù distributions of an estimator below."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html",
    "href": "notes/lec05-matrix-form-regression.html",
    "title": "Matrix algebra of regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "href": "notes/lec05-matrix-form-regression.html#learning-objectives",
    "title": "Matrix algebra of regression",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nwrite down simple linear regression in matrix form\nunderstand the geometry of OLS regression\nbe able to explain the following vocabulary: design matrix, Hessian matrix, projection"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "href": "notes/lec05-matrix-form-regression.html#simple-linear-regression-p-2",
    "title": "Matrix algebra of regression",
    "section": "Simple linear regression (p = 2)",
    "text": "Simple linear regression (p = 2)\nWe write down simple linear regression\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon},\n\\]\nwhere\n\n\\(\\boldsymbol{y}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\\(\\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\nand \\(p = 2\\), i.e.¬†there‚Äôs an intercept and 1 slope. More explicitly:\n\\[\n\\mathbf{y} = \\mathbf{X}\\beta + \\boldsymbol{\\varepsilon},\n\\]\n\\[\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n\nDefinition: design matrix\n\n\n\n\\(\\boldsymbol{X}\\) is called the ‚Äúdesign matrix‚Äù, ‚Äúcovariate matrix‚Äù, ‚Äúmodel matrix‚Äù or even sometimes the ‚Äúdata matrix‚Äù. It includes columns each predictors and an intercept (if applicable)."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "href": "notes/lec05-matrix-form-regression.html#linear-algebra-background",
    "title": "Matrix algebra of regression",
    "section": "Linear algebra background",
    "text": "Linear algebra background\n\nIdentity Matrix\nThe identity matrix of size \\(n \\times n\\), denoted \\(\\mathbf{I}_n\\), is a square matrix with ones on the diagonal and zeros elsewhere:\n\\[\n\\mathbf{I}_n =\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}.\n\\]\nFor any vector \\(\\mathbf{v}\\),\n\\[\n\\mathbf{I}_n \\mathbf{v} = \\mathbf{v}.\n\\]\n\n\n\nMatrices as Linear Operators\nAn \\(m \\times n\\) matrix \\(\\mathbf{A}\\) represents a linear operator that maps vectors to vectors:\n\\[\n\\mathbf{A} : \\mathbb{R}^n \\to \\mathbb{R}^m, \\quad \\mathbf{v} \\mapsto \\mathbf{A}\\mathbf{v}.\n\\]\nMatrix multiplication distributes over addition:\n\\[\n\\mathbf{A}(\\mathbf{u} + \\mathbf{v}) = \\mathbf{A}\\mathbf{u} + \\mathbf{A}\\mathbf{v},\n\\]\nand scalar multiplication:\n\\[\n\\mathbf{A}(c \\mathbf{v}) = c (\\mathbf{A}\\mathbf{v}).\n\\]\n\n\n\nMatrix Inverses\nA square matrix \\(\\mathbf{A}\\) of size \\(n \\times n\\) has an inverse \\(\\mathbf{A}^{-1}\\) if\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n.\n\\]\nOnly full-rank matrices (rank \\(n\\)) are invertible.\nIf \\(\\det(\\mathbf{A}) = 0\\), then \\(\\mathbf{A}\\) is singular and has no inverse.\nTo invert a matrix in R, use solve(), for example:\n\n(A = matrix(c(1,0,3,4), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    0    4\n\n\n\nsolve(A)\n\n     [,1]  [,2]\n[1,]    1 -0.75\n[2,]    0  0.25\n\n\nCheck:\n\nsolve(A) %*% A\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\n\n\nSymmetric Matrices\nA square matrix \\(\\mathbf{A}\\) is symmetric if\n\\[\n\\mathbf{A} = \\mathbf{A}^T.\n\\]\nThat is, \\(\\mathbf{A}_{ij} = \\mathbf{A}_{ji}\\) for all \\(i,j\\).\nTo take a transpose in R, use t(A), for example:\n\n(A = matrix(c(1,2,3,4), ncol = 2))\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\n\nt(A)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\n\n\n\nPositive Definite and Positive Semi-Definite Matrices\nA symmetric matrix \\(\\mathbf{A}\\) is\n\nPositive Definite (PD) if\n\\[\n\\mathbf{x}^T \\mathbf{A} \\mathbf{x} &gt; 0 \\quad \\text{for all } \\mathbf{x} \\neq \\mathbf{0}.\n\\]\nPositive Semi-Definite (PSD) if\n\\[\n\\mathbf{x}^T \\mathbf{A} \\mathbf{x} \\geq 0 \\quad \\text{for all } \\mathbf{x}.\n\\]\n\nEquivalently, all eigenvalues of \\(\\mathbf{A}\\) are positive (the matrix is PD) or nonnegative (the matrix is PSD).\n\n\n\nVector Orthogonal to a Matrix\nA vector \\(\\mathbf{v} \\in \\mathbb{R}^n\\) is said to be orthogonal to a matrix \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times p}\\) if it is orthogonal to every column of \\(\\mathbf{A}\\).\nThat is,\n\\[\n\\mathbf{v}^T \\mathbf{a}_j = 0 \\quad \\text{for each column } \\mathbf{a}_j \\text{ of } \\mathbf{A}.\n\\]\nEquivalently,\n\\[\n\\mathbf{A}^T \\mathbf{v} = \\mathbf{0}.\n\\]\nIn this case, \\(\\mathbf{v}\\) lies in the orthogonal complement of the column space of \\(\\mathbf{A}\\), often written as\n\\[\n\\mathbf{v} \\in \\text{Col}(\\mathbf{A})^\\perp.\n\\]"
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "href": "notes/lec05-matrix-form-regression.html#matrix-calculus",
    "title": "Matrix algebra of regression",
    "section": "Matrix calculus",
    "text": "Matrix calculus\nLet \\(\\mathbf{x} \\in \\mathbb{R}^n\\) be a column vector and \\(\\mathbf{A} \\in \\mathbb{R}^{n \\times n}\\) a matrix.\nGradients with respect to vectors are written as\n\\[\n\\frac{\\partial f}{\\partial \\mathbf{x}} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\\n\\frac{\\partial f}{\\partial x_2} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial x_n}\n\\end{bmatrix}.\n\\]\n\n\n1. Derivative of an Inner Product\nConsider the scalar function\n\\[\nf(\\mathbf{x}) = \\mathbf{a}^T \\mathbf{x},\n\\]\nwhere \\(\\mathbf{a} \\in \\mathbb{R}^n\\).\n\nDerivative with respect to \\(\\mathbf{x}\\):\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{a}^T \\mathbf{x}) = \\mathbf{a}.\n\\]\nEquivalently,\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{a}) = \\mathbf{a}.\n\\]\n\n\n\n2. Derivative of a Quadratic Form\nConsider the quadratic form\n\\[\nf(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}.\n\\]\n\nIf \\(\\mathbf{A}\\) is not assumed symmetric:\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = (\\mathbf{A} + \\mathbf{A}^T)\\mathbf{x}.\n\\]\n\nIf \\(\\mathbf{A}\\) is symmetric (\\(\\mathbf{A} = \\mathbf{A}^T\\)):\n\n\\[\n\\frac{\\partial}{\\partial \\mathbf{x}} \\, (\\mathbf{x}^T \\mathbf{A} \\mathbf{x}) = 2 \\mathbf{A} \\mathbf{x}.\n\\]\nFor all else, see the matrix cookbook."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "href": "notes/lec05-matrix-form-regression.html#ols-in-matrix-form",
    "title": "Matrix algebra of regression",
    "section": "OLS in Matrix form",
    "text": "OLS in Matrix form\nThe ordinary least squares (OLS) estimator is defined as\n\\[\n\\hat{\\beta}_{\\text{OLS}}\n= \\arg \\min_{\\beta} \\; \\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon},\n\\]\nwhere\n\\[\n\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\beta.\n\\]\nNotice this is equivalent to the sum of squares we saw before:\n\\[\n\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon} =\n\\begin{bmatrix}\n\\varepsilon_1 & \\varepsilon_2 & \\cdots & \\varepsilon_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n= \\sum_{i=1}^n \\varepsilon_i^2.\n\\]\nTo optimize, we need to take the derivative with respect to the vector \\(\\beta\\) and set it equal to zero. To begin, we differentiate,\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial{\\beta}}\n\\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n(\\boldsymbol{y}- \\boldsymbol{X}\\beta)^T(\\boldsymbol{y}- \\boldsymbol{X}\\beta)\\\\\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n\\left(\n\\boldsymbol{y}^T \\boldsymbol{y}- \\boldsymbol{y}^T \\boldsymbol{X}\\beta - \\beta^T \\boldsymbol{X}^T \\boldsymbol{y}+ \\beta^T \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&=\n\\frac{\\partial}{\\partial{\\beta}}\n\\left(\n\\boldsymbol{y}^T \\boldsymbol{y}\n-2 \\beta^T \\boldsymbol{X}^T \\boldsymbol{y}+ \\beta^T \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&=\n-2 \\boldsymbol{X}^T \\boldsymbol{y}+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\beta.\n\\end{aligned}\n\\]\nWhen we set the derivative equal to zero and solve,\n\\[\n\\begin{aligned}\n-2 \\boldsymbol{X}^T \\boldsymbol{y}+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\hat{\\beta} &= 0 \\implies\\\\\n\\hat{\\beta} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\end{aligned}\n\\]\n\nDid we find a minimum?\nQuestion: this may be some optima, but did we find the value that minimizes the sum of squared residuals?\nAnswer: check the Hessian matrix. The Hessian matrix is the square matrix of partial second derivatives.\n\\[\n\\begin{aligned}\n\\frac{\\partial^2}{\\partial{\\beta}^2} \\boldsymbol{\\varepsilon}^T \\boldsymbol{\\varepsilon}\n&=\n\\frac{\\partial}{\\partial{\\beta}} \\left(\n-2 \\boldsymbol{X}^T \\boldsymbol{y}\n+ 2 \\boldsymbol{X}^T \\boldsymbol{X}\\beta\n\\right)\\\\\n&= 2 \\boldsymbol{X}^T \\boldsymbol{X}\n\\end{aligned}\n\\]\n\nIf the Hessian is positive definite, we have a minimum.\nIf the Hessian is negative definite, we have a maximum.\nIf the Hessian is neither, then we have a saddle point.\n\nExercise on homework 1: show that the Hessian matrix \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) is positive definite."
  },
  {
    "objectID": "notes/lec05-matrix-form-regression.html#geometry",
    "href": "notes/lec05-matrix-form-regression.html#geometry",
    "title": "Matrix algebra of regression",
    "section": "Geometry",
    "text": "Geometry\nSee handwritten notes."
  },
  {
    "objectID": "notes/lec03-slr.html",
    "href": "notes/lec03-slr.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec03-slr.html#notation",
    "href": "notes/lec03-slr.html#notation",
    "title": "Simple linear regression",
    "section": "Notation",
    "text": "Notation\n\\(\\boldsymbol{y}\\): a vector of observations\n\\(\\boldsymbol{y}= [y_1, y_2, \\ldots, y_n]^T = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\\). We say ‚Äúy is of dimension n‚Äù.\n\\(\\hat{\\boldsymbol{y}}\\): vector of ‚Äúfitted‚Äù outcomes or ‚Äúpredicted‚Äù response variable.\n\\(\\boldsymbol{1}= \\begin{bmatrix}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{bmatrix}\\). \\(\\boldsymbol{1}\\) is of dimension of n.\n\n\n\n\n\n\nNote\n\n\n\nFor the handwritten in-class notes, we use the convention that a line underneath the symbol represents a vector."
  },
  {
    "objectID": "notes/lec03-slr.html#last-time",
    "href": "notes/lec03-slr.html#last-time",
    "title": "Simple linear regression",
    "section": "Last time",
    "text": "Last time\n\\(|cor(\\boldsymbol{x},\\boldsymbol{y})| = 1 \\iff \\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}\\) for some \\(\\beta_0\\), \\(\\beta_1\\). See Figure 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is more typical that \\(|cor(\\boldsymbol{x}, \\boldsymbol{y})| &lt; 1\\), in which case \\(\\boldsymbol{y}= \\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}+ \\boldsymbol{\\varepsilon}\\), where \\(\\boldsymbol{\\varepsilon}\\) is the error vector. See Figure 2 for an example. The observation by observation representation is given by the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i.\\]\n\n\n\n\n\n\n\n\n\nQuestion: what line, (i.e.¬†what (\\(\\beta_0, \\beta_1\\))) provides the ‚Äúbest fit‚Äù?\nAnswer: the set (\\(\\beta_0, \\beta_1\\)) that satisfy an objective function.\n\n\n\n\n\n\nDefinition: objective function\n\n\n\nAn objective function is some function we want to optimize.\nExample 1: least absolute value (LAV) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n |\\varepsilon_i|\n\\]\nExample 2: Ordinary least squares (OLS) regression\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]"
  },
  {
    "objectID": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "href": "notes/lec03-slr.html#ordinary-least-squares-ols-regression-line",
    "title": "Simple linear regression",
    "section": "Ordinary least squares (OLS) regression line",
    "text": "Ordinary least squares (OLS) regression line\n\\(\\sum_{i=1}^n \\varepsilon_i^2\\) is called the ‚Äúresidual sum of squares‚Äù or RSS for short.\n\\[\n\\begin{aligned}\nRSS(\\beta_0, \\beta_1) &= \\sum_{i=1}^n \\varepsilon_i^2\\\\\n&= \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_i))^2\\\\\n&= (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))^T (\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))\\\\\n&= ||(\\boldsymbol{y}- (\\beta_0 \\boldsymbol{1}+ \\beta_1 \\boldsymbol{x}))||^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nDefinition: OLS estimates\n\n\n\nThe OLS values of \\(\\beta_0\\), \\(\\beta_1\\) are the values \\((\\hat{\\beta}_0, \\hat{\\beta}_1)\\) that minimize \\(RSS(\\beta_0, \\beta_1)\\). Again, in math,\n\\[\n\\hat{\\beta_0}, \\hat{\\beta_1}\n= \\operatorname*{arg\\,min}_{\\beta_0, \\beta_1}\n\\sum_{i=1}^n \\varepsilon_i^2\n\\]\n\n\nQuestion: How can we find the OLS line?\nAnswer: (1) Geometry (we‚Äôll do this later); (2) calculus\n\nComputing the OLS estimates using calculus\n\n\n\n\n\n\n\nlm(tmax ~ tmin, data = weather)\n\n\nCall:\nlm(formula = tmax ~ tmin, data = weather)\n\nCoefficients:\n(Intercept)         tmin  \n    61.1017       0.3729  \n\n\n\nNotice that the RSS is a quadratic (convex) function of \\(\\beta_0, \\beta_1\\).\nThe global minimum occurs where the derivative (gradient) equals zero.\n\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta_0} RSS &=  -2 \\sum (y_i - (\\beta_0 + \\beta_1 x_i)) = 0\\\\\n\\frac{\\partial}{\\partial \\beta_1} RSS &=  -2 \\sum x_i(y_i - (\\beta_0 + \\beta_1 x_i)) = 0\n\\end{aligned}\n\\]\nTherefore the OLS values \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) will satisfy the normal equations,\n\\[\n\\begin{align}\n\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{1}\\\\\n\\sum_{i=1}^n x_i \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) &= 0 \\tag{2}\n\\end{align}\n\\] Question: why are these called the normal equations?\nAnswer: Let \\(\\hat{\\varepsilon}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\) then,\n\\[\n\\begin{aligned}\n(1) &\\implies \\sum \\hat{\\varepsilon}_i  = \\hat{\\boldsymbol{\\varepsilon}}\\cdot \\boldsymbol{1}= 0\\\\\n(2) &\\implies \\sum x_i \\hat{\\varepsilon}_i = \\boldsymbol{x}^T \\hat{\\boldsymbol{\\varepsilon}}= 0.\n\\end{aligned}\n\\]\nIn words, the residual vector \\(\\hat{\\boldsymbol{\\varepsilon}}\\) is normal (orthogonal) to the vectors \\(\\boldsymbol{1}\\) and \\(\\boldsymbol{x}\\).\n\nExerciseSolution\n\n\nShow that the OLS regression line goes through \\(\\bar{x}, \\bar{y}\\). Reminder: \\(\\bar{x} = \\frac{1}{n}\\sum x_i\\) and \\(\\bar{y} = \\frac{1}{n} \\sum y_i\\)\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n(1) \\implies &\\sum y_i - \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i) = 0\\\\\n&\\sum y_i = \\sum(\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\\\\n& n\\bar{y} = n \\hat{\\beta_0} + n \\hat{\\beta_1} \\bar{x}\\\\\n&\\bar{y} = \\hat{\\beta_0} + \\hat{\\beta_1} \\bar{x}.\n\\end{aligned}\n\\] Note the commonly used ‚Äútrick‚Äù:\n\\(\\sum y_i = n \\bar{y}\\).\n\n\n\nSolving the normal equations\n\\[\n\\begin{aligned}\n\\text{From (1): } &\\sum_{i=1}^n \\big(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1} x_i)\\big) = 0\\\\\n& \\bar{y}  = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\bar{x}\\\\\n&\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\] Plugging this result into (2):\n\\[\n\\begin{aligned}\n&\\sum x_i (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i)) = 0\\\\\n&\\sum x_i (y_i - \\bar{y} + \\hat{\\beta}_1 \\bar{x} - \\hat{\\beta}_1 x_i) = 0\\\\\n&\\sum x_i (y_i - \\bar{y}) = \\hat{\\beta}_1 \\sum x_i (x_i - \\bar{x}) \\text{    }& (*)\n\\end{aligned}\n\\]\nNotice the trick:\n\\[\n\\begin{aligned}\n\\sum (x_i - \\bar{x}) (y_i - \\bar{y}) &= \\sum x_i (y_i - \\bar{y}) - \\sum \\bar{x}(y_i - \\bar{y})\\\\\n&= \\sum x_i (y_i - \\bar{y})  - \\bar{x} (0)\\\\\n&= \\sum x_i (y_i - \\bar{y}).\n\\end{aligned}\n\\] Similarly,\n\\[\n\\begin{aligned}\n\\sum x_i (x_i - \\bar{x}) &= \\sum (x_i - \\bar{x})(x_i - \\bar{x})\\\\\n&= \\sum (x_i - \\bar{x})^2.\n\\end{aligned}\n\\]\nLet\n\\[\n\\begin{aligned}\nS_{xx} &= \\sum (x_i - \\bar{x})^2\\\\\nS_{yy} &= \\sum (y_i - \\bar{y})^2\\\\\nS_{xy} &= \\sum (x_i - \\bar{x}) (y_i - \\bar{y})\n\\end{aligned}\n\\] Then \\((*)\\) above says \\(S_{xy} = \\hat{\\beta}_1 S_{xx}\\), implying that the OLS values are\n\\[\n\\begin{aligned}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\\\\n\\hat{\\beta}_1 &= \\frac{S_{xy}}{S_{xx}}.\n\\end{aligned}\n\\]\nAn important take-away: the slope is closely related to the correlation. Notice\n\\[\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n\\]\nThe numerator looks like covariance, or correlation between \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). The denominator looks like \\(var(\\boldsymbol{x})\\). If we multiply by the number ‚Äú1‚Äù in a fancy way, the relationship becomes clear,\n\\[\n\\begin{aligned}\n\\frac{S_{yy}^{1/2}}{S_{yy}^{1/2}} \\cdot \\frac{S_{xy}}{S_{xx}^{1/2} S_{xx}^{1/2}}\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot \\frac{S_{xy}}{\\left(S_{xx} S_{yy}\\right)^{1/2}}\\\\\n&= \\left(\\frac{S_{yy}}{S_{xx}}\\right)^{1/2}  \\cdot cor(\\boldsymbol{x},\\boldsymbol{y})\n\\end{aligned}\n\\]\nSummary: what do you need to find \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)?\n\n\\(\\bar{x}\\), \\(\\bar{y}\\)\n\\(S_{xx}, S_{yy}\\)\n\\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\)\n\nWe can write the fitted regression line in terms of these quantities:\n\\[\n\\begin{aligned}\n\\hat{y}_i &= \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y} -\\hat{\\beta}_1 \\bar{x} + \\hat{\\beta}_1 x_i\\\\\n&= \\bar{y}  +\\hat{\\beta}_1 (x_i - \\bar{x})\\\\\n&= \\bar{y} + \\left(S_{yy}/S_{xx} \\right)^{1/2} \\cdot cor(\\boldsymbol{x}, \\boldsymbol{y}) \\cdot (x_i - \\bar{x})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/construction/anova.html",
    "href": "notes/construction/anova.html",
    "title": "Analysis of variance (ANOVA1)",
    "section": "",
    "text": "Show libraries used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(broom)"
  },
  {
    "objectID": "notes/construction/anova.html#definition",
    "href": "notes/construction/anova.html#definition",
    "title": "Analysis of variance (ANOVA1)",
    "section": "Definition",
    "text": "Definition\nANOVA refers to (1) procedures for fitting and testing linear models in which the independent variables are categorical and (2) partitioning the dependent-variable sum of squares into ‚Äúexplained‚Äù and ‚Äúunexplainbed‚Äù components."
  },
  {
    "objectID": "notes/construction/anova.html#one-way-anova-example",
    "href": "notes/construction/anova.html#one-way-anova-example",
    "title": "Analysis of variance (ANOVA1)",
    "section": "One-way ANOVA example",
    "text": "One-way ANOVA example\nOne-way ANOVA means that we have exactly 1 categorical dependent variable. As an example, consider the penguin data set. We‚Äôll take our outcome variable, \\(y\\), to be the body mass of the penguin in grams body_mass_g and the dependent variable \\(x\\) to be categorical species of penguin species. Notice there are 3 species of penguins in this data set: Adelie, Chinstrap and Gentoo.\n\npenguin_subset = penguins %&gt;%\n  select(body_mass_g, species) %&gt;%\n  drop_na()\n\npenguin_subset %&gt;%\n  count(species)\n\n# A tibble: 3 √ó 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      151\n2 Chinstrap    68\n3 Gentoo      123\n\nglimpse(penguin_subset)\n\nRows: 342\nColumns: 2\n$ body_mass_g &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3475, 4250, 3300‚Ä¶\n$ species     &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ad‚Ä¶\n\n\nNext let‚Äôs fit the linear model and report the least squares estimates for each parameter:\n\nmodel = lm(body_mass_g ~ species, data = penguin_subset)\nmodel %&gt;%\n  tidy()\n\n# A tibble: 3 √ó 5\n  term             estimate std.error statistic   p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        3701.       37.6    98.4   2.49e-251\n2 speciesChinstrap     32.4      67.5     0.480 6.31e-  1\n3 speciesGentoo      1375.       56.1    24.5   5.42e- 77\n\n\nANOVA:\n\nanova(model) %&gt;%\n  tidy()\n\n# A tibble: 2 √ó 6\n  term         df      sumsq    meansq statistic   p.value\n  &lt;chr&gt;     &lt;int&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 species       2 146864214. 73432107.      344.  2.89e-82\n2 Residuals   339  72443483.   213698.       NA  NA       \n\n\nManual calculation:\n\n# penguin_subset %&gt;%\n#   group_by(species) %&gt;%\n#   mutate(ybar = mean(body_mass_g))\n\ny &lt;- penguin_subset$body_mass_g\n\nanova_df = penguin_subset %&gt;%\n  group_by(species) %&gt;%\n  summarize(ybar = mean(body_mass_g), n = n()) %&gt;%\n  mutate(y_total_bar = mean(y)) \n\nanova_df\n\n# A tibble: 3 √ó 4\n  species    ybar     n y_total_bar\n  &lt;fct&gt;     &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;\n1 Adelie    3701.   151       4202.\n2 Chinstrap 3733.    68       4202.\n3 Gentoo    5076.   123       4202.\n\nanova_df %&gt;%\n  summarize(sum(n * (ybar - y_total_bar)^2)) # sum sq x\n\n# A tibble: 1 √ó 1\n  `sum(n * (ybar - y_total_bar)^2)`\n                              &lt;dbl&gt;\n1                        146864214.\n\n# sum sq resid: \n\nleft_join(penguin_subset, anova_df, by = \"species\") %&gt;%\n  summarize(rss = sum(((body_mass_g - ybar)^2)))\n\n# A tibble: 1 √ó 1\n        rss\n      &lt;dbl&gt;\n1 72443483.\n\npf(343.6263, 2, 339, log.p = TRUE)\n\n[1] -2.892344e-82\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#introductions",
    "href": "labs/slides/lab0-slides.html#introductions",
    "title": "Welcome to Lab",
    "section": "Introductions",
    "text": "Introductions\n\nMeet the TA!\nIntroduce yourself (icebreaker)\nFollow along these slides on the course website (under slides): sta221-fa25.github.io\nBookmark this! It‚Äôs the course website."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "href": "labs/slides/lab0-slides.html#what-to-expect-in-labs",
    "title": "Welcome to Lab",
    "section": "What to expect in labs",
    "text": "What to expect in labs\n\nIntroduce lab assignment (5-10 minutes, longer today)\nWork on the lab assignment (you can find it on the course website). You will work with others but your submission must be your own for the first few labs.\nTypically you won‚Äôt finish labs in-class and they will be due 1 week after they are released."
  },
  {
    "objectID": "labs/slides/lab0-slides.html#tips",
    "href": "labs/slides/lab0-slides.html#tips",
    "title": "Welcome to Lab",
    "section": "Tips",
    "text": "Tips\n\nRead all instructions on the lab.\nOne work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when we can help you on the spot and leave the narrative writing until later.\nMake use of office hours. Before you need help!"
  },
  {
    "objectID": "labs/slides/lab0-slides.html#beginnings",
    "href": "labs/slides/lab0-slides.html#beginnings",
    "title": "Welcome to Lab",
    "section": "Beginnings",
    "text": "Beginnings\n\nFind the lab instructions here\nFollow the instructions in the lab as I demo."
  },
  {
    "objectID": "labs/lab02.html",
    "href": "labs/lab02.html",
    "title": "Lab 02: Linear regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due Tuesday, September 16 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab02.html#learning-goals",
    "href": "labs/lab02.html#learning-goals",
    "title": "Lab 02: Linear regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nContinue developing a reproducible workflow using RStudio and GitHub\nProduce visualizations and summary statistics to describe distributions\nFit, interpret, and evaluate linear regression models\nUse the matrix representation of the linear regression model to estimate coefficients\nExplore properties of the linear regression model"
  },
  {
    "objectID": "labs/lab02.html#exercise-1",
    "href": "labs/lab02.html#exercise-1",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe begin with univariate exploratory data analysis.\n\nVisualize the distribution of the response variable total_cup_points and calculate summary statistics.\nComment on the features of the distribution of this variable by describing the shape, center, spread, and presence of potential outliers.\nBased on this distribution, do you think the data set is representative of all coffee available to consumers? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nMake sure your data visualizations have clear and informative titles and axis labels."
  },
  {
    "objectID": "labs/lab02.html#exercise-2",
    "href": "labs/lab02.html#exercise-2",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nNow let‚Äôs consider the relationship between how good a coffee smells and its overall quality.\n\nVisualize the relationship between aroma and total_cup_points.\nDoes there appear to be a relationship between a coffee‚Äôs aroma and its overall quality? If so, what is the shape and direction of the relationship?"
  },
  {
    "objectID": "labs/lab02.html#exercise-3",
    "href": "labs/lab02.html#exercise-3",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe have seen the mathematical formulation for simple linear regression in class. In particular, given a response variable \\(Y\\) and predictor variable \\(X\\), the simple linear regression model is \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nfor some unknown regression coefficients \\((\\beta_0, \\beta_1)\\) and error terms \\(\\epsilon\\) that are centered at 0 and have variance \\(\\sigma^2_{\\epsilon}\\) . This means that the expected value of each observation lies on the regression line\n\\[ E(Y|X) = \\beta_0 + \\beta_1 X\\]\nAnswer the following questions about simple linear regression. Your response should be in general terms about regression, not be specific to the coffee data.\n\nWhat does \\(E(Y|X) = \\beta_0 + \\beta_1X\\) mean in terms of a given value of \\(X\\)?\nWhat are the interpretations of the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in terms of the expected value of \\(Y\\)?\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-4",
    "href": "labs/lab02.html#exercise-4",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nFit the model of the relationship between aroma and total_cup_points. Neatly display the output using 3 digits.\nInterpret the slope in the context of the data.\nWhat is the expected total_cup_points for coffees that receive the worst aroma score of 0? Is this a reliable estimate of the total_cup_points for these coffees? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-5",
    "href": "labs/lab02.html#exercise-5",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nNow let‚Äôs add flavor to the model, so we will use both flavor and aroma to understand variability in the overall quality of coffees. Use this model for the remainder of the lab.\nIn class we have seen how vectors and matrices can be used to represent the linear regression model:\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{\\epsilon}\n\\]\n\nState the dimensions of \\(\\mathbf{y}, \\mathbf{X}, \\boldsymbol{\\beta}, \\boldsymbol{\\epsilon}\\) for this model. Your answer should have exact values given the coffee data set.\nCompute the estimated regression coefficients using the matrix form of the model. Show the code used to get the answer.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the model.matrix() function to get the design matrix. The code takes the general form:\n\nmodel.matrix(y ~ x, data = my_data)\n\nSee Lab 01 for other matrix operations in R.\n\n\n\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits.\nWrite the estimated regression equation.\n\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab02.html#exercise-6",
    "href": "labs/lab02.html#exercise-6",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nThe coefficient for aroma for the model fit in Exercise 5 is different than the coefficient from the model fit in Exercise 4. Briefly explain why these coefficients are different.\nWould you willingly drink a coffee represented by the intercept of the model in Exercise 5? Briefly explain why or why not."
  },
  {
    "objectID": "labs/lab02.html#exercise-7",
    "href": "labs/lab02.html#exercise-7",
    "title": "Lab 02: Linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCompute \\(\\mathbf{H}\\), the hat matrix corresponding to the model from Exercise 5. Then use \\(\\mathbf{H}\\) to compute the residuals for this model. Do not print out \\(\\mathbf{H}\\) or the residuals.\nCompute the mean and standard deviation of the residuals.\nRecall root mean square error RMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}}\n\\]\nSimilar to other statistics we‚Äôve seen thus far, we can write the RMSE in matrix form. Compute RMSE for the model from Exercise 5 using matrix form. Show the code used to get the answer.\nHow do the standard deviation of the residuals and RMSE compare?\n\n\nYou‚Äôre done and ready to submit your work! render, commit, and push all remaining changes. You can use the commit message ‚ÄúDone with Lab 02!‚Äù, and make sure you have pushed all the files to GitHub (your Git pane in RStudio should be empty) and that all documents are updated in your repo on GitHub. The PDF document you submit to Gradescope should be identical to the one in your GitHub repo."
  },
  {
    "objectID": "labs/lab00.html",
    "href": "labs/lab00.html",
    "title": "Lab 0: Getting Started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today‚Äôs lab tasks before leaving lab today."
  },
  {
    "objectID": "labs/lab00.html#rstudio",
    "href": "labs/lab00.html#rstudio",
    "title": "Lab 0: Getting Started",
    "section": "RStudio",
    "text": "RStudio\n\n\n\n\n\n\nNote\n\n\n\nR is the name of the programming language itself and RStudio is a convenient graphical user interface (GUI).\n\n\n\nReserve RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers. You will log in using your NetID credentials.\nClick ‚ÄúReserve STA 221‚Äù to reserve an RStudio container. Be sure you reserve the container labeled STA 221 to ensure you have the computing set up you need for the class.\n\nYou only need to reserve a container once per semester.\n\n\nOpen RStudio container\n\nGo to https://cmgr.oit.duke.edu/containers and log in with your Duke NetID and Password.\nClick STA221 to log into the Docker container. You should now see the RStudio environment."
  },
  {
    "objectID": "labs/lab00.html#git-and-github",
    "href": "labs/lab00.html#git-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Git and GitHub",
    "text": "Git and GitHub\nIn addition to R and RStudio, we will use git and GitHub for version control and collaboration.\n\n\n\n\n\n\nNote\n\n\n\nGit is a version control system (like ‚ÄúTrack Changes‚Äù features from Microsoft Word but more powerful) and GitHub is the home for your Git-based projects on the internet (like DropBox but much better).\n\n\n\nSign up for GitHub account\nYou will need a GitHub account to access the assignments, project, and in-class exercises for the course.\n\nIf you do not have a GitHub account, go to https://github.com and sign up for an account.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for advice on choosing a username. tl;dr choose something that you would be proud to show a future employer.\n\n\n\nIf you already have a GitHub account, you can move on to the next step."
  },
  {
    "objectID": "labs/lab00.html#connect-rstudio-and-github",
    "href": "labs/lab00.html#connect-rstudio-and-github",
    "title": "Lab 0: Getting Started",
    "section": "Connect RStudio and GitHub",
    "text": "Connect RStudio and GitHub\nNow that you have RStudio and a GitHub account, we will configure git so that RStudio and GitHub communicate with one another.\n\nSet up your SSH Key\nYou will authenticate GitHub using SSH. An outline of the authentication steps is below; you are encouraged to follow along as your TA demonstrates the steps.\n\n\n\n\n\n\nNote\n\n\n\nYou only need to do this authentication process one time on a single system.\n\n\n\nStep 0: Open your STA 221 RStudio container.\nStep 1: Type credentials::ssh_setup_github() into the console on the bottom left of the RStudio environment.\nStep 2: R will ask ‚ÄúNo SSH key found. Generate one now?‚Äù Click 1 for yes.\nStep 3: You will generate a key. It will begin with ‚Äússh-rsa‚Ä¶.‚Äù R will then ask ‚ÄúWould you like to open a browser now?‚Äù Click 1 for yes.\nStep 4: You may be asked to provide your username and password to log into GitHub. This would be the ones associated with your account that you set up. After entering this information, paste the key in and give it a name. You might name it in a way that indicates where the key will be used, e.g., sta221)\n\n\n\nConfigure git\nThe last thing we need to do is configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your name and email address.\nTo do so, you will use the use_git_config() function from the usethis package.\nType the following lines of code in the console in RStudio filling in your name and the email address associated with your GitHub account.\n\nusethis::use_git_config(\n  user.name = \"Your name\", \n  user.email = \"Email associated with your GitHub account\")\n\nFor example, mine would be\n\nusethis::use_git_config(\n  user.name = \"Alexander Fisher\",\n  user.email = \"alexander.fisher@duke.edu\")\n\nIt may look like nothing happened but you are now ready interact between GitHub and RStudio!"
  },
  {
    "objectID": "labs/lab00.html#getting-started",
    "href": "labs/lab00.html#getting-started",
    "title": "Lab 0: Getting Started",
    "section": "Getting started",
    "text": "Getting started\n\nClick here to create your individual lab-00 repo: https://classroom.github.com/a/PMZfNgLX\nClick to open your lab-00 repo.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File ‚Üí New Project ‚Üí Version Control ‚Üí Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-00.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab00.html#update-the-quarto-document",
    "href": "labs/lab00.html#update-the-quarto-document",
    "title": "Lab 0: Getting Started",
    "section": "Update the Quarto document",
    "text": "Update the Quarto document\n\nTask 1: Change the author name at the top of the document to your name. Render the document. You will see your name at the top of the rendered PDF.\nTask 2: The plot shows the relationship between the daily temperature and number of bike rentals in Washington, D.C.‚Äôs Capital Bikeshare in 2012.\n\n\n\n\n\n\n\n\n\n\nWrite 1 - 2 observations from the plot. Render the document. You will see your response in the rendered PDF."
  },
  {
    "objectID": "labs/lab00.html#commit-and-push-changes-to-github",
    "href": "labs/lab00.html#commit-and-push-changes-to-github",
    "title": "Lab 0: Getting Started",
    "section": "Commit and push changes to GitHub",
    "text": "Commit and push changes to GitHub\n\nOnce you have made your final updates, go to the Git pane in your RStudio instance. This is a tab in the top right corner of the RStudio window.\nCheck the appropriate boxes on every file in the Git pane. All checked files will be sent to GitHub.\nNext, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box.\nClick Commit. Note that every commit needs to have a commit message associated with it.\nNow that you have made an update and committed this change, click Push to send the changes to GitHub.\nGo to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Analysis: Theory and Applications",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\nWeek\nDate\nTopic\nPrepare\nNotes\nAssignment\nProject\n\n\n\n\n1\nTue Aug 26\nwelcome\n\nüíª\n\n\n\n\n\nWed Aug 27\nlab: hello git and R\n\nüíª\nlab 0\n\n\n\n\nThu Aug 28\ncorrelation\nüìñ\nüóí\n\n\n\n\n2\nTue Sep 02\nsimple linear regression\n\nüóí\n\n\n\n\n\nWed Sep 03\nlab: computing and linear algebra review\n\n\nlab 1\n\n\n\n\nThu Sep 04\nmodel assessment\nüìñ\nüóí\n\n\n\n\n3\nTue Sep 09\nmatrix representation\nüìñ\nüóí üìù\nhw 1\n\n\n\n\nWed Sep 10\nlab: linear regression\n\n\nlab 2\n\n\n\n\nThu Sep 11\nmultiple linear regression\n\nüóí üìù\n\n\n\n\n4\nTue Sep 16\nintro to inference\nüìñ\nüóí\n\n\n\n\n\nWed Sep 17\nlab: multiple linear regression\n\n\nlab 3\n\n\n\n\nThu Sep 18\ninference continued\n\nüóí\nhw 2\n\n\n\n5\nTue Sep 23\nthe normal assumption\n\nüóí\n\n\n\n\n\nWed Sep 24\nlab: project workday\n\n\nproject 1\n\n\n\n\nThu Sep 25\nhypothesis tests\n\nüóí\n\n\n\n\n6\nTue Sep 30\nconfidence intervals\n\nüóíüìù\n\nresearch topics due\n\n\n\nWed Oct 01\nlab: interaction effects and exam practice\n\nüíª\nlab 4\n\n\n\n\nThu Oct 02\nexam review\n\n\n\n\n\n\n7\nTue Oct 07\nexam 1\n\n\n\n\n\n\n\nWed Oct 08\nNO LAB (exam)\n\n\n\n\n\n\n\nThu Oct 09\nchecking assumptions\n\nüìù\n\n\n\n\n8\nTue Oct 14\nNO CLASS\n\n\n\n\n\n\n\nWed Oct 15\nlab: project workday\n\n\n\nproject proposal due\n\n\n\nThu Oct 16\nmulticollinearity\n\nüóí\n\n\n\n\n9\nTue Oct 21\nlikelihoods\n\nüìù R\n\n\n\n\n\nWed Oct 22\nlab: variable transformations and interpretation\n\n\nlab 5\n\n\n\n\nThu Oct 23\nmodel selection (AIC)\n\nüóí\n\n\n\n\n10\nTue Oct 28\nridge regression\n\nüóí\nhw 3\n\n\n\n\nWed Oct 29\nlab: project workday\n\n\n\n\n\n\n\nThu Oct 30\ngeneralized least squares\n\n\n\n\n\n\n11\nTue Nov 04\n\n\n\n\nEDA due\n\n\n\nWed Nov 05\n\n\n\n\n\n\n\n\nThu Nov 06\n\n\n\nhw 4\n\n\n\n12\nTue Nov 11\n\n\n\n\n\n\n\n\nWed Nov 12\nlab: project presentations\n\n\n\npresentation\n\n\n\nThu Nov 13\n\n\n\n\n\n\n\n13\nTue Nov 18\n\n\n\n\ndraft report due\n\n\n\nWed Nov 19\nlab: peer review\n\n\n\npeer review\n\n\n\nThu Nov 20\n\n\n\n\n\n\n\n14\nTue Nov 25\n\n\n\n\n\n\n\n\nWed Nov 26\nNO CLASS\n\n\n\n\n\n\n\nThu Nov 27\nNO CLASS\n\n\n\n\n\n\n15\nTue Dec 02\nexam review\n\n\n\n\n\n\n\nWed Dec 03\n\n\n\n\n\n\n\n\nThu Dec 04\nexam 2\n\n\n\n\n\n\nFinal day\nFri Dec 12\nproject due\n\n\n\nfinal report & repo due"
  },
  {
    "objectID": "hw/hw01.html",
    "href": "hw/hw01.html",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in your Quarto document.\n\n\nWe use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]\n\n\n\n\n\nExercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?\n\n\n\nLet\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the affine transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?\n\n\n\n\nShow that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]\n\n\n\nExercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#exercise-1",
    "href": "hw/hw01.html#exercise-1",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "We use the sum of square errors \\(\\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) to estimate the regression coefficients, \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\\) . To show this is the least squares estimate, we now need to show that we have, in fact, found the estimate of \\(\\boldsymbol{\\beta}\\) that minimizes the sum of squared residuals.\nIf the Hessian matrix \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\) is positive definite, then we know we have found the \\(\\hat{\\boldsymbol{\\beta}}\\) that minimizes the sum of squared residuals, i.e., the least squares estimator.\nShow that \\(\\nabla_{\\boldsymbol{\\beta}}^2 \\boldsymbol{\\varepsilon}^\\mathsf{T}\\boldsymbol{\\varepsilon}\\propto \\mathbf{X}^\\mathsf{T}\\mathbf{X}\\) is positive definite.\n\n\n\n\n\n\nNote\n\n\n\nEquivalent notation. Note that\n\\[\n\\frac{\\partial^2}{\\partial \\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta})\n\\] is another way to write\n\\[\n\\nabla_{\\boldsymbol{\\beta}^2} f(\\boldsymbol{\\beta}).\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-2",
    "href": "hw/hw01.html#exercise-2",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Fox (2015).\nLet \\(Y \\in \\mathbb{R}^n\\) and \\(X \\in \\mathbb{R}^n\\).\nSuppose that the means and standard deviations of \\(Y\\) and \\(X\\) are the same, i.e.¬†\\(\\bar{Y} = \\bar{X}\\) and \\(S_Y = S_X\\).\n\nShow that under these circumstances, the slope and interecept for both the regression of \\(Y\\) on \\(X\\) and \\(X\\) on \\(Y\\) are identical. Mathematically, show that\n\n\\[\n\\hat{\\beta}_{1Y|X} = \\hat{\\beta}_{1X|Y} = cor(X, Y)\n\\]\nwhere \\(\\hat{\\beta}_{1Y|X}\\) is the least-squares slope for the simple linear regression of \\(Y\\) on \\(X\\) and \\(\\hat{\\beta}_{1X|Y}\\) is the least squares slope for the simple regression of \\(X\\) on \\(Y\\). Moreover, show that the intercepts \\(\\hat{\\beta}_{0Y|X} = \\hat{\\beta}_{0X|Y}\\).\n\nSince the slopes are equivalent and the intercepts are equivalent, why is the least-squares line for the regression of \\(Y\\) on \\(X\\) different from the line for the regression of \\(X\\) on \\(Y\\) (assuming \\(r^2 &lt; 1\\))?\nImagine that \\(X\\) is mother‚Äôs height and \\(Y\\) is daughter‚Äôs height for sampled mother-daughter pairs. Again suppose \\(S_y = S_x\\) and \\(\\bar{Y} = \\bar{X}\\). Further suppose \\(0 &lt; r_{XY} &lt; 1\\), i.e.¬†mother-daughter heights are correlated, but not perfectly so. Show that the expected height of a daughter whose mother is shorter than average is also less than average, but to a smaller extent; likewise, show that the expected height of a daughter whose mother is taller than average is also greater than average, but to a smaller extent. Does this result imply a contradiction ‚Äìthat the standard deviation of a daughter‚Äôs height is in fact less than that of a mother‚Äôs height?\nWhat is the expected height for a mother whose daughter is shorter than average? Of a mother whose daughter is taller than average?\nRegression effects in research design: Imagine that medical researchers want to assess the effectiveness of a new rehabilitation program designed to improve lung function in patients recovering from pneumonia. To test the program, they recruit a group of patients whose lung function is substantially below normal; after a year in the program, the researchers observe that these patients, on average, have improved their lung function.\n\nQuestion: Why is this a weak research design? How could it be improved?"
  },
  {
    "objectID": "hw/hw01.html#exercise-3",
    "href": "hw/hw01.html#exercise-3",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Let\n\\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0 + \\boldsymbol{x}\\beta_1 + \\boldsymbol{\\varepsilon}.\n\\]\n\n(i) How do the least squares estimates \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) change when we transform the predictor variable \\(\\boldsymbol{x}\\rightarrow \\boldsymbol{x}^*\\). \\(\\boldsymbol{x}^* = a \\boldsymbol{x}+ b \\boldsymbol{1}\\)? In other words, compare \\(\\hat{\\beta_0}, \\hat{\\beta_1}\\) corresponding to the model above to \\(\\hat{\\beta_0}^*\\) and \\(\\hat{\\beta_1}^*\\), which are estimators of parameters defined by the model below. \\[\n\\boldsymbol{y}= \\boldsymbol{1}\\beta_0^* + \\left(a\\boldsymbol{x}+  b \\boldsymbol{1}\\right) \\beta_1^* + \\boldsymbol{\\varepsilon}\n\\] (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}^*, \\boldsymbol{y})\\)?\n(i) How do the least squares estimates change when we transform \\(\\boldsymbol{y}\\rightarrow \\boldsymbol{y}^*\\) according to the affine transformation: \\(\\boldsymbol{y}^* = c \\boldsymbol{y}+ d \\boldsymbol{1}\\)? (ii) How does \\(cor(\\boldsymbol{x}, \\boldsymbol{y})\\) compare to \\(cor(\\boldsymbol{x}, \\boldsymbol{y}^*)\\)?"
  },
  {
    "objectID": "hw/hw01.html#exercise-4",
    "href": "hw/hw01.html#exercise-4",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Show that the sum of squared residuals (SSR) can be written as the following:\n\\[\n\\boldsymbol{y}^\\mathsf{T}\\boldsymbol{y}- \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\boldsymbol{y}\n\\]"
  },
  {
    "objectID": "hw/hw01.html#exercise-5",
    "href": "hw/hw01.html#exercise-5",
    "title": "Homework 01: simple linear regression",
    "section": "",
    "text": "Exercise is adapted from Montgomery, Peck, and Vining (2021).\nProve that the maximum value of \\(R^2\\) must be less than 1 if the data set contains observations such that there are different observed values of the response for the same value of the predictor (e.g., the data set contains observations \\((x_i, y_i)\\) and \\((x_j, y_j)\\) such that \\(x_i = x_j\\) and \\(y_i \\neq y_j\\) )."
  },
  {
    "objectID": "hw/hw01.html#data",
    "href": "hw/hw01.html#data",
    "title": "Homework 01: simple linear regression",
    "section": "Data",
    "text": "Data\nThe datasets wi-icecover.csv and wi-air-temperature.csv contain information about ice cover and air temperature, respectively, at Lake Monona and Lake Mendota (both in Madison, Wisconsin) for days in 1886 through 2019. The data were obtained from the ntl_icecover and ntl_airtemp data frames in the lterdatasampler R package. They were originally collected by the US Long Term Ecological Research program (LTER) Network.\n\nicecover &lt;- read_csv(\"data/wi-icecover.csv\")\nairtemp &lt;- read_csv(\"data/wi-air-temperature.csv\")\n\nThe analysis will focus on the following variables:\n\nyear: year of observation\nlakeid: lake name\nice_duration: number of days between the freeze and ice breakup dates of each lake\nair_temp_avg: yearly average air temperature in Madison, WI (degrees Celsius)"
  },
  {
    "objectID": "hw/hw01.html#analysis-goal",
    "href": "hw/hw01.html#analysis-goal",
    "title": "Homework 01: simple linear regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goal of this analysis is to use linear regression explain variability in ice duration for lakes in Madison, WI based on air temperature. Because ice cover is impacted by various environmental factors, researchers are interested in examining the association between these two factors to better understand the changing climate."
  },
  {
    "objectID": "hw/hw01.html#exercise-6",
    "href": "hw/hw01.html#exercise-6",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nLet‚Äôs start by looking at the response variable ice_duration.\n\nVisualize the distribution of ice duration versus year with separate lines for each lake.\nThere are separate yearly measurements for each lake in the icecover data frame. In this analysis, we will combine the data from both lakes and use the average ice duration each year.\nComment on the analysis choice to use the average per year rather than the individual lake measurements. Some things to consider in your comments: Does the average accurately reflects the ice duration for these lakes in a given year year? Will there be information lost? How might that impact (or not) the analysis conclusions? Etc.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the ggplot2 reference for example code and plots."
  },
  {
    "objectID": "hw/hw01.html#exercise-7",
    "href": "hw/hw01.html#exercise-7",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNext, let‚Äôs combine the ice duration and air temperature data into a single analysis data frame.\n\nFill in the code below to create a new data frame, icecover_avg, of the average ice duration by year.\nThen join icecover_avg and airtemp to create a new data frame. The new data frame should have 134 observations.\n\nicecover_avg &lt;- icecover |&gt;\n  group_by(_____) |&gt;\n  summarise(_____) |&gt;\n  ungroup()\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the new data frame with average ice duration and average air temperature for the remainder of the assignment.\n\n\n\nVisualize the relationship between the air temperature and average ice duration. Do you think a linear model is a reasonable choice to model the relationship between the two variables? Briefly explain.\n\n\nNow is a good time to render your document again if you haven‚Äôt done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw01.html#exercise-8",
    "href": "hw/hw01.html#exercise-8",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nWe will fit a model using the average air temperature to explain variability in ice duration. The model takes the form\n\\[\n\\boldsymbol{y}= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\n\nState the dimensions of \\(\\boldsymbol{y}\\), \\(\\mathbf{X}\\), \\(\\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\varepsilon}\\) for this analysis. Your answer should have exact values given this data set.\nEstimate the regression coefficients \\(\\hat{\\boldsymbol{\\beta}}\\) in R using the matrix representation. Show the code used to get the answer.\nCheck your results from part (b) by using the lm function to fit the model. Neatly display your results using 3 digits."
  },
  {
    "objectID": "hw/hw01.html#exercise-9",
    "href": "hw/hw01.html#exercise-9",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCalculate \\(R^2\\) for the model in the previous exercise and interpret it in the context of the data.\nCalculate \\(RMSE\\) for the model from the previous exercise and interpret it in the context of the data.\nComment on the model fit based on \\(R^2\\) and \\(RMSE\\)."
  },
  {
    "objectID": "hw/hw01.html#exercise-10",
    "href": "hw/hw01.html#exercise-10",
    "title": "Homework 01: simple linear regression",
    "section": "Exercise 10",
    "text": "Exercise 10\na. Interpret the slope in the context of the data.\nb. The average air temperature in 2019, the most recent year in the data set, was 7.925 degrees Celsius. What was the predicted ice duration for 2019? What is the residual?"
  },
  {
    "objectID": "hw/hw02.html",
    "href": "hw/hw02.html",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in a Quarto document. Note: there is no GitHub repository for this assignment since there are no coding exercises.\nIn all exercises below, you may assume \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{cov}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\).\n\n\nIn lecture, we defined the hat matrix \\(\\boldsymbol{H}\\) as a projection matrix that projects \\(\\boldsymbol{y}\\) onto \\(Col(\\boldsymbol{X})\\) and discussed the properties of a projection matrix. In class we showed that \\(\\boldsymbol{H}\\) is symmetric and idempotent. Now we will focus on two other properties.\n\nShow that for any vector \\(\\boldsymbol{v}\\) in \\(Col(\\boldsymbol{X})\\). \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{v}\\).\nShow that any vector \\(\\boldsymbol{v}\\) orthogonal to \\(Col(\\boldsymbol{X})\\), \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{0}\\)\n\n\n\n\nDerive the expected value and covariance matrix of the residual vector, \\(\\hat{\\boldsymbol{\\varepsilon}}\\). In other words, derive the \\(E[\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X}]\\) and \\(\\text{var}(\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X})\\).\nHint: consider the hat matrix.\n\n\n\nShow that all eigenvalues of the hat matrix \\(\\boldsymbol{H}\\) are 0 or 1.\n\n\n\nLet \\(\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\epsilon\\) where \\(\\beta \\in \\mathbb{R}^p\\) and \\(p = 2\\).\n\nShow that \\((I-\\boldsymbol{H})\\) is orthogonal to \\(Col(\\boldsymbol{X})\\).\nShow \\(E\\left[\\frac{\\text{RSS}}{n-2}| \\boldsymbol{X}\\right]\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nHint: note the fact that \\(a = tr(a)\\) for all scalar numbers \\(a\\). Also note the cyclic property of trace: \\(tr(ABC) = tr(CAB) = tr(BCA)\\).\n\n\n\nShow that if \\(\\bar{x} &gt; 0\\), \\(\\text{cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &lt; 0\\) in simple linear regression.\n\n\n\nBecause the least squares estimator \\(\\hat{\\beta} = \\boldsymbol{A}\\boldsymbol{y}\\), for some matrix \\(\\boldsymbol{A}\\), we call \\(\\hat{\\beta}\\) a linear estimator.\nIf \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{var}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\), then show (a) that \\(E[\\hat{\\beta}] = \\beta\\) and (b) that \\(\\hat{\\beta}\\) has the smallest variance among all linear, unbiased estimators.\nHint for part (b): consider an alternative linear estimator \\(\\tilde{\\beta} = \\boldsymbol{B} \\boldsymbol{y}\\) and show that its variance is equal to the variance of \\(\\hat{\\beta}\\) plus some positive semi-definite matrix.\n\n\n\nDescribe, in your own words, the difference between \\(\\hat{\\beta}\\) and \\(\\beta\\) as well as the difference between \\(\\hat{y}\\) and \\(y\\). In your explanation, specifically identify whether each is random or fixed, known or unknown both before and after collecting the data."
  },
  {
    "objectID": "hw/hw02.html#exercise-1",
    "href": "hw/hw02.html#exercise-1",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "In lecture, we defined the hat matrix \\(\\boldsymbol{H}\\) as a projection matrix that projects \\(\\boldsymbol{y}\\) onto \\(Col(\\boldsymbol{X})\\) and discussed the properties of a projection matrix. In class we showed that \\(\\boldsymbol{H}\\) is symmetric and idempotent. Now we will focus on two other properties.\n\nShow that for any vector \\(\\boldsymbol{v}\\) in \\(Col(\\boldsymbol{X})\\). \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{v}\\).\nShow that any vector \\(\\boldsymbol{v}\\) orthogonal to \\(Col(\\boldsymbol{X})\\), \\(\\boldsymbol{H}\\boldsymbol{v} = \\boldsymbol{0}\\)"
  },
  {
    "objectID": "hw/hw02.html#exercise-2",
    "href": "hw/hw02.html#exercise-2",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Derive the expected value and covariance matrix of the residual vector, \\(\\hat{\\boldsymbol{\\varepsilon}}\\). In other words, derive the \\(E[\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X}]\\) and \\(\\text{var}(\\hat{\\boldsymbol{\\varepsilon}}| \\boldsymbol{X})\\).\nHint: consider the hat matrix."
  },
  {
    "objectID": "hw/hw02.html#exercise-3",
    "href": "hw/hw02.html#exercise-3",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Show that all eigenvalues of the hat matrix \\(\\boldsymbol{H}\\) are 0 or 1."
  },
  {
    "objectID": "hw/hw02.html#exercise-4",
    "href": "hw/hw02.html#exercise-4",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Let \\(\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\epsilon\\) where \\(\\beta \\in \\mathbb{R}^p\\) and \\(p = 2\\).\n\nShow that \\((I-\\boldsymbol{H})\\) is orthogonal to \\(Col(\\boldsymbol{X})\\).\nShow \\(E\\left[\\frac{\\text{RSS}}{n-2}| \\boldsymbol{X}\\right]\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nHint: note the fact that \\(a = tr(a)\\) for all scalar numbers \\(a\\). Also note the cyclic property of trace: \\(tr(ABC) = tr(CAB) = tr(BCA)\\)."
  },
  {
    "objectID": "hw/hw02.html#exercise-5",
    "href": "hw/hw02.html#exercise-5",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Show that if \\(\\bar{x} &gt; 0\\), \\(\\text{cov}(\\hat{\\beta}_0, \\hat{\\beta}_1) &lt; 0\\) in simple linear regression."
  },
  {
    "objectID": "hw/hw02.html#exercise-6",
    "href": "hw/hw02.html#exercise-6",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Because the least squares estimator \\(\\hat{\\beta} = \\boldsymbol{A}\\boldsymbol{y}\\), for some matrix \\(\\boldsymbol{A}\\), we call \\(\\hat{\\beta}\\) a linear estimator.\nIf \\(E[\\boldsymbol{\\varepsilon}| \\boldsymbol{X}] = \\boldsymbol{0}\\) and \\(\\text{var}(\\boldsymbol{\\varepsilon}| \\boldsymbol{X}) = \\sigma^2 \\boldsymbol{I}\\), then show (a) that \\(E[\\hat{\\beta}] = \\beta\\) and (b) that \\(\\hat{\\beta}\\) has the smallest variance among all linear, unbiased estimators.\nHint for part (b): consider an alternative linear estimator \\(\\tilde{\\beta} = \\boldsymbol{B} \\boldsymbol{y}\\) and show that its variance is equal to the variance of \\(\\hat{\\beta}\\) plus some positive semi-definite matrix."
  },
  {
    "objectID": "hw/hw02.html#exercise-7",
    "href": "hw/hw02.html#exercise-7",
    "title": "Homework 02: properties of linear regression",
    "section": "",
    "text": "Describe, in your own words, the difference between \\(\\hat{\\beta}\\) and \\(\\beta\\) as well as the difference between \\(\\hat{y}\\) and \\(y\\). In your explanation, specifically identify whether each is random or fixed, known or unknown both before and after collecting the data."
  },
  {
    "objectID": "labs/lab-project.html",
    "href": "labs/lab-project.html",
    "title": "Lab: project planning",
    "section": "",
    "text": "The goal of today‚Äôs lab is to begin your project proposal.\n\nread the entire project description here or by clicking the tab ‚Äúproject‚Äù at the top of the website.\nnavigate to your GitHub repo project-team_name in the course organization and clone the repo.\nedit research-topics.qmd. Push the qmd and rendered pdf documents to GitHub by the deadline: Tuesday, September 30 at 5:00pm. There is no Gradescope submission. This exercise will be graded according to the following rubric:\n\n\n0pts: missing or severely incomplete research-topics\n1pt: somewhat incomplete research-topics\n2pts: each itemized point is thoroughly addressed for each topic\n\n\nAfter you complete this write-up, begin to plan your project proposal."
  },
  {
    "objectID": "labs/lab-project.html#todays-lab",
    "href": "labs/lab-project.html#todays-lab",
    "title": "Lab: project planning",
    "section": "",
    "text": "The goal of today‚Äôs lab is to begin your project proposal.\n\nread the entire project description here or by clicking the tab ‚Äúproject‚Äù at the top of the website.\nnavigate to your GitHub repo project-team_name in the course organization and clone the repo.\nedit research-topics.qmd. Push the qmd and rendered pdf documents to GitHub by the deadline: Tuesday, September 30 at 5:00pm. There is no Gradescope submission. This exercise will be graded according to the following rubric:\n\n\n0pts: missing or severely incomplete research-topics\n1pt: somewhat incomplete research-topics\n2pts: each itemized point is thoroughly addressed for each topic\n\n\nAfter you complete this write-up, begin to plan your project proposal."
  },
  {
    "objectID": "labs/lab01.html",
    "href": "labs/lab01.html",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 9 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab01.html#learning-goals",
    "href": "labs/lab01.html#learning-goals",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will‚Ä¶\n\nRecall some basic matrix operations and linear algebra rules\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to produce visualizations and summary statistics to describe distributions"
  },
  {
    "objectID": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta221-fa25 organization on GitHub.\nClick on the repo with the prefix lab-01. It contains the starter documents you need to complete the lab.\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you‚Äôll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\n\nSee the lab 0 instructions if you have not set up the SSH key or configured git.\n\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab01.html#r-and-r-studio",
    "href": "labs/lab01.html#r-and-r-studio",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\n\n\n\n\nBelow are the components of an Quarto (.qmd) file."
  },
  {
    "objectID": "labs/lab01.html#yaml",
    "href": "labs/lab01.html#yaml",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "YAML",
    "text": "YAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ‚ÄúYAML Ain‚Äôt Markup Language‚Äù. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document."
  },
  {
    "objectID": "labs/lab01.html#committing-changes",
    "href": "labs/lab01.html#committing-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Committing changes",
    "text": "Committing changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you‚Äôre happy with these changes, we‚Äôll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, ‚Äúupdated author name‚Äù) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don‚Äôt have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let‚Äôs make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you‚Äôre good to go!"
  },
  {
    "objectID": "labs/lab01.html#push-changes",
    "href": "labs/lab01.html#push-changes",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Push changes",
    "text": "Push changes\nNow that you have made an update and committed this change, it‚Äôs time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab01.html#instructions",
    "href": "labs/lab01.html#instructions",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\nWrite all code and narrative in your Quarto file. Write all narrative in complete sentences. Throughout the assignment, you should periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\n\n\n\n\n\n\nTip\n\n\n\nMake sure we can read all of your code in your PDF document. This means you will need to break up long lines of code. One way to help avoid long lines of code is is start a new line after every pipe (|&gt;) and plus sign (+)."
  },
  {
    "objectID": "labs/lab01.html#exercise-1",
    "href": "labs/lab01.html#exercise-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 1",
    "text": "Exercise 1\nViewing a summary of the data is a useful starting point for data analysis, especially if the data set has a large number of observations (rows) or variables (columns). Run the code below to use the glimpse function to see a summary of the ikea data set.\nHow many observations are in the ikea data set? How many variables?\n\nglimpse(ikea)\n\n\n\n\n\n\n\nNote\n\n\n\nIn your `lab-01.qmd` document you‚Äôll see that we already added the code required for the exercise as well as a sentence where you can fill in the blanks to report the answer. Use this format for the remaining exercises.\nAlso note that the code chunk has a label: glimpse-data. Labeling your code chunks is not required, but it is good practice and highly encouraged."
  },
  {
    "objectID": "labs/lab01.html#exercise-2",
    "href": "labs/lab01.html#exercise-2",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 2",
    "text": "Exercise 2\nWe begin each regression analysis with exploratory data analysis (EDA) to help us ‚Äúget to know‚Äù the data and examine the variable distributions and relationships between variables. We do this by visualizing the data and calculating summary statistics to describe the variables in our data set. In this lab, we will focus on data visualizations.\nWhen we make visualizations, we want them to be clear and suitable to present to a professional audience. This means that, at a minimum, each visualization should have an informative title and informative axis labels.\nFill in the code below to visualize the distribution of price_usd, the price in US dollars.\n\nggplot(data = ikea, aes(x = _____)) +\n  geom_histogram() +\n    labs(x = \"_____\",\n       y = \"_____\", \n       title = \"_____\")"
  },
  {
    "objectID": "labs/lab01.html#exercise-3",
    "href": "labs/lab01.html#exercise-3",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the visualization to describe the distribution of price. In your narrative, include descriptions of the shape, approximate center, approximate spread, and any presence of outliers. Briefly explain why the median is more representative of the center of this distribution than the mean.\nNote: You may compute summary statistics to more precisely describe the center and spread.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 1 - 3‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-4",
    "href": "labs/lab01.html#exercise-4",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn this course, we‚Äôll be most interested in the relationship between two or more variables, so let‚Äôs begin by looking at the distribution of price by category. We‚Äôll focus on two categories, Sofas & armchairs and Bookcases & shelving units, since these may be types of furniture most useful to furnish an office.\nFill in the code below to create a new data frame called ikea_sub that only includes the furniture categories of interest. We‚Äôre assigning this subsetted data frame to an object with a new name, so we don‚Äôt overwrite the original data.\n\nikea_sub &lt;- ikea |&gt;\n  filter(_____ %in% c( \"_____\",\n                       \"_____\"))\n\nNow, run the code below to remove observations that have that have a missing value for at least one of width or price_usd.\n\nikea_sub &lt;- ikea_sub |&gt;\n  drop_na(width, price_usd)\n\nHow many observations are in the ikea_sub data frame? How many variables?\n\n\n\n\n\n\nImportant\n\n\n\nUse the ikea_sub data frame for the remainder of lab."
  },
  {
    "objectID": "labs/lab01.html#exercise-5",
    "href": "labs/lab01.html#exercise-5",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate a visualization of the relationship between the width and price of your items at Ikea in the two categories of interest. Include informative axis labels and an informative title. Use the visualization to describe the relationship between the two variables.\nThen, recreate your visualization, but now adding color based on furniture category. Comment on your observations from this visualization.\nNote: Show both visualizations in the response.\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g.¬†‚ÄúCompleted exercises 4 - 5‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#instructions-1",
    "href": "labs/lab01.html#instructions-1",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Instructions",
    "text": "Instructions\n\nThe conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\n\nPut any relevant R code in the Quarto document. You may write the answers and show any associated work for conceptual exercises by hand or type them in your Quarto document using LaTex.\nLet\n\\[\nA = \\begin{bmatrix}\n1 & 2\\\\\n3 & 4\\\\\n5 & 6\\end{bmatrix},\n\\qquad\nB = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n0 & 1 & 2 & 3\n\\end{bmatrix},\n\\qquad\nC = \\begin{bmatrix}\n1 & 4\\\\\n2 & 5\\\\\n3 & 6\\end{bmatrix}\n\\qquad\n\\]\n\\[\n\\mathbf{X} = \\begin{bmatrix}\nx_{11} & x_{12}& \\dots & x_{1p}\\\\\nx_{21} & x_{22}& \\dots & x_{2p}\\\\\n\\vdots & \\vdots& \\ddots & \\vdots\\\\\nx_{n1} & x_{n2}& \\dots & x_{np}\\\\\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-6",
    "href": "labs/lab01.html#exercise-6",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 6",
    "text": "Exercise 6\nWrite the dimensions of the following matrices:\n\n\\(A\\)\n\\(B\\)\n\\(A^\\top\\)\n\\(\\mathbf{X}\\)\n\\(\\mathbf{X}^\\top\\)"
  },
  {
    "objectID": "labs/lab01.html#exercise-7",
    "href": "labs/lab01.html#exercise-7",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 7",
    "text": "Exercise 7\ni. Which of the following is a proper matrix multiplication operation? Explain why.\n\n\\(A\\times C\\)\n\\(A\\times B\\)\n\\(A^\\top \\times B\\)\n\\(B \\times A\\)\n\\(B^\\top \\times C\\)\n\\(B\\times B\\)\n\nii. Perform the multiplication you chose in part (i)."
  },
  {
    "objectID": "labs/lab01.html#matrix-operations-in-r",
    "href": "labs/lab01.html#matrix-operations-in-r",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Matrix operations in R",
    "text": "Matrix operations in R\nR has built in matrix tools such as addition, multiplication, transpose, etc. We will now practice using these tools to review some matrix properties.\nWe first begin by creating matrices using matrix() function. We provide elements of our matrices as the data argument and specify how many rows our matrices have. byrow = TRUE allows us to fill the matrix by row.\n\nA &lt;- matrix(data = c(1, 2,\n                     3, 4,\n                     5, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nB &lt;- matrix(data = c(1, 1, 1, 1, \n                     0, 1, 2, 3), \n            nrow = 2,\n            byrow = TRUE)\n\nC &lt;-  matrix(data = c(1, 4,\n                      2, 5,\n                      3, 6),\n            nrow = 3, \n            byrow = TRUE) \n\nYou can learn more about matrix() function by typing ?matrix in console."
  },
  {
    "objectID": "labs/lab01.html#exercise-8",
    "href": "labs/lab01.html#exercise-8",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 8",
    "text": "Exercise 8\ni. To perform addition or subtraction, we can simply use a + or - operators.\n\n# Add A and C\nA + C\n\n     [,1] [,2]\n[1,]    2    6\n[2,]    5    9\n[3,]    8   12\n\n\nUsing R, find \\(C + A\\). Is addition commutative (i.e.¬†does \\(A + C = C + A\\))? Show the work to support your response.\nii. In R, we have to use a special matrix multiplication operator, %*% .\n\n# multiply A and B\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    3    7   11   15\n[3,]    5   11   17   23\n\n\nDoes the output match your answer to Exercise 7 (ii)? What happens if you try to multiply \\(B\\times A\\) in R?\n\n\n\n\n\n\nWarning\n\n\n\nMatrix multiplication is not commutative! Matrix multiplication satisfies left and right distributivity: \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} = \\mathbf{A}\\mathbf{C} + \\mathbf{B}\\mathbf{C}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}\\), but the order here matters. \\((\\mathbf{A} + \\mathbf{B}) \\mathbf{C} \\neq \\mathbf{C}\\mathbf{A} +\\mathbf{C} \\mathbf{B}\\), and \\(\\mathbf{A}( \\mathbf{B}+ \\mathbf{C}) \\neq \\mathbf{B}\\mathbf{A} + \\mathbf{C}\\mathbf{A}\\). Pay attention to the order and dimensions of matrices.\n\n\niii. In this class, we will work a lot with matrix transposes. You can transpose a matrix in R by applying t() function.\n\n# transpose A\nt(A)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nFind \\(B^\\top \\times A^\\top\\) using R. How is your answer compare to the result of \\(A\\times B\\) you found in previous part?\n\nThis is a good place to render, commit, and push changes to your lab-01 repo on GitHub. Write an informative commit message (e.g., ‚ÄúCompleted exercises 6 - 8‚Äù), and push every file to GitHub by clicking the check box next to each file in the Git pane. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab01.html#exercise-9",
    "href": "labs/lab01.html#exercise-9",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 9",
    "text": "Exercise 9\nLet \\(\\mathbf a = \\begin{bmatrix}a_1 \\\\ \\vdots \\\\ a_n\\end{bmatrix}\\) and \\(\\mathbf b = \\begin{bmatrix}b_1 \\\\ \\vdots \\\\ b_n\\end{bmatrix}\\). Recall, \\[\\mathbf{a}^\\top \\mathbf{a} = \\sum_{i=1}^n a_i^2.\\]\nWrite \\((\\mathbf{a} - \\mathbf{b})^\\top (\\mathbf{a} - \\mathbf{b})\\) using summation notation."
  },
  {
    "objectID": "labs/lab01.html#more-linear-algebra",
    "href": "labs/lab01.html#more-linear-algebra",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "More Linear Algebra",
    "text": "More Linear Algebra\nRecall the definition of linear dependence:\nA sequence of vectors \\(\\mathbf{x_1}, \\mathbf{x_2}, \\dots, \\mathbf{x_p}\\) is said to be linearly dependent if there exists a series of scalars \\(a_1, a_2, \\dots, a_p\\), not all zero, such that\n\\[\na_1 \\mathbf{x_1} + a_2 \\mathbf{x_2} + \\dots + a_p \\mathbf{x_p} = \\mathbf{0}\n\\] Further, matrix \\(\\mathbf{X}\\) has full column rank if all of its columns are linearly independent.\nFor example, the following matrix is not full rank,\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n1 & 2 & 3\\\\\n1 & 1 & 2\n\\end{bmatrix}\n\\] since, letting letting \\(\\mathbf{x_1} = \\begin{bmatrix}1\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_2} = \\begin{bmatrix}2\\\\1 \\end{bmatrix}\\), \\(\\mathbf{x_3} = \\begin{bmatrix}3\\\\2 \\end{bmatrix}\\), and letting \\(a_1 = 1\\), \\(a_2 = 1\\), and \\(a_3 = -1\\), we have:\n\\[\na_1 \\mathbf{x_1} + a_2\\mathbf{x_2} + a_3\\mathbf{x_3} =  \\begin{bmatrix}1\\\\1 \\end{bmatrix} + \\begin{bmatrix}2\\\\1 \\end{bmatrix} - \\begin{bmatrix}3\\\\2 \\end{bmatrix} = \\mathbf{0}.\n\\]"
  },
  {
    "objectID": "labs/lab01.html#exercise-10",
    "href": "labs/lab01.html#exercise-10",
    "title": "Lab 1: Computing and linear algebra review",
    "section": "Exercise 10",
    "text": "Exercise 10\nFor each of the following matrices, state whether it is full rank. If not full rank, show why (find corresponding coefficients \\(a\\)‚Äôs).\n\n\\[\\begin{bmatrix}\n1 & 0 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n0 & 0 & 1\\\\\n1 & 0 & 0\\\\\n1 & 0 & 0\\\\\n0 & 1 & 0\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n1 & 0 & 0 & 1\\\\\n1 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0\\\\\n1 & 0 & 1 & 0\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "labs/lab03.html",
    "href": "labs/lab03.html",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, September 23 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab03.html#exercise-1",
    "href": "labs/lab03.html#exercise-1",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet‚Äôs start with some exploratory data analysis. Visualize the distribution of the response variable mc_preschool and calculate summary statistics. Describe the distribution of this variable, including the shape, center, spread, and presence of potential outliers.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the childcare_train for all analysis in Exercises 1 - 7."
  },
  {
    "objectID": "labs/lab03.html#exercise-2",
    "href": "labs/lab03.html#exercise-2",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nAs you can see from the data dictionary in the README of the data folder, there are many interesting potential variables that could be included in the model to predict median childcare cost for preschool-age children. Therefore, we will do some feature selection and feature design to choose potential predictors and construct new ones.\nAs a team, select four variables you want to use as predictors for the model. For each variable, state the variable name, definition, and a brief explanation about why your team hypothesizes this will be a relevant predictor of median childcare costs. The explanation may (but is not required to) include some short exploratory analysis.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It‚Äôs your turn! Type the team‚Äôs response to exercises 3 - 4."
  },
  {
    "objectID": "labs/lab03.html#exercise-3",
    "href": "labs/lab03.html#exercise-3",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nOnce we‚Äôve identified potential predictor variables, we often need to transform some variables (e.g., change raw counts into proportions) or create new ones (e.g., create a categorical variable out of quantitative data) before fitting the regression model. This process is particularly useful when putting a variable in the model ‚Äúas-is‚Äù may result in interpretation issues.\nChoose one of the variables selected in the previous exercise. For this variable,\n\nTransform the variable or use it to create a new variable. Be sure to save the variable to the childcare_train data frame.\nBriefly explain your reasoning for the transformation or new variable.\nUse visualizations and/or summary statistics to display the distribution of the original variable and the transformed / newly created variable. Note: This is to help ensure the transformation / new variable is what you expect.\n\n\n\n\n\n\n\n\n\n\n\nYou may decide to transform and/or create multiple new variables; however, you will only be graded on the one of them.\n\n\n\n\n\n\nImportant\n\n\n\nYou will use the transformed / new variable (not the original variable) in the model!"
  },
  {
    "objectID": "labs/lab03.html#exercise-4",
    "href": "labs/lab03.html#exercise-4",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nNow let‚Äôs conduct bivariate exploratory data analysis. Visualize the relationship between the response variable and one of your predictor variables.\nWrite two distinct observations from the visualization.\n\nTeam Member 2: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 3 - 4.\nTeam Member 3: It‚Äôs your turn! Type the team‚Äôs response to exercises 5 - 6."
  },
  {
    "objectID": "labs/lab03.html#exercise-5",
    "href": "labs/lab03.html#exercise-5",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nUse the matrix form of the model to represent the regression model with the variables you selected and transformed/created in exercises 2 and 3 as the predictors. For each symbol in the model\n\ndescribe what it represents, and\nstate the dimensions.\n\nThe description and dimensions should be in the context of these data, not in general."
  },
  {
    "objectID": "labs/lab03.html#exercise-6",
    "href": "labs/lab03.html#exercise-6",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse lm() to fit the regression model you described in the previous exercise.\n\nNeatly display the model using a reasonable number of digits.\nInterpret the coefficient for one predictor in the model.\n\n\nTeam Member 3: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 5 - 6.\nTeam Member 4: It‚Äôs your turn! Type the team‚Äôs response to exercises 7 - 9."
  },
  {
    "objectID": "labs/lab03.html#exercise-7",
    "href": "labs/lab03.html#exercise-7",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow let‚Äôs assess the fit of the model.\n\nHow much of the variability in the childcare costs is explained by your chosen predictor variables?\nBased on this, do you think the model explains a significant portion of the variability in childcare costs for preschool-age children in North Carolina? Briefly explain."
  },
  {
    "objectID": "labs/lab03.html#exercise-8",
    "href": "labs/lab03.html#exercise-8",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs use the testing data to explore the predictive power of the model.\n\nAdd the variable you created in Exercise 3 to the testing data.\nThen, use the code below to compute the predicted childcare costs for the observations in the testing data using the predict function.\n\n\n# compute predictions\npred &lt;- predict(childcare_fit, childcare_test)\n\n# add predictions to testing data set\nchildcare_test &lt;- childcare_test |&gt;\n  mutate(pred = pred)"
  },
  {
    "objectID": "labs/lab03.html#exercise-9",
    "href": "labs/lab03.html#exercise-9",
    "title": "Lab 03: Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nCompute the RMSE for the test set, and compare it to the standard deviation of the response variable mc_preschool.\nHow do these values compare?\nBased on this, how would assess the predictive power of the model?\n\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team‚Äôs completed lab!"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Links",
    "section": "",
    "text": "Code\n\nRStudio containers\n\nCommunication\n\nEd Discussion\n\nCollaboration\n\ncourse GitHub organization\n\nAssignment turn-in\n\nGradescope"
  },
  {
    "objectID": "notes/lec02-correlation.html",
    "href": "notes/lec02-correlation.html",
    "title": "Correlation and intro to simple linear regresion",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(DT)\n \nweather &lt;-\n  read_csv(\"https://sta221-fa25.github.io/data/rdu-weather-history.csv\") %&gt;%\n  arrange(date)"
  },
  {
    "objectID": "notes/lec02-correlation.html#learning-objectives",
    "href": "notes/lec02-correlation.html#learning-objectives",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of the day you should be able to explain the following concepts:\n\ncovariance\ncorrelation\nlocation and scale invariance\nordinary least squares"
  },
  {
    "objectID": "notes/lec02-correlation.html#example",
    "href": "notes/lec02-correlation.html#example",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Example",
    "text": "Example\n\nThis data set contains Raleigh Durham International Airport weather data pulled from the NOAA web service between September 01 and September 30, 2021. The data were sourced from https://catalog.data.gov/ August 28, 2025.\n\n\n\n\n\n\n\n\n\n\n\nWe‚Äôve recorded 30 observations of two measurements. We are interested in the association between these two measurements.\nWe‚Äôll call the minimum daily temperature measurement ‚Äúx‚Äù and the maximum daily temperature ‚Äúy‚Äù."
  },
  {
    "objectID": "notes/lec02-correlation.html#vocabulary",
    "href": "notes/lec02-correlation.html#vocabulary",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Vocabulary",
    "text": "Vocabulary\n\n\n\n\n\n\n\nVariable\nExplanation\n\n\n\n\n\\(y\\)\nThe outcome variable. Also called ‚Äúresponse‚Äù or ‚Äúdependent variable‚Äù. In prediction tasks, this is the variable we are interested in predicting.\n\n\n\\(x\\)\nThe predictor. Also called ‚Äúcovariate‚Äù, ‚Äúfeature‚Äù, or ‚Äúindependent variable‚Äù.\n\n\n\nHow are \\(x\\) and \\(y\\) associated?\n\nScatter plotCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweather %&gt;%\n  ggplot(aes(x = tmin, y = tmax)) +\n  geom_point() +\n  labs(title = \"Minimum and maxmimum temperature (F) at RDU in September, 2021\") +\n  theme_bw()\n\n\n\n\nNotice: visualized this way, each of the thirty data points, \\((x_i, y_i)\\), is an element of two-dimensional space.\n\nHow would you describe the association?\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\(cov(x, y) = \\frac{1}{n-1}\\sum_{i =1}^n (x_i - \\bar{x}) (y_i - \\bar{y})\\) \\(^*\\)\n\\(^*\\) Note that we divide by \\(n-1\\) when computing the sample covariance\n\n\\(\\bar{x} = \\frac{1}{n} \\sum x_i\\) (mean of x)\n\\(\\bar{y} = \\frac{1}{n} \\sum y_i\\) (mean of y)\n\n\n\n\nx = weather %&gt;%\n  select(tmin) %&gt;%\n  pull()\ny = weather %&gt;%\n  select(tmax) %&gt;%\n  pull()\n\ncov(x, y)\n\n[1] 18.68736\n\nsum((x - mean(x)) * (y - mean(y))) / (30 - 1)\n\n[1] 18.68736\n\n\n\nShould the association be the same if the thermometer recording measurements was consistently off by 2 degrees Farenheit?\n\n\n# compute covariance of true temperature measurement\nweather %&gt;%\n  mutate(temp_min_true = tmin + 2,\n         temp_max_true = tmax + 2) %&gt;%\n  summarize(cov(temp_min_true, temp_max_true)) %&gt;%\n  pull()\n\n[1] 18.68736\n\n\nCovariance is location invariant.\n\nWill the association stay the same if we recorded temperature in celsius?\n\n\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cov(temp_min_c, temp_max_c)) %&gt;%\n  pull()\n\n[1] 5.767703\n\n\nCovariance is not scale invariant. That is, covariance does depend on the scale of the measurements."
  },
  {
    "objectID": "notes/lec02-correlation.html#correlation",
    "href": "notes/lec02-correlation.html#correlation",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\n\n\nDefinition: correlation\n\n\n\n\\(cor(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) (y_i - \\bar{y})}{\\left(\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2\\right)^{1/2}}\\)\nMore concisely, we may write\n\\(cor(x, y) = \\frac{S_{xy}}{S_{xx}^{1/2} S_{yy}^{1/2}}\\)\n\n\n\n# correlation in Farenheit vs Celsi\n# compute the covariance between temperature in celsius\nweather %&gt;%\n  mutate(temp_min_c = (tmin - 32) * 5 / 9,\n         temp_max_c = (tmax - 32) * 5 / 9) %&gt;%\n  summarize(cor_F = cor(tmin, tmax), \n            cor_C = cor(temp_min_c, temp_max_c))\n\n# A tibble: 1 √ó 2\n  cor_F cor_C\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.554 0.554\n\n\n\nExerciseSolution\n\n\nShow \\(cor(x, y) = cor(ax + b, cy + d)\\).\n\n\nLet \\[\n\\begin{aligned}\nx^* &= ax_i + b,\\\\\ny^* &= cx_i + d,\n\\end{aligned}\n\\]\nthen we want to prove that \\(cor(x, y) = cor(x^*, y^*)\\).\n\\[\n\\begin{aligned}\ncor(x^*, y^*) &= \\frac{\\sum(ax_i + b - a\\bar{x} - b)(cy_i + d - c\\bar{y} - d)}{\\sqrt{\n\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{ac \\sum(x_i -\\bar{x})(y_i - \\bar{y})}{\nac \\sqrt{\\sum (ax_i + b - a\\bar{x} - b)^2 \\sum (cy_i + d - c\\bar{y} - d)^2\n}\n}\\\\\n&= \\frac{\nS_{xy}}{\\sqrt{S_{xx} S_{yy}}\n}\n\\end{aligned}\n\\]\n\n\n\nCorrelation is location and scale invariant!\nAdditional facts about correlation:\n\n\n\n\n\n\nFact 1\n\n\n\n\nCorrelation is not invariant to monotone transformations of the data\n\n\n\n\n\n\n\n\n\nDefinition: monotone\n\n\n\n\\(g\\) is a monotonic function iff \\(x \\leq y\\) implies \\(g(x) \\leq g(y)\\)\n\n\n\nExerciseSolution\n\n\nShow by example that correlation is not invariant to monotonic transformations.\n\n\n\nset.seed(221)\nx = c(1:10) \nlogx = log(x)\ny = rnorm(10)\ncor(x, y)\n\n[1] -0.1757986\n\ncor(logx, y)\n\n[1] -0.2686214\n\n\n\n\n\n\n\n\n\n\n\nFact 2\n\n\n\n\nCorrelation is not robust to outliers\n\n\n\n\nExample 1Example 2\n\n\n\nset.seed(221)\nx = c(1:5)\ny= x + rnorm(5)\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 0.9042223\n\n\n\n\n\nx2 = c(x, 0)\ny2 = c(y, 20)\nplot(x2, y2)\n\n\n\n\n\n\n\ncor(x2, y2)\n\n[1] -0.5195233\n\n\n\n\n\n\n\n\n\n\n\nFact 3\n\n\n\nCorrelation is bounded between -1 and 1.\n\n\n\nExerciseSolution\n\n\nShow that \\(|cor(x, y)| \\leq 1\\)\n\n\nCauchy-Schwarz inequality:\nLet \\(u\\) and \\(v\\) be vectors of dimension \\(n\\), then\n\\[\n|u^T v|^2 \\leq (u^Tu) (v^Tv).\n\\] To prove that \\(\\|cor(x,y)\\| \\leq 1\\), let \\(u = x - \\bar{x}\\) and let \\(v = y - \\bar{y}\\).\n\n\n\n\n\n\nImportant\n\n\n\nNotice that the dimension of a vector inner product, e.g.¬†\\(u^Tv\\) is \\(1 \\times 1\\), in other words, it is a ‚Äúscalar‚Äù, a number.\n\n\n\n\n\n\n\n\n\n\n\nFact 4\n\n\n\n\ncorrelation is a measure of linear association\n\n\n\n\nExample 1Example 2\n\n\n\nx = -10:10\ny = x^2\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y) \n\n[1] -4.786989e-17\n\n\n\n\n\nx = -10:10\ny = x\nplot(x, y)\n\n\n\n\n\n\n\ncor(x, y)\n\n[1] 1\n\n\n\n\n\nSince correlation is related to a linear relationship between the data, strong correlation implies that\n\\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\n\\]\n\nExercise\n\n\nCheck out the interactive regression web app here:\nhttps://seeing-theory.brown.edu/regression-analysis/index.html#section1"
  },
  {
    "objectID": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "href": "notes/lec02-correlation.html#nomenclature-of-simple-linear-regression",
    "title": "Correlation and intro to simple linear regresion",
    "section": "Nomenclature of simple linear regression",
    "text": "Nomenclature of simple linear regression\n\n\n\n\n\n\n\nSymbol\nDescription\n\n\n\n\n\\(y\\)\nThe response variable. Also called the ‚Äúoutcome‚Äù or ‚Äúdependent variable‚Äù.\n\n\n\\(x\\)\nA covariate. Also called the ‚Äúpredictor‚Äù, ‚Äúfeature‚Äù or ‚Äúindependent variable‚Äù.\n\n\n\\(\\beta_0, \\beta_1\\)\nThese are population parameters, i.e.¬†fixed and unknown constants.\n\n\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1\\)\nEstimates of \\(\\beta_0, \\beta_1\\) based on a sample.\n\n\n\\(\\hat{y}\\)\nThe prediction outcome. \\(\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\). May also be referred to as the ‚Äúfitted regression model‚Äù.\n\n\n\\(\\epsilon\\)\nThe error. Defined by the regression equation: \\(\\epsilon = y - \\beta_0 - \\beta_1 x\\).\n\n\n\\(\\hat{\\epsilon}\\) or \\(e\\)\nThe residual, i.e.¬†the difference between the outcome and the fitted model. Defined as \\(y - \\hat{y}\\), or equivalently, \\(y - \\hat{\\beta_0} - \\hat{\\beta_1}x\\)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html",
    "href": "notes/lec04-model-assessment.html",
    "title": "Model assessment",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse) # data wrangling and visualization\nlibrary(DT) # shows the data table\nlibrary(patchwork) # arranging plots\nlibrary(tidymodels)  # modeling (includes broom, yardstick, and other packages)\nlibrary(knitr)       # aesthetic tables\n \nlife_exp &lt;- read_csv(\n    \"https://sta221-fa25.github.io/data/life-expectancy-data.csv\") |&gt; \n  rename(life_exp = `Life_expectancy_at_birth`, \n         income_inequality = `Income_inequality_Gini_coefficient`) |&gt;\n  mutate(education = if_else(Education_Index &gt; median(Education_Index), \"High\", \"Low\"), \n         education = factor(education, levels = c(\"Low\", \"High\"))) |&gt;\n  select(Country, life_exp, Health_expenditure, income_inequality)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#learning-objectives",
    "href": "notes/lec04-model-assessment.html#learning-objectives",
    "title": "Model assessment",
    "section": "Learning objectives",
    "text": "Learning objectives\nBy the end of today you will be able to\n\nfit simple linear regression models in R\ninterpret coefficients in context\nanalyze the variance\ncheck model fit and interpret in context"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#example",
    "href": "notes/lec04-model-assessment.html#example",
    "title": "Model assessment",
    "section": "Example",
    "text": "Example\nThe data set comes from Zarulli et al. (2021) who analyze the effects of a country‚Äôs healthcare expenditures and other factors on the country‚Äôs life expectancy. The data are originally from the Human Development Database and World Health Organization.\nThere are 140 countries (observations) in the data set.\n\n\n\n\n\n\nGoal: Use the income inequality in a country to understand variability in the life expectancy.\n\n\nClick here for the original research paper.\n\nlife_exp: The average number of years that a newborn could expect to live, if he or she were to pass through life exposed to the sex- and age-specific death rates prevailing at the time of his or her birth, for a specific year, in a given country, territory, or geographic area (from the World Health Organization).\nincome_inequality: Measure of the deviation of the distribution of income among individuals or households within a country from a perfectly equal distribution. A value of 0 represents absolute equality, a value of 100 absolute inequality, based on ‚ÄúGini coefficient‚Äù (from Zarulli et al. (2021)).\nhealth_expenditure: Per capita current spending on healthcare goods and services, expressed in respective currency - international Purchasing Power Parity (PPP) dollar (from the World Health Organization)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "href": "notes/lec04-model-assessment.html#exploratory-data-analysis-eda",
    "title": "Model assessment",
    "section": "Exploratory data analysis (EDA)",
    "text": "Exploratory data analysis (EDA)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#univariate-eda",
    "href": "notes/lec04-model-assessment.html#univariate-eda",
    "title": "Model assessment",
    "section": "Univariate EDA",
    "text": "Univariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np1 &lt;- ggplot(data = life_exp, aes(x = life_exp))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Life expectancy (years)\", \n       y = \"Count\") +\n  theme_bw()\n\np2 &lt;- ggplot(data = life_exp, aes(x = income_inequality))  + \n  geom_histogram(fill = \"steelblue\", color = \"black\", binwidth = 2) + \n  labs(x = \"Income inequality\", \n       y = \"Count\") +\n  theme_bw()\n\np1 + p2 # uses patchwork package"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#bivariate-eda",
    "href": "notes/lec04-model-assessment.html#bivariate-eda",
    "title": "Model assessment",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA\n\nplotcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Scatterplot of life expectancy vs income inequality\np1 &lt;- ggplot(life_exp, aes(x = income_inequality, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Income inequality\",\n    y = \"Life expectancy (years)\"\n  ) +\n  theme_bw()\n\n# Scatterplot of life expectancy vs health expenditure\np2 &lt;- ggplot(life_exp, aes(x = Health_expenditure, y = life_exp)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\") +\n  labs(\n    x = \"Health expenditure\",\n    y = \"\"\n  ) +\n  theme_bw()\n\n# Display plots side by side; uses patchwork pacakge\np1 + p2"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "href": "notes/lec04-model-assessment.html#ordinary-least-squares-regression-in-r",
    "title": "Model assessment",
    "section": "Ordinary least squares regression in R",
    "text": "Ordinary least squares regression in R\n\nTemplate\nTo fit a model by OLS linear regression, we use the lm function. The arguments look as follows:\n\nlm(y ~ x, data = data_frame)\n\nIn words, we ‚Äúregress y on x‚Äù where ‚Äúy‚Äù and ‚Äúx‚Äù are column names in the data frame ‚Äúdata_frame‚Äù.\n\n\nFor our data\n\n# income inequality model\nmodel_ii = lm(life_exp ~ income_inequality, \n              data = life_exp)\n\n# health expenditure model\nmodel_he = lm(life_exp ~ Health_expenditure,\n              data = life_exp)\n\n\nQ: why is Health_expenditure capitalized, but income_inequality is not?\nA: That‚Äôs how they are named in the data! See the column names: names(life_exp).\n\n\n\nLook at results (income inequality model)\nRegular output:\n\nmodel_ii\n\n\nCall:\nlm(formula = life_exp ~ income_inequality, data = life_exp)\n\nCoefficients:\n      (Intercept)  income_inequality  \n          85.4202            -0.6973  \n\n\nTidy the output:\n\nmodel_ii |&gt;\n  tidy()\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         85.4      0.855       99.9 1.41e-130\n2 income_inequality   -0.697    0.0388     -18.0 6.17e- 38\n\n\nPut results in a more aesthetic table using kable from the knitr package:\n\nmodel_ii |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n85.420\n0.855\n99.877\n0\n\n\nincome_inequality\n-0.697\n0.039\n-17.961\n0\n\n\n\n\n\nThe fitted regression model:\n\\[\n\\hat{y_i} = 85.420 - 0.697 x_i\n\\]\n\n\nInterpretation\n\nThe intercept estimate \\(\\hat{\\beta}_0\\) is in units of the response variable. It is the expected value of the response variable when the predictor is set to 0.\nThe estimate of the slope coefficient, \\(\\hat{\\beta}_1\\) is measured in the units of the response variable per unit of the explanatory variable.\n\n\nExercise\n\n\nInterpret the coefficients above (for the income inequality model) in context.\n\n\n\n\n\nPrediction\nUse the predict function to calculate predictions for new observations\nSingle observation\n\nnew_ii &lt;- tibble(income_inequality = 50)\npredict(model_ii, new_ii)\n\n       1 \n50.55618 \n\n\nMultiple observations\n\nmore_new_ii &lt;- tibble(income_inequality = c(25,50, 100))\npredict(model_ii, more_new_ii)\n\n       1        2        3 \n67.98821 50.55618 15.69213 \n\n\nNote the range:\n\nrange(life_exp$income_inequality)\n\n[1]  5.4 44.2\n\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing the model to predict for values outside the range of the original data is extrapolation. Why do we want to avoid extrapolation?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#model-assessment",
    "href": "notes/lec04-model-assessment.html#model-assessment",
    "title": "Model assessment",
    "section": "Model assessment",
    "text": "Model assessment\nWe fit a model but is it any good?\n\nTwo statistics\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\n\nWhat indicates a good model fit? Higher or lower RMSE? Higher or lower \\(R^2\\)?\n\n\n\nRMSE\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n}} = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n}}\n\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "href": "notes/lec04-model-assessment.html#analysis-of-variance-anova",
    "title": "Model assessment",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of Variance (ANOVA): Technique to partition variability in \\(Y\\) by the sources of variability"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response",
    "href": "notes/lec04-model-assessment.html#total-variability-response",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMin\nMedian\nMax\nMean\nStd.Dev\n\n\n\n\n51.6\n72.85\n84.1\n71.614\n8.075"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "href": "notes/lec04-model-assessment.html#partition-sources-of-variability-in-life_exp",
    "title": "Model assessment",
    "section": "Partition sources of variability in life_exp",
    "text": "Partition sources of variability in life_exp"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#total-variability-response-1",
    "href": "notes/lec04-model-assessment.html#total-variability-response-1",
    "title": "Model assessment",
    "section": "Total variability (Response)",
    "text": "Total variability (Response)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Total (SST)} = S_{yy} = \\sum_{i=1}^n(y_i - \\bar{y})^2 = (n-1) \\cdot var(y)\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#explained-variability-model",
    "href": "notes/lec04-model-assessment.html#explained-variability-model",
    "title": "Model assessment",
    "section": "Explained variability (Model)",
    "text": "Explained variability (Model)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Model (SSM)} = \\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "href": "notes/lec04-model-assessment.html#unexplained-variability-residuals",
    "title": "Model assessment",
    "section": "Unexplained variability (Residuals)",
    "text": "Unexplained variability (Residuals)\n\n\n\n\n\n\n\n\n\n\\[\\text{Sum of Squares Residuals (SSR)} = \\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2\\]"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#sum-of-squares",
    "href": "notes/lec04-model-assessment.html#sum-of-squares",
    "title": "Model assessment",
    "section": "Sum of Squares",
    "text": "Sum of Squares\n\\[\n\\begin{aligned}\n\\color{#407E99}{SST} \\hspace{5mm}&= &\\color{darkred}{SSM} &\\hspace{5mm} +  &\\color{#8BB174}{SSR} \\\\[10pt]\n\\color{#407E99}{\\sum_{i=1}^n(y_i - \\bar{y})^2} \\hspace{5mm}&= &\\color{darkred}{\\sum_{i = 1}^{n}(\\hat{y}_i - \\bar{y})^2} &\\hspace{5mm}+ &\\color{#8BB174}{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}\n\\end{aligned}\n\\]\n\n\nClick here to see why this equality holds."
  },
  {
    "objectID": "notes/lec04-model-assessment.html#r2",
    "href": "notes/lec04-model-assessment.html#r2",
    "title": "Model assessment",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\nThe coefficient of determination \\(R^2\\) is the proportion of variation in the response, \\(Y\\), that is explained by the regression model\n\\[\\large{R^2 = \\frac{SSM}{SST} = 1 - \\frac{SSR}{SST}}\\]\n\nWhat is the range of \\(R^2\\)? Does \\(R^2\\) have units?"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#interpreting-r2",
    "href": "notes/lec04-model-assessment.html#interpreting-r2",
    "title": "Model assessment",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)"
  },
  {
    "objectID": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "href": "notes/lec04-model-assessment.html#using-r-to-look-at-these-quantities",
    "title": "Model assessment",
    "section": "Using R to look at these quantities",
    "text": "Using R to look at these quantities\n\nAugmented data frame\nUse the augment() function from the broom package to add columns for predicted values, residuals, and other observation-level model statistics\n\nlife_exp_aug &lt;- augment(model_ii)\nlife_exp_aug\n\n# A tibble: 140 √ó 8\n   life_exp income_inequality .fitted .resid    .hat .sigma   .cooksd .std.resid\n      &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1     63.8              28.2    65.8 -1.96  0.0125    4.45 0.00125       -0.444\n 2     78.2              12.2    76.9  1.29  0.0116    4.45 0.000498       0.292\n 3     59.9              32.4    62.8 -2.93  0.0193    4.44 0.00437       -0.667\n 4     76.2              14      75.7  0.542 0.00972   4.45 0.0000739      0.123\n 5     74.6               8.6    79.4 -4.82  0.0168    4.43 0.0102        -1.10 \n 6     83                 8.3    79.6  3.37  0.0173    4.44 0.00515        0.766\n 7     81.3               7.4    80.3  1.04  0.0189    4.45 0.000540       0.237\n 8     72.5              10.1    78.4 -5.88  0.0144    4.42 0.0130        -1.33 \n 9     71.8              27.6    66.2  5.62  0.0118    4.43 0.00972        1.28 \n10     74                 6.5    80.9 -6.89  0.0207    4.41 0.0260        -1.57 \n# ‚Ñπ 130 more rows\n\n\n\n\nFinding RMSE in R\nUse the rmse() function from the yardstick package (part of tidymodels)\n\nrmse(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        4.40\n\n\n\n\nFinding \\(R^2\\) in R\nUse the rsq() function from the yardstick package (part of tidymodels)\n\nrsq(life_exp_aug, truth = life_exp, estimate = .fitted)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.700\n\n\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(model_ii)$r.squared\n\n[1] 0.7003831"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html",
    "href": "notes/lec06-multiple-regression.html",
    "title": "Multiple linear regression",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(tidymodels)\nlibrary(scatterplot3d)\nlibrary(DT)"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "href": "notes/lec06-multiple-regression.html#fitting-a-regression-model",
    "title": "Multiple linear regression",
    "section": "Fitting a regression model",
    "text": "Fitting a regression model\nTo ‚Äúfit‚Äù a (multiple) linear regression model means finding \\(\\hat{\\beta}\\) that defines the ‚Äúbest‚Äù hyperplane. Here, ‚Äúbest‚Äù means that \\(\\hat{\\beta}\\) is the optimal solution of some objective function."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#full-data-set",
    "href": "notes/lec06-multiple-regression.html#full-data-set",
    "title": "Multiple linear regression",
    "section": "Full data set",
    "text": "Full data set\n\ndatatable(penguins, rownames = FALSE, options = list(pageLength = 5),\n           caption = \"penguins\")\n\n\n\n\n\n\nExerciseSolution\n\n\nWrite down the mathematical formula for linear regression where penguin bodymass is the outcome variable and flipper length, bill length, and sex of the penguin are the covariates. Note the dimension of each symbol in your model.\n\n\n\\[\n\\boldsymbol{y}= \\boldsymbol{X}\\beta + \\boldsymbol{\\varepsilon}\n\\]\nAs usual,\n\n\\(\\boldsymbol{y}, \\boldsymbol{\\varepsilon}\\in \\mathbb{R}^n\\)\n\\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\)\n\\(\\beta \\in \\mathbb{R}^p\\)\n\nbut here \\(p = 4\\) (3 covariates + intercept).\n\\[\n\\boldsymbol{X}=\n\\begin{bmatrix}\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\mathbf{1} & \\mathbf{x}_1 & \\mathbf{x}_2 & \\mathbf{x}_3 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots\n\\end{bmatrix}\n\\]\n\n\n\nQuestion: Why is sex (a categorical predictor) represented as 1 column and not 2?\nAnswer: It would create a linearly dependent column in the matrix \\(\\boldsymbol{X}\\), then \\(\\boldsymbol{X}^T \\boldsymbol{X}\\) would be rank deficient and could not be inverted.\nQuestion: Suppose we add the covariate ‚Äúisland‚Äù, what is the new dimension of \\(\\boldsymbol{X}\\) and \\(\\beta\\) then?\nAnswer: Island is categorical with three states, therefore we would need 2 predictors to represent it. Now \\(p = 6\\) and \\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times 6}\\) and \\(\\beta \\in \\mathbb{R}^6\\)."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "href": "notes/lec06-multiple-regression.html#fitting-multiple-regression-in-r-ols",
    "title": "Multiple linear regression",
    "section": "Fitting multiple regression in R (OLS)",
    "text": "Fitting multiple regression in R (OLS)\nThe formula for the least squares estimator \\(\\hat{\\beta}_{OLS}\\) is the same,\n\\[\n\\hat{\\beta}_{OLS} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\]\n\nlmmanualin-between\n\n\n\npenguin_fit &lt;- lm(body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island, data = penguins)\n\npenguin_fit\n\n\nCall:\nlm(formula = body_mass_g ~ flipper_length_mm + bill_length_mm + \n    sex + island, data = penguins)\n\nCoefficients:\n      (Intercept)  flipper_length_mm     bill_length_mm            sexmale  \n        -3629.563             37.638              5.808            390.055  \n      islandDream    islandTorgersen  \n         -376.124           -288.380  \n\n\nQuestion: Why didn‚Äôt I have to specify an intercept?\nAnswer: lm includes intercept by default.\nQuestion: What if I don‚Äôt want an intercept?\nAnswer: Use + 0, like this: lm(y ~ x1 + x2 + ... + x_5 + 0, data = penguin). This will tell lm not to exclude an intercept term from the model.\n\n\n\nyX &lt;- penguins %&gt;%\n  select(c(\"body_mass_g\", \"bill_length_mm\", \"flipper_length_mm\",\n           \"sex\", \"island\")) %&gt;%\n  mutate(isMale = ifelse(sex == \"male\", 1, 0), ## create 1 dummy variable \n           isDream = ifelse(island == \"Dream\", 1, 0), ## create other 2 dummy variables\n         isTorgersen = ifelse(island == \"Torgersen\", 1, 0)) %&gt;%\n  select(-c(island, sex)) %&gt;% ## remove redundant columns \n  mutate(one = 1) %&gt;% # create vector of ones\n  drop_na() %&gt;% # drop NAs, just like the lm() function does by default\n  as.matrix() # turn into a matrix for matrix algebra in R\n\ny &lt;- yX[,1] # grab first column (which was the outcome variable)\nX &lt;- yX[,-1] # grab all but first column (X matrix)\n\nbetaHat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\nbetaHat\n\n                          [,1]\nbill_length_mm        5.808086\nflipper_length_mm    37.637788\nisMale              390.055414\nisDream            -376.124225\nisTorgersen        -288.380366\none               -3629.562552\n\n\n\n\nAn alternative, easy way to grab \\(X\\) quickly:\n\nX = model.matrix( body_mass_g ~ flipper_length_mm + bill_length_mm +  sex + island,\n              data = penguins)\nX %&gt;%\n  head(n = 10) # notice NAs are dropped by default, (see the row number skips)\n\n   (Intercept) flipper_length_mm bill_length_mm sexmale islandDream\n1            1               181           39.1       1           0\n2            1               186           39.5       0           0\n3            1               195           40.3       0           0\n5            1               193           36.7       0           0\n6            1               190           39.3       1           0\n7            1               181           38.9       0           0\n8            1               195           39.2       1           0\n13           1               182           41.1       0           0\n14           1               191           38.6       1           0\n15           1               198           34.6       1           0\n   islandTorgersen\n1                1\n2                1\n3                1\n5                1\n6                1\n7                1\n8                1\n13               1\n14               1\n15               1"
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#interpretation",
    "href": "notes/lec06-multiple-regression.html#interpretation",
    "title": "Multiple linear regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nSlopes\nWe interpret slopes \\(\\hat{\\beta}_j\\) as the expected change in the mean of \\(Y\\) when \\(X_j\\) increases by one unit, holding the value of all other predictor variables constant.\nFor example, for each additional mm of flipper length a given penguin has, we expect their body mass to increase by 37.6 grams holding all other covariates constant.\n\n\nIntercept\nThe intercept is the expected value of \\(Y\\) when all predictors are zero. Note: when there is a categorical predictor, this corresponds to some ‚Äúdefault‚Äù category."
  },
  {
    "objectID": "notes/lec06-multiple-regression.html#prediction",
    "href": "notes/lec06-multiple-regression.html#prediction",
    "title": "Multiple linear regression",
    "section": "Prediction",
    "text": "Prediction\nWe can still make predictions in R; now we require more covariates. For example:\n\nnew_penguin &lt;- tibble(\n  flipper_length_mm = 180, \n  bill_length_mm = 40,\n  sex = \"male\",\n  island = \"Dream\"\n)\n\ncat(\"The predicted body mass of the new penguin is:\\n\")\n\nThe predicted body mass of the new penguin is:\n\npredict(penguin_fit, new_penguin)\n\n       1 \n3391.494 \n\n\n\n\n\n\n\n\nWarning\n\n\n\nRegression shows association not causality."
  },
  {
    "objectID": "notes/lec08-inference-2.html",
    "href": "notes/lec08-inference-2.html",
    "title": "Inference part II",
    "section": "",
    "text": "This section re-states the definitions of variance and covariance as well as lists a few important properties.\nIn this section, let \\(y\\) and \\(z\\) be random variables and let \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{z}\\) be random vectors. Let \\(a\\) be a constant and let \\(\\boldsymbol{A}\\) be a constant matrix.\n\n\n\n\n\n\nDefinition: variance\n\n\n\n\\[\n\\text{var}(y) = E[(y - E[y])^2] = E[y^2] - \\left(E[y]\\right)^2\n\\]\nSimilarly,\n\\[\n\\text{var}(\\boldsymbol{y}) = E \\boldsymbol{y}\\boldsymbol{y}^T - E\\boldsymbol{y}E\\boldsymbol{y}^T\n\\]\nNotice that the ‚Äúvariance of a vector‚Äù is a square matrix where each entry is the covariance between each pair of elements of the random vector.\n\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\[\n\\text{cov}(y, z) = E[(y - E[y])(z - E[z])] = E[yz] - \\left(E[y]E[z]\\right)\n\\]\nSimilarly,\n\\[\n\\text{cov}(\\boldsymbol{y}, \\boldsymbol{z}) = E \\boldsymbol{y}\\boldsymbol{z}^T - E\\boldsymbol{y}E\\boldsymbol{z}^T\n\\]\nNotice that \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{y}) = \\text{var}(\\boldsymbol{y})\\)\n\n\nProperties\n\n\\(\\text{var}(ay) = a^2 \\text{var}(y)\\) and similarly, \\(\\text{var}(\\boldsymbol{A} \\boldsymbol{y}) = \\boldsymbol{A} \\text{var}(\\boldsymbol{y}) \\boldsymbol{A}^T\\)\nBilinearity: \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{az}) = a \\cdot \\text{cov}(\\boldsymbol{y}, \\boldsymbol{z})\\)\nvariance of a sum: \\(\\text{var}(y + z) = \\text{var}(y) + \\text{var}(z) + 2\\text{cov}(y, z)\\)\n\n\nExerciseSolution\n\n\nWhat is \\(\\text{var}(ay - bz)\\)? Hint: apply properties 2 and 3 together.\n\n\n\\(\\text{var}(ay - bz) = \\text{var}(ay) + \\text{var}(-bz) + 2 \\text{cov}(ay, -bz) = a^2 \\text{var}(y) + b^2\\text{var}(z) - 2ab\\text{cov}(y, z)\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#background",
    "href": "notes/lec08-inference-2.html#background",
    "title": "Inference part II",
    "section": "",
    "text": "This section re-states the definitions of variance and covariance as well as lists a few important properties.\nIn this section, let \\(y\\) and \\(z\\) be random variables and let \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{z}\\) be random vectors. Let \\(a\\) be a constant and let \\(\\boldsymbol{A}\\) be a constant matrix.\n\n\n\n\n\n\nDefinition: variance\n\n\n\n\\[\n\\text{var}(y) = E[(y - E[y])^2] = E[y^2] - \\left(E[y]\\right)^2\n\\]\nSimilarly,\n\\[\n\\text{var}(\\boldsymbol{y}) = E \\boldsymbol{y}\\boldsymbol{y}^T - E\\boldsymbol{y}E\\boldsymbol{y}^T\n\\]\nNotice that the ‚Äúvariance of a vector‚Äù is a square matrix where each entry is the covariance between each pair of elements of the random vector.\n\n\n\n\n\n\n\n\nDefinition: covariance\n\n\n\n\\[\n\\text{cov}(y, z) = E[(y - E[y])(z - E[z])] = E[yz] - \\left(E[y]E[z]\\right)\n\\]\nSimilarly,\n\\[\n\\text{cov}(\\boldsymbol{y}, \\boldsymbol{z}) = E \\boldsymbol{y}\\boldsymbol{z}^T - E\\boldsymbol{y}E\\boldsymbol{z}^T\n\\]\nNotice that \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{y}) = \\text{var}(\\boldsymbol{y})\\)\n\n\nProperties\n\n\\(\\text{var}(ay) = a^2 \\text{var}(y)\\) and similarly, \\(\\text{var}(\\boldsymbol{A} \\boldsymbol{y}) = \\boldsymbol{A} \\text{var}(\\boldsymbol{y}) \\boldsymbol{A}^T\\)\nBilinearity: \\(\\text{cov}(\\boldsymbol{y}, \\boldsymbol{az}) = a \\cdot \\text{cov}(\\boldsymbol{y}, \\boldsymbol{z})\\)\nvariance of a sum: \\(\\text{var}(y + z) = \\text{var}(y) + \\text{var}(z) + 2\\text{cov}(y, z)\\)\n\n\nExerciseSolution\n\n\nWhat is \\(\\text{var}(ay - bz)\\)? Hint: apply properties 2 and 3 together.\n\n\n\\(\\text{var}(ay - bz) = \\text{var}(ay) + \\text{var}(-bz) + 2 \\text{cov}(ay, -bz) = a^2 \\text{var}(y) + b^2\\text{var}(z) - 2ab\\text{cov}(y, z)\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#last-time",
    "href": "notes/lec08-inference-2.html#last-time",
    "title": "Inference part II",
    "section": "Last time",
    "text": "Last time\nLast time we asked the question: what assumptions are required so that \\(\\hat{\\beta}\\) is a good representation of \\(\\beta\\)?\n\n\n\n\n\n\nAssumption 1\n\n\n\n\\(E[\\boldsymbol{\\varepsilon}|\\boldsymbol{x}] = \\boldsymbol{0}\\), or equivalently, \\(E[\\varepsilon_i|\\boldsymbol{x}] = 0\\) for all \\(i\\).\n\n\n\nExerciseSolution\n\n\nShow that assumption 1 implies that \\(E[\\hat{\\beta}|\\boldsymbol{x}] = \\beta\\).\n\n\n\\[\n\\begin{aligned}\nE[\\hat{\\beta}|\\boldsymbol{X}] &= E[(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T E[\\boldsymbol{y}| \\boldsymbol{X}]\\\\\n&= (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T \\boldsymbol{X}\\beta\\\\\n&= \\beta\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nAssumption 2\n\n\n\nConstant Variance: \\(\\text{var}(\\varepsilon_i | \\boldsymbol{X}) = \\sigma^2\\) for all \\(i\\),\nand uncorrelated errors: \\(\\text{cov}(\\varepsilon_{i}, \\varepsilon_j) = 0\\) for all pairs \\(i, j\\) where \\(i \\neq j\\).\nEquivalently, the two statements above can be written together concisely in matrix form:\n\\[\n\\text{var}(\\boldsymbol{\\varepsilon}) = \\sigma^2 \\boldsymbol{I}\n\\]\n\n\n\nExercise\n\n\nShow that assumption 2 implies that \\(\\text{var}(\\hat{\\beta}|\\boldsymbol{X}) = \\sigma^2 (\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\)"
  },
  {
    "objectID": "notes/lec08-inference-2.html#interpreting-variance-of-hatbeta-in-simple-linear-regression",
    "href": "notes/lec08-inference-2.html#interpreting-variance-of-hatbeta-in-simple-linear-regression",
    "title": "Inference part II",
    "section": "Interpreting variance of \\(\\hat{\\beta}\\) in simple linear regression",
    "text": "Interpreting variance of \\(\\hat{\\beta}\\) in simple linear regression\n\n\\(\\hat{\\beta}_1\\)\nConsider simple linear regression, where we have 1 predictor variable \\(\\boldsymbol{x} = x_1, \\ldots, x_n\\) and \\(\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\).\nLet‚Äôs compute the variance of \\(\\hat{\\beta}_1\\):\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\n&= \\text{var}\n\\left(\n\\frac{\\sum_{i=1}^n (x_i - \\bar{x}) y_i}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\big| \\boldsymbol{x}\n\\right)\\\\\n&=\n\\frac{1}{\\left(\\sum_{i=1}^n (x_i - \\bar{x})^2\\right)^2} \\cdot \\sum_{i=1}^n (x_i - \\bar{x})^2 \\text{var}(y_i)\\\\\n&= \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\end{aligned}\n\\]\nThe second equality above follows from the fact that (1) \\(x_1, \\ldots, x_n\\) are constant and (2) the \\(y_i\\)‚Äôs are uncorrelated.\nWe can multiply by ‚Äú1‚Äù in a fancy way to rearrange slightly:\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x}) &=\n\\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\cdot \\frac{1/n}{1/n}\\\\\n&= \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})}\n\\end{aligned}\n\\]\nwhere the last equality follows from the definition of the variance.\n\n\n\n\n\n\nImportant\n\n\n\nThe important take-away point here is that the variance of \\(\\hat{\\beta}_1\\) depends on - the variance of the error, \\(\\sigma^2\\) - the number of samples, \\(n\\), and - the variance of \\(x_1, \\ldots, x_n\\)\n\n\n\n\n\\(\\hat{\\beta}_0\\)\nRecall that \\(\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\). Therefore,\n\\[\n\\begin{aligned}\n\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x})\n&=\n\\text{var}(\\bar{y} - \\hat{\\beta}_1 \\bar{x}| \\boldsymbol{x})\\\\\n&=\n\\text{var}(\\bar{y} | \\boldsymbol{x}) + \\bar{x}^2 \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x}) - 2 \\bar{x} \\text{cov}(\\bar{y}, \\hat{\\beta}_1 | \\boldsymbol{x})\n\\\\\n&= \\text{var}\\left(\n\\frac{1}{n} \\sum y_i \\Big|\\boldsymbol{x}\n\\right) +\n\\bar{x}^2 \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})} - 0\\\\\n&= \\frac{1}{n} \\sigma^2  + \\bar{x}^2 \\frac{\\sigma^2}{n} \\cdot \\frac{1}{\\text{var}(\\boldsymbol{x})}\\\\\n&= \\frac{1}{n} \\sigma^2 \\left(\n1 + \\frac{\\bar{x}^2}{\\text{var}(\\boldsymbol{x})}\n\\right)\n\\end{aligned}\n\\]\nwhere the second inequality follows from the first exercise in these notes. The third equality follows from the fact that \\(\\text{cov}(\\bar{y}, \\hat{\\beta}_1 | \\boldsymbol{x})\\) is 0. To see this,\n\n\n\n\n\n\nclick here\n\n\n\n\n\nFor notational convenience, I‚Äôll drop the writing of the conditioning on \\(\\boldsymbol{x}\\).\n\\[\n\\begin{aligned}\n\\text{cov}(\\bar{y}, \\hat{\\beta}_1)\n&= \\text{cov}\\left(\n\\frac{1}{n}\\sum_{j=1}^n y_j,\\sum_{i=1}^n w_iy_i\n\\right)\n\\end{aligned}\n\\] where\n\\(w_i = \\frac{(x_i - \\bar{x})}{\\sum(x_k - \\bar{x})^2}\\). Note \\(\\sum_i w_i = 0\\).\nContinuing the proof,\n\\[\n\\text{cov}\\left(\n\\frac{1}{n}\\sum_{j=1}^n y_j,\\sum_{i=1}^n w_iy_i\n\\right) =\n\\frac{1}{n} \\sum_j \\sum_i w_i \\text{cov}(y_j, y_i)\n\\]\nNotice \\(\\text{cov}(y_i, y_j) = 0\\) for \\(i \\neq j\\). If \\(i = j\\), then \\(\\text{cov}(y_j, y_i) = \\text{var}(y_i) = \\sigma^2\\).\nThis let‚Äôs us simplify:\n\\[\n\\frac{1}{n} \\sigma^2 \\sum_i w_i = 0\n\\]\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe important take-away point here is that \\(\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x}) \\geq \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\\)\nQuestion: under what circumstance does \\(\\text{var}(\\hat{\\beta}_0 | \\boldsymbol{x}) = \\text{var}(\\hat{\\beta}_1 | \\boldsymbol{x})\\)?"
  },
  {
    "objectID": "notes/lecture-notes-playground.html",
    "href": "notes/lecture-notes-playground.html",
    "title": "lecture-notes-playground",
    "section": "",
    "text": "library(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# helper to add a vector with a large diamond tip\nadd_vector &lt;- function(fig, x, y, z, color, name=NULL) {\n  fig %&gt;%\n    add_trace(\n      type = \"scatter3d\",\n      mode = \"lines+markers\",\n      x = c(0, x),\n      y = c(0, y),\n      z = c(0, z),\n      line = list(width = 6, color = color),\n      marker = list(\n        size = c(1, 12),  # &lt;-- make diamond much larger\n        color = c(color, color),\n        symbol = c(\"circle\", \"diamond\")\n      ),\n      name = name,\n      showlegend = TRUE\n    )\n}\n\n# build plot with all six vectors\nfig &lt;- plot_ly() %&gt;%\n  add_vector(1, 1, 1, \"blue\", \"(1,1,1)\") %&gt;%\n  add_vector(2, 0, 0, \"green\", \"(2,0,0)\") %&gt;%\n  add_vector(0, 1, 0, \"red\", \"(0,1,0)\") %&gt;%\n  add_vector(1, 0, 0, \"purple\", \"(1,0,0)\") %&gt;%\n  add_vector(0, 2, 0, \"orange\", \"(0,2,0)\") %&gt;%\n  add_vector(0, 0, 2, \"brown\", \"(0,0,2)\") %&gt;%\n  layout(\n    scene = list(\n      xaxis = list(title = \"X\"),\n      yaxis = list(title = \"Y\"),\n      zaxis = list(title = \"Z\"),\n      camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8))\n    )\n  )\n\nfig\n\n\n\n\n\n\n{r} # # install.packages(\"plotly\") # library(plotly) #  # # --- small helpers ----------------------------------------------------------- #  # # cross product (no extra packages) # vcross &lt;- function(a, b) { #   c(a[2]*b[3] - a[3]*b[2], #     a[3]*b[1] - a[1]*b[3], #     a[1]*b[2] - a[2]*b[1]) # } #  # # Rodrigues' rotation matrix to rotate vector a onto b # rot_from_a_to_b &lt;- function(a, b) { #   a &lt;- a / sqrt(sum(a^2)) #   b &lt;- b / sqrt(sum(b^2)) #   v &lt;- vcross(a, b) #   s &lt;- sqrt(sum(v^2)) #   cth &lt;- sum(a * b) #   I3 &lt;- diag(3) #   if (s &lt; 1e-12) { #     # parallel or anti-parallel #     if (cth &gt; 0) return(I3)                 # already aligned #     # 180¬∞: rotate around any axis ‚üÇ to a; x-axis works since a = (0,0,¬±1) #     return(matrix(c(1,0,0, 0,-1,0, 0,0,-1), nrow=3, byrow=TRUE)) #   } #   K &lt;- matrix(c(  0,   -v[3],  v[2], #                  v[3],   0,   -v[1], #                 -v[2],  v[1],   0), nrow=3, byrow=TRUE) #   I3 + K + K %*% K * ((1 - cth) / (s^2)) # } #  # # Build a cone mesh at `tip` pointing along `dir`. # # The cone is defined in local coords as: tip at origin, base at z = -length. # cone_mesh &lt;- function(tip, dir, length = 0.15, radius = 0.06, n = 24) { #   u &lt;- as.numeric(dir) #   if (sqrt(sum(u^2)) &lt; 1e-12) stop(\"Direction for cone is zero-length.\") #   u &lt;- u / sqrt(sum(u^2)) #  #   # Local cone vertices: tip + circular base in plane z = -length #   theta &lt;- seq(0, 2*pi, length.out = n + 1)[- (n + 1)]  # n points, exclude duplicate #   tip_local  &lt;- c(0, 0, 0) #   base_local &lt;- rbind(radius * cos(theta), radius * sin(theta), -length) #  #   # Rotate from local +Z axis to u #   R &lt;- rot_from_a_to_b(c(0,0,1), u) #   tip_world  &lt;- R %*% tip_local #   base_world &lt;- R %*% base_local #  #   # Translate to desired tip position #   tip_world  &lt;- tip_world  + tip #   base_world &lt;- sweep(base_world, 2, tip, `+`) #  #   # Assemble vertices: tip first, then base ring #   verts &lt;- cbind(tip_world, base_world)  # 3 x (1+n) #   x &lt;- verts[1,]; y &lt;- verts[2,]; z &lt;- verts[3,] #  #   # Triangles for the side surface: (tip, base_i, base_{i+1}) #   # Indices for plotly mesh3d are 0-based. #   tip_idx  &lt;- 0 #   base_idx &lt;- 1:(n)  # 1-based in R #   i_idx &lt;- rep(tip_idx, n) #   j_idx &lt;- base_idx #   k_idx &lt;- c(base_idx[-1], 1) #  #   list( #     x = x, y = y, z = z, #     i = i_idx, j = j_idx, k = k_idx #   ) # } #  # # Add one vector (line) + cone head to a plotly figure # add_vector_with_cone &lt;- function(fig, x, y, z, color = \"blue\", #                                  shaft_width = 6, #                                  cone_len = 0.15, cone_radius = 0.06, cone_n = 24, #                                  name = NULL) { #   tip &lt;- c(x, y, z) #   dir &lt;- c(x, y, z) #  #   # line (shaft) #   fig &lt;- fig %&gt;% #     add_trace( #       type = \"scatter3d\", mode = \"lines\", #       x = c(0, x), y = c(0, y), z = c(0, z), #       line = list(width = shaft_width, color = color), #       name = name %||% sprintf(\"(%g,%g,%g)\", x, y, z), #       showlegend = TRUE #     ) #  #   # cone head #   m &lt;- cone_mesh(tip = tip, dir = dir, length = cone_len, radius = cone_radius, n = cone_n) #   fig %&gt;% #     add_trace( #       type = \"mesh3d\", #       x = m$x, y = m$y, z = m$z, #       i = m$i, j = m$j, k = m$k, #       color = color, #       opacity = 1, #       showscale = FALSE, #       lighting = list(ambient = 0.5, diffuse = 0.8, specular = 0.2), #       hoverinfo = \"skip\", #       name = name %||% sprintf(\"(%g,%g,%g) head\", x, y, z), #       showlegend = FALSE #     ) # } #  # `%||%` &lt;- function(a, b) if (is.null(a)) b else a #  # # --- demo: your six vectors with true cone heads ----------------------------- #  # fig &lt;- plot_ly() #  # fig &lt;- fig %&gt;% #   add_vector_with_cone(1, 1, 1, color = \"blue\",  name = \"(1,1,1)\") %&gt;% #   add_vector_with_cone(2, 0, 0, color = \"green\", name = \"(2,0,0)\") %&gt;% #   add_vector_with_cone(0, 1, 0, color = \"red\",   name = \"(0,1,0)\") %&gt;% #   add_vector_with_cone(1, 0, 0, color = \"purple\",name = \"(1,0,0)\") %&gt;% #   add_vector_with_cone(0, 2, 0, color = \"orange\",name = \"(0,2,0)\") %&gt;% #   add_vector_with_cone(0, 0, 2, color = \"brown\", name = \"(0,0,2)\") #  # fig &lt;- fig %&gt;% #   layout( #     scene = list( #       xaxis = list(title = \"X\"), #       yaxis = list(title = \"Y\"), #       zaxis = list(title = \"Z\"), #       camera = list(eye = list(x = 1.5, y = 1.5, z = 0.8)) #     ) #   ) #  # fig #  #"
  },
  {
    "objectID": "prepare/prepare-lec02.html",
    "href": "prepare/prepare-lec02.html",
    "title": "Prepare: simple linear regression",
    "section": "",
    "text": "üìñ Read Simple linear regression, Section 4.1 - 4.6\nüìñ Read R for Data Science, Introduction: What you will learn\nüìñ Read GitHub for supporting, reusing, contributing, and failing safely\nFor computing introduction / review\nüé• Watch Meet the Toolkit: R + RStudio\nüé• Watch Meet the Toolkit: Quarto"
  },
  {
    "objectID": "prepare/prepare-lec04.html",
    "href": "prepare/prepare-lec04.html",
    "title": "Prepare: matrix representation",
    "section": "",
    "text": "Review linear algebra concepts (as needed)\n\nMatrices and vectors: [slides][video]\nMatrix-Vector products: [slides][video]\nMatrix multiplication: [slides][video]\n\n\n\n\n\n\n\nNote\n\n\n\nAll linear algebra review materials from Math 218: Matrices and Vectors (Summer 2024) taught by Dr.¬†Brian Fitzpatrick at Duke University"
  },
  {
    "objectID": "prepare/prepare-lec08.html",
    "href": "prepare/prepare-lec08.html",
    "title": "Prepare for Lecture 08: Inference for regression",
    "section": "",
    "text": "üìñ Read Inference for Simple Linear Regression:\n\nSections 5.1 - 5.3\nSection 5.6\nSection 5.8\nSection 5.9"
  },
  {
    "objectID": "prepare/prepare-lec20.html",
    "href": "prepare/prepare-lec20.html",
    "title": "Prepare for Lecture 20: Logistic regression - Prediction",
    "section": "",
    "text": "üìñ Classification module in Google Machine Learning Crash Course"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final project",
    "section": "",
    "text": "Research topics due Tuesday September 30 at 5:00pm\nProject proposal due Wednesday October 15 at 5:00pm\nExploratory data analysis due Tuesday November 4\nProject presentations (in lab) Wednesday November 12\nDraft report due Tuesday November 18 at 5:00pm\nPeer review Wednesday November 19, in lab\nFinal written report + reproducible GitHub repository due December 12 at 5:00pm."
  },
  {
    "objectID": "project.html#timeline",
    "href": "project.html#timeline",
    "title": "Final project",
    "section": "",
    "text": "Research topics due Tuesday September 30 at 5:00pm\nProject proposal due Wednesday October 15 at 5:00pm\nExploratory data analysis due Tuesday November 4\nProject presentations (in lab) Wednesday November 12\nDraft report due Tuesday November 18 at 5:00pm\nPeer review Wednesday November 19, in lab\nFinal written report + reproducible GitHub repository due December 12 at 5:00pm."
  },
  {
    "objectID": "project.html#description",
    "href": "project.html#description",
    "title": "Final project",
    "section": "Description",
    "text": "Description\nThe goal of the final project is for you to use regression analysis to analyze a data set of your own choosing. The data set may already exist or you may collect your own data by scraping the web.\nChoose the data based on your group‚Äôs interests or work you all have done in other courses or research projects. The goal of this project is for you to demonstrate proficiency in the techniques we have covered in this class (and beyond, if you like!) and apply them to a data set to analyze it in a meaningful way.\nAll analyses must be done in RStudio using Quarto and GitHub, and your analysis and written report must be reproducible."
  },
  {
    "objectID": "project.html#deliverables",
    "href": "project.html#deliverables",
    "title": "Final project",
    "section": "Deliverables",
    "text": "Deliverables\nYou will work on the project with your lab groups. The primary deliverables for the project are\n\nan in-person presentation about the exploratory data analysis and initial modeling\na written, reproducible final report detailing your analysis\na summary of your project highlights to share with the class\na GitHub repository containing all work from the project\n\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the primary deliverables.\nThere are intermediate milestones and peer review assignments throughout the semester to help you work towards the primary deliverables."
  },
  {
    "objectID": "project.html#research-topics",
    "href": "project.html#research-topics",
    "title": "Final project",
    "section": "Research topics",
    "text": "Research topics\nThe goal of this milestone is to discuss topics and develop potential research questions your team is interested in investigating for the project. You are only developing ideas at this point; you do not need to have a data set identified right now.\nDevelop three potential research topics. Include the following for each topic:\n\nA brief description of the topic\nA statement about your motivation for investigating this topic\nThe potential audience(s), i.e., who might be most interested in this research?\nTwo or three potential research questions you could analyze about this topic. (Note: These are draft questions at this point. You will finalize the questions in the next stage of the project.)\nIdeas about the type of data you might use to answer this question or potential data sets you‚Äôre interested in using. (Note: The goal is to generate ideas at this point, so it is fine if you have not identified any particular data sets at this point.)\n\nTurn-in: Write your responses in research-topics.qmd in your team‚Äôs project GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline, Tuesday, September 30 at 5:00pm. There is no Gradescope submission."
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Final project",
    "section": "Project proposal",
    "text": "Project proposal\nThe purpose of the project proposal is for your team to identify the data set you‚Äôre interested in analyzing to investigate one of your potential research topics. You will also do some preliminary exploration of the response variable and begin thinking about the modeling strategy. If you‚Äôre unsure where to find data, you can use the list of potential data sources here as a starting point.\n\n\n\n\n\n\nImportant\n\n\n\nYou must use the data set(s) in the proposal for the final project, unless instructed otherwise when given feedback.\n\n\nThe data set must meet the following criteria:\n\nAt least 500 observations\nAt least 10 columns, such that at least 6 of the columns are useful and unique predictor variables.\n\ne.g., identifier variables such as ‚Äúname‚Äù, ‚ÄúID number‚Äù, etc. are not useful predictor variables.\ne.g., if you have multiple columns with the same information (e.g.¬†‚Äústate abbreviation‚Äù and ‚Äústate name‚Äù), then they are not unique predictors.\n\nAt least one variable that can be identified as a reasonable response variable.\n\nThe response variable can be quantitative or categorical.\n\nA mix of quantitative and categorical variables that can be used as predictors.\nMay not be data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\nThe project proposal should include the following:\n\nA statement of a research question.\nWhy your research question is important.\nA description of your data\nA description of how your data will be used to answer your research question\nA description of your planned analysis (what is the outcome variable, relevant predictors, regression technique, e.g.¬†multiple linear regression for continuous outcome vs logistic regression for a binary outcome). We will cover logistic regression in the coming weeks but you can read ahead about it here.\n\nTurn-in: submit a pdf proposal to Gradescope by the deadline."
  },
  {
    "objectID": "notes/lec10-hypothesis.html",
    "href": "notes/lec10-hypothesis.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT) # datatable viewing\nlibrary(patchwork)\nlibrary(knitr)\n\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\",\n  fig.align = \"center\"\n)\n\nset.seed(221)\n\nfootball &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/ncaa-football-exp.csv\") |&gt;\n  mutate(nonsense = runif(n(), 0, 10))"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#data-ncaa-football-expenditures",
    "href": "notes/lec10-hypothesis.html#data-ncaa-football-expenditures",
    "title": "Hypothesis testing",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nToday‚Äôs data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 (2019 - 2020 season) expenditures on football for institutions in the NCAA - Division 1 FBS (Football Bowl Subdivision). The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\nnonsense: a created variable (see above) which has nothing to do with expenditure"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#univariate-eda",
    "href": "notes/lec10-hypothesis.html#univariate-eda",
    "title": "Hypothesis testing",
    "section": "Univariate EDA",
    "text": "Univariate EDA"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#bivariate-eda",
    "href": "notes/lec10-hypothesis.html#bivariate-eda",
    "title": "Hypothesis testing",
    "section": "Bivariate EDA",
    "text": "Bivariate EDA"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#regression-model",
    "href": "notes/lec10-hypothesis.html#regression-model",
    "title": "Hypothesis testing",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type + nonsense, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#from-sample-to-population",
    "href": "notes/lec10-hypothesis.html#from-sample-to-population",
    "title": "Hypothesis testing",
    "section": "From sample to population",
    "text": "From sample to population\nFor every additional 1,000 students, we expect an institution‚Äôs total expenditures on football to increase by $780,000, on average, holding institution type constant.\n\nThis estimate is valid for the single sample of 127 higher education institutions in the 2019 - 2020 academic year.\nBut what if we‚Äôre not interested quantifying the relationship between student enrollment, institution type, and football expenditures for this single sample?\nWhat if we want to say something about the relationship between these variables for all colleges and universities with football programs and across different years?"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#varhatboldsymbolbeta-for-ncaa-data",
    "href": "notes/lec10-hypothesis.html#varhatboldsymbolbeta-for-ncaa-data",
    "title": "Hypothesis testing",
    "section": "\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data",
    "text": "\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\nsigma_sq &lt;- glance(exp_fit)$sigma^2\n\nvar_beta &lt;- sigma_sq * solve(t(X) %*% X)\nvar_beta\n\n              (Intercept) enrollment_th typePublic     nonsense\n(Intercept)    12.4139465  -0.170593886 -5.4231006 -0.692309267\nenrollment_th  -0.1705939   0.012597357 -0.1315619  0.007350219\ntypePublic     -5.4231006  -0.131561941 10.1018139 -0.136025423\nnonsense       -0.6923093   0.007350219 -0.1360254  0.137611482"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#sehatboldsymbolbeta-for-ncaa-data",
    "href": "notes/lec10-hypothesis.html#sehatboldsymbolbeta-for-ncaa-data",
    "title": "Hypothesis testing",
    "section": "\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data",
    "text": "\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n\n\n\n\n\n\nsqrt(diag(var_beta))\n\n  (Intercept) enrollment_th    typePublic      nonsense \n    3.5233431     0.1122379     3.1783351     0.3709602"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#example",
    "href": "notes/lec10-hypothesis.html#example",
    "title": "Hypothesis testing",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#compute-p-value",
    "href": "notes/lec10-hypothesis.html#compute-p-value",
    "title": "Hypothesis testing",
    "section": "Compute p-value",
    "text": "Compute p-value\n\nn &lt;- nrow(football)\np &lt;- 4\n2 * (1 - pt(abs(0.803), n - p))\n\n[1] 0.4235237\n\n\nVisually,"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#t-vs.-n01",
    "href": "notes/lec10-hypothesis.html#t-vs.-n01",
    "title": "Hypothesis testing",
    "section": "t vs.¬†N(0,1)",
    "text": "t vs.¬†N(0,1)\n\n\n\n\n\n\n\n\nFigure¬†1: Standard normal vs.¬†t distributions"
  },
  {
    "objectID": "notes/lec11-ci.html",
    "href": "notes/lec11-ci.html",
    "title": "Confidence intervals",
    "section": "",
    "text": "View libraries and data sets used in these notes\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(DT) # datatable viewing\nlibrary(patchwork)\nlibrary(knitr)\n\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\",\n  fig.align = \"center\"\n)\n\nset.seed(221)\n\nfootball &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/ncaa-football-exp.csv\") |&gt;\n  mutate(nonsense = runif(n(), 0, 10))"
  },
  {
    "objectID": "notes/lec11-ci.html#data-ncaa-football-expenditures",
    "href": "notes/lec11-ci.html#data-ncaa-football-expenditures",
    "title": "Confidence intervals",
    "section": "Data: NCAA Football expenditures",
    "text": "Data: NCAA Football expenditures\nSame data as before (reminder):\nToday‚Äôs data come from Equity in Athletics Data Analysis and includes information about sports expenditures and revenues for colleges and universities in the United States. This data set was featured in a March 2022 Tidy Tuesday.\nWe will focus on the 2019 (2019 - 2020 season) expenditures on football for institutions in the NCAA - Division 1 FBS (Football Bowl Subdivision). The variables are :\n\ntotal_exp_m: Total expenditures on football in the 2019 - 2020 academic year (in millions USD)\nenrollment_th: Total student enrollment in the 2019 - 2020 academic year (in thousands)\ntype: institution type (Public or Private)\nnonsense: a created variable (see above) which has nothing to do with expenditure"
  },
  {
    "objectID": "notes/lec11-ci.html#regression-model",
    "href": "notes/lec11-ci.html#regression-model",
    "title": "Confidence intervals",
    "section": "Regression model",
    "text": "Regression model\n\nexp_fit &lt;- lm(total_exp_m ~ enrollment_th + type + nonsense, data = football)\ntidy(exp_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423"
  },
  {
    "objectID": "notes/lec11-ci.html#confidence-interval-in-r",
    "href": "notes/lec11-ci.html#confidence-interval-in-r",
    "title": "Confidence intervals",
    "section": "Confidence interval in R",
    "text": "Confidence interval in R\nWe can compute the confidence intervals in R easily:\n\n# alpha = 0.05; CI = 95%\ntidy(exp_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n10.859\n24.807\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n0.574\n1.018\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n-19.812\n-7.229\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n-0.436\n1.032\n\n\n\n\n\nWe can verify manually. For example, for the intercept:\n\nn = nrow(football)\np = 4 # 4 entries in beta (dimension p)\n17.833 - (3.523 * qt(0.975, df = n - p))\n\n[1] 10.85944\n\n17.833 + (3.523 * qt(0.975, df = n - p))\n\n[1] 24.80656\n\n\nNotice that\n\nqt(0.975, df = n - p)\n\n[1] 1.979439\n\n\nis close to the number 2.\nWe could approximate the 95% CI:\n\n17.833 - (3.523 * 2)\n\n[1] 10.787\n\n17.833 + (3.523 * 2)\n\n[1] 24.879\n\n\nQuestion: when is this approximation valid?"
  },
  {
    "objectID": "notes/lec11-ci.html#prediction-interval",
    "href": "notes/lec11-ci.html#prediction-interval",
    "title": "Confidence intervals",
    "section": "Prediction interval",
    "text": "Prediction interval\nFor a public college with an enrollment of 50,000 and a nonsense variable of 9, what is the predicted 95% expenditure interval?\nPrediction:\n\nxstar_df &lt;- tibble(\n  enrollment_th = 50,\n  nonsense = 9,\n  type = \"Public\"\n)\npredict(exp_fit, newdata = xstar_df, interval = \"prediction\",\n        level = 0.95)\n\n     fit      lwr     upr\n1 46.811 22.54961 71.0724\n\n\n\nExerciseSolution\n\n\nCompute the CI manually.\n\n\n\\(\\hat{y}_*\\):\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\n\ny = football$total_exp_m\n\nbetaHat = solve(t(X) %*% X) %*% t(X) %*% y\n\nxstar = matrix(c(1, 50, 1, 9), ncol = 1)\nprediction = t(xstar) %*% betaHat\nprediction\n\n       [,1]\n[1,] 46.811\n\n\nConfidence interval:\n\nsigma_hat &lt;- glance(exp_fit)$sigma\nt &lt;- qt(0.975, df = n - p)\nse = sigma_hat * sqrt(1 + t(xstar) %*% solve(t(X) %*% X) %*% xstar)\n\nprediction - (se*t)\n\n         [,1]\n[1,] 22.54961\n\nprediction + (se*t)\n\n        [,1]\n[1,] 71.0724"
  },
  {
    "objectID": "labs/lab04.html",
    "href": "labs/lab04.html",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "",
    "text": "Important\n\n\n\nThis lab will not be submitted for grade."
  },
  {
    "objectID": "labs/lab04.html#exploratory-data-analysis",
    "href": "labs/lab04.html#exploratory-data-analysis",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\np1 &lt;- ggplot(data = tips, aes(x = Tip)) + \n  geom_histogram(color = \"white\", binwidth = 2) +\n  labs(x = \"Tips ($)\",\n       title = \"Tips at local restaurant\")\n\np2 &lt;- ggplot(data = tips, aes(x = Party)) + \n  geom_histogram(color = \"white\") +\n  labs(x = \"Party\",\n       title = \"Number of diners in party\") +\n  xlim(c(0, 7))\n\np3 &lt;- ggplot(data = tips, aes(x = Age)) + \n  geom_bar(color = \"white\") +\n  labs(x = \"\",\n       title = \"Age of Payer\") \n\np1 / (p2 + p3)\n\n\n\n\n\n\n\n\n\np4 &lt;- ggplot(data = tips, aes(x = Party, y = Tip)) + \n  geom_jitter() + \n  labs(x = \"Number of diners in party\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Party\")\n\np5 &lt;- ggplot(data = tips, aes(x = Age, y = Tip)) + \n  geom_boxplot() + \n  labs(x = \"Age of payer\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Age\")\n\np4 + p5\n\n\n\n\n\n\n\n\nWe will use the number of diners in the party and age of the payer to understand variability in the tips."
  },
  {
    "objectID": "labs/lab04.html#exercise-1",
    "href": "labs/lab04.html#exercise-1",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe will start with the main effects model that includes Age and Party.\n\nHow many indicator variables for Age can we create from the data?\nHow many indicator variables for Age will be in the regression model?\nAre the responses to parts a and b equal? If not, explain why not.\nWhich of the following is true for this model? Select all that apply.\n\nThe intercepts are the same for every level of Age.\nThe intercepts differ by Age.\nThe effect of Party is the same for every level of Age.\nThe effect of Party differs by Age."
  },
  {
    "objectID": "labs/lab04.html#exercise-2",
    "href": "labs/lab04.html#exercise-2",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider the main effects model that includes Age and Party.\n\nWhat is the dimension of the design matrix \\(\\mathbf{X}\\) for the main effects model?\nCalculate the coefficient estimates \\(\\hat{\\boldsymbol{\\beta}}\\) directly from the data (without using lm), but you can use stats::model.matrix() to grab the X matrix.\nWrite the equation of the estimated regression model.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-3",
    "href": "labs/lab04.html#exercise-3",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 3",
    "text": "Exercise 3\nConsider the main effects model that includes Age and Party. Get \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the data.\n\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(\\hat{\\sigma}_{\\epsilon}\\) .\nInterpret \\(\\hat{\\sigma}_\\epsilon\\) in the context of the data.\nCompute \\(Var(\\hat{\\boldsymbol{\\beta}})\\).\nYou wish to test whether there is a linear relationship between tips and the number of diners in the party, after adjusting for the age of the payer. Compute the test statistic.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-4",
    "href": "labs/lab04.html#exercise-4",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 4",
    "text": "Exercise 4\nConsider the main effects model that includes Age and Party. Get \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) from the data.\n\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(R^2\\). Interpret this value in the context of the data.\nUse \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\) to compute \\(RMSE\\). Interpret this value in the context of the data.\n\n\n# add code here"
  },
  {
    "objectID": "labs/lab04.html#exercise-5",
    "href": "labs/lab04.html#exercise-5",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou decide to add an interaction effect between Age and Party to the model and fit a model of the following form:\n\\[\nTip_i = \\beta_0 + \\beta_1Party_i + \\beta_2SenCit_i + \\beta_3Yadult_i + \\beta_4Party_i \\times SenCit_i + \\beta_5 Party_i \\times Yadult_i + \\epsilon_i\n\\]\n\nWhich of the following is true for this model? Select all that apply.\n\nThe intercepts are the same for every level of Age.\nThe intercepts differ by Age.\nThe effect of Party is the same for every level of Age.\nThe effect of Party differs by Age.\n\nBy how much does the intercept for tables with young adult payers differ from tables with middle age payers? Write the answer in terms of the \\(\\beta\\)‚Äôs.\nWrite the equation of the model for tables in which the payer is a senior citizen.\nSuppose you wish to test the hypotheses: \\(H_0: \\beta_5 = 0 \\text{ vs. }H_a: \\beta_5 \\neq 0\\) . State what is being tested in terms of the effect of Party."
  },
  {
    "objectID": "labs/lab04.html#exercise-6",
    "href": "labs/lab04.html#exercise-6",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the lm() function to fit the model that includes Age and Party and the interaction between the two variables. Display the 90% confidence interval for the coefficients.\n\nThe standard error for AgeSenCit is 0.784. State what this value means in the context of the data.\nWrite code to show how the 90% confidence interval for AgeSenCit was computed.\nBased on the confidence interval, is there evidence that tables with senior citizen payers tip differently on average than tables with middle age payers?"
  },
  {
    "objectID": "labs/lab04.html#exercise-7",
    "href": "labs/lab04.html#exercise-7",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 7",
    "text": "Exercise 7\nThe following are general questions about regression. They are not specific to the tips data set.\n\nWhat does it mean for an estimator to be the ‚Äúleast-squares‚Äù estimator?\nConsider the derivation of the least-squares estimator:\n\\[\n\\begin{aligned}\n\\nabla_{\\beta}\\boldsymbol{\\epsilon}^\\mathsf{T}\\boldsymbol{\\epsilon} &= \\nabla_{\\boldsymbol{\\beta}}[(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})] \\\\[5pt]\n&=\\nabla_{\\boldsymbol{\\beta}}[\\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}] \\\\[5pt]\n&=\\nabla_{\\boldsymbol{\\beta}}[\\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta}]\\\\[5pt]\n& = -2\\mathbf{X}^\\mathsf{T}\\mathbf{y} + 2\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} \\\\[5pt]\n&\\Rightarrow \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\n\\end{aligned}\n\\]\n\nExplain how \\(-\\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) is simplified to \\(-2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) when going from lines 2 to 3.\n\nExplain what rules were used to compute the gradient in line 4."
  },
  {
    "objectID": "labs/lab04.html#exercise-8",
    "href": "labs/lab04.html#exercise-8",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Exercise 8",
    "text": "Exercise 8\nBelow we show how SSR = \\(\\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\)\n\\[\n\\begin{aligned}\nSSR = \\mathbf{e}^\\mathsf{T}\\mathbf{e} &= (\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}})^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\hat{\\mathbf{\\beta}}) \\\\[5pt]\n& = \\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\mathbf{y}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} + \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} \\\\[5pt]\n&= \\mathbf{y}^\\mathsf{T}\\mathbf{y} - 2\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}+\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\\\[5pt]\n& = \\mathbf{y}^\\mathsf{T}\\mathbf{y} - \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\n\\end{aligned}\n\\]\na. Explain how \\(-\\mathbf{y}^\\mathsf{T}\\mathbf{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) is simplified to \\(-2\\boldsymbol{\\beta}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y}\\) when going from lines 2 to 3.\nb. Explain how we know \\(\\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{y} = \\hat{\\boldsymbol{\\beta}}^\\mathsf{T}\\mathbf{X}^\\mathsf{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}}\\) when going from lines 3 to 4."
  },
  {
    "objectID": "labs/lab04.html#footnotes",
    "href": "labs/lab04.html#footnotes",
    "title": "Lab 04: Main effects and interaction effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. ‚ÄúThe Effects of Credit Cards on Tipping.‚Äù Project for Statistics 212-Statistics for the Sciences, St.¬†Olaf College.‚Ü©Ô∏é"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#packages-in-these-notes",
    "href": "labs/slides/lab-interactions.html#packages-in-these-notes",
    "title": "Interaction effects",
    "section": "Packages in these notes",
    "text": "Packages in these notes\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidymodels)"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-terms",
    "href": "labs/slides/lab-interactions.html#interaction-terms",
    "title": "Interaction effects",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#example-penguins",
    "href": "labs/slides/lab-interactions.html#example-penguins",
    "title": "Interaction effects",
    "section": "Example: penguins",
    "text": "Example: penguins\nThe relationship between penguin bill length and body mass depends on the penguin‚Äôs island.\n\nplotcodediscuss\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins |&gt;\n  ggplot(aes(x = bill_length_mm, \n             y = body_mass_g, color = island)) +\n  geom_point() +\n  theme_bw() +\n  geom_smooth(method = 'lm', se = F) +\n  labs(x = \"Bill length (mm)\", \n       y = \"Body mass (g)\", \n       title = \"Island interaction effect\")\n\n\n\nThe lines are not parallel indicating there is a potential interaction effect. The slope for bill length differs based on the penguin‚Äôs island."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-term-in-model",
    "href": "labs/slides/lab-interactions.html#interaction-term-in-model",
    "title": "Interaction effects",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\npenguin_fit &lt;- lm(body_mass_g ~ bill_length_mm * island,\n      data = penguins)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1726.02\n292.08\n-5.91\n0\n\n\nbill_length_mm\n142.34\n6.42\n22.18\n0\n\n\nislandDream\n4478.69\n395.31\n11.33\n0\n\n\nislandTorgersen\n2870.56\n777.69\n3.69\n0\n\n\nbill_length_mm:islandDream\n-120.60\n8.77\n-13.75\n0\n\n\nbill_length_mm:islandTorgersen\n-76.57\n19.53\n-3.92\n0"
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interaction-term-in-model-1",
    "href": "labs/slides/lab-interactions.html#interaction-term-in-model-1",
    "title": "Interaction effects",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nExerciseSolution\n\n\n\n\nWrite the regression equation for penguins from island Torgersen\nWrite the regression equation for penguins from island Dream\nWrite the regression equation for penguins from island Biscoe\n\n\n\n\nPenguins equations for each island:\n\\[\n\\begin{aligned}\n\\hat{y}_{\\text{torgersen}} &= (-1726.02 + 2870.56)  + (142.34 - 76.57)x_1\\\\\n&= 1144.54 + 65.77x_1\\\\\n\\hat{y}_{\\text{dream}} &= (-1726.02 + 4478.69   )   + (142.34 - 120.60)x_1\\\\\n&= 2752.67 + 21.74x_1\\\\\n\\hat{y}_{\\text{biscoe}} &= -1726.02 + 142.34 x_1\n\\end{aligned}\n\\]\nwhere \\(\\hat{y}\\) is the predicted body mass for a given bill length \\(x_1\\)."
  },
  {
    "objectID": "labs/slides/lab-interactions.html#interpreting-interaction-terms",
    "href": "labs/slides/lab-interactions.html#interpreting-interaction-terms",
    "title": "Interaction effects",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\nWhat the interaction means:\n\nThe effect of bill length on body mass differs by -76.5 when the penguin comes from island Torgersen instead of Biscoe.\nThe effect of bill length on body mass differs by -120 when the penguin comes from island Dream instead of Biscoe."
  },
  {
    "objectID": "notes/lec10-hypothesis.html#manually-computing-the-standard-error",
    "href": "notes/lec10-hypothesis.html#manually-computing-the-standard-error",
    "title": "Hypothesis testing",
    "section": "Manually computing the standard error",
    "text": "Manually computing the standard error\n\n\\(Var(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\nX &lt;- model.matrix(total_exp_m ~ enrollment_th + type + nonsense, \n                  data = football)\nsigma_sq &lt;- glance(exp_fit)$sigma^2\n\nvar_beta &lt;- sigma_sq * solve(t(X) %*% X)\nvar_beta\n\n              (Intercept) enrollment_th typePublic     nonsense\n(Intercept)    12.4139465  -0.170593886 -5.4231006 -0.692309267\nenrollment_th  -0.1705939   0.012597357 -0.1315619  0.007350219\ntypePublic     -5.4231006  -0.131561941 10.1018139 -0.136025423\nnonsense       -0.6923093   0.007350219 -0.1360254  0.137611482\n\n\n\n\n\\(SE(\\hat{\\boldsymbol{\\beta}})\\) for NCAA data\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.833\n3.523\n5.061\n0.000\n\n\nenrollment_th\n0.796\n0.112\n7.095\n0.000\n\n\ntypePublic\n-13.520\n3.178\n-4.254\n0.000\n\n\nnonsense\n0.298\n0.371\n0.803\n0.423\n\n\n\n\n\n\nsqrt(diag(var_beta))\n\n  (Intercept) enrollment_th    typePublic      nonsense \n    3.5233431     0.1122379     3.1783351     0.3709602"
  },
  {
    "objectID": "notes/lec10-hypothesis.html#p-value",
    "href": "notes/lec10-hypothesis.html#p-value",
    "title": "Hypothesis testing",
    "section": "p-value",
    "text": "p-value\nA p-value is: (1) a tail probability of a statistic under the null hypothesis, (2) the lowest value of \\(\\alpha\\) such that \\(H_0\\) is rejected, (3) \\(Pr(|t| &gt; t_{\\beta_j (obs)})\\).\nIn R we can compute it: \\(2 \\times\\) (1 - pt(abs(t), n - p))."
  },
  {
    "objectID": "notes/lec12.html",
    "href": "notes/lec12.html",
    "title": "Checking assumptions",
    "section": "",
    "text": "Code\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "notes/lec12.html#computing-set-up",
    "href": "notes/lec12.html#computing-set-up",
    "title": "Checking assumptions",
    "section": "",
    "text": "Code\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "notes/lec12.html#topics",
    "href": "notes/lec12.html#topics",
    "title": "Checking assumptions",
    "section": "Topics",
    "text": "Topics\n\nModel conditions\nInfluential points\nModel diagnostics\n\nLeverage\nStudentized residuals\nCook‚Äôs Distance"
  },
  {
    "objectID": "notes/lec12.html#data-duke-lemurs",
    "href": "notes/lec12.html#data-duke-lemurs",
    "title": "Checking assumptions",
    "section": "Data: Duke lemurs",
    "text": "Data: Duke lemurs\nToday‚Äôs data contains a subset of the original Duke Lemur data set available in the TidyTuesday GitHub repo. This data includes information on ‚Äúyoung adult‚Äù lemurs from the Coquerel‚Äôs sifaka species (PCOQ), the largest species at the Duke Lemur Center. The analysis will focus on the following variables:\n\nage_at_wt_mo: Age in months: Age of the animal when the weight was taken, in months (((Weight_Date-DOB)/365)*12)\nweight_g: Weight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights &gt;500g generally to the nearest 1-20g.\n\nThe goal of the analysis is to use the age of the lemurs to understand variability in the weight.\n\n\nRows: 76 Columns: 54\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (17): taxon, hybrid, sex, name, current_resident, estimated_dob, birth_...\ndbl  (28): dlc_id, stud_book, birth_month, litter_size, expected_gestation, ...\nlgl   (1): age_last_verified_y\ndate  (8): dob, estimated_concep, dam_dob, sire_dob, dod, weight_date, conce...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "notes/lec12.html#eda",
    "href": "notes/lec12.html#eda",
    "title": "Checking assumptions",
    "section": "EDA",
    "text": "EDA\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "notes/lec12.html#eda-1",
    "href": "notes/lec12.html#eda-1",
    "title": "Checking assumptions",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "notes/lec12.html#fit-model",
    "href": "notes/lec12.html#fit-model",
    "title": "Checking assumptions",
    "section": "Fit model",
    "text": "Fit model\n\nlemurs_fit &lt;- lm(weight_g ~ age_at_wt_mo, data = lemurs)\n\ntidy(lemurs_fit) |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-12314.360\n4252.696\n-2.896\n0.005\n\n\nage_at_wt_mo\n496.591\n131.225\n3.784\n0.000"
  },
  {
    "objectID": "notes/lec12.html#assumptions-for-regression",
    "href": "notes/lec12.html#assumptions-for-regression",
    "title": "Checking assumptions",
    "section": "Assumptions for regression",
    "text": "Assumptions for regression\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the errors (residuals) is approximately normal.\nIndependence: The errors (residuals) are independent from one another.\n\n. . .\n\nHow do we know if these assumptions hold in our data?"
  },
  {
    "objectID": "notes/lec12.html#linearity",
    "href": "notes/lec12.html#linearity",
    "title": "Checking assumptions",
    "section": "Linearity",
    "text": "Linearity\n\nLook at plot of residuals versus fitted (predicted) values.\nLinearity is satisfied if there is no discernible pattern in the plot (i.e., points randomly scattered around \\(residuals = 0\\)\n\n. . .\n\n\n\n\n\n\n\n\n\n. . .\n\nLinearity is satisfied"
  },
  {
    "objectID": "notes/lec12.html#example-linearity-not-satisfied",
    "href": "notes/lec12.html#example-linearity-not-satisfied",
    "title": "Checking assumptions",
    "section": "Example: Linearity not satisfied",
    "text": "Example: Linearity not satisfied\n\n\n\n\n\n\n\n\n\n. . .\n\nIf linearity is not satisfied, examine the plots of residuals versus each predictor.\nAdd higher order term(s), as needed."
  },
  {
    "objectID": "notes/lec12.html#constant-variance",
    "href": "notes/lec12.html#constant-variance",
    "title": "Checking assumptions",
    "section": "Constant variance",
    "text": "Constant variance\n\nLook at plot of residuals versus fitted (predicted) values.\nConstant variance is satisfied if the vertical spread of the points is approximately equal for all fitted values\n\n. . .\n\n\n\n\n\n\n\n\n\n. . .\n\nConstant variance is satisfied"
  },
  {
    "objectID": "notes/lec12.html#example-constant-variance-not-satisfied",
    "href": "notes/lec12.html#example-constant-variance-not-satisfied",
    "title": "Checking assumptions",
    "section": "Example: Constant variance not satisfied",
    "text": "Example: Constant variance not satisfied\n\n\n\n\n\n\n\n\n\n. . .\n\nConstant variance is critical for reliable inference\nAddress violations by applying transformation on the response"
  },
  {
    "objectID": "notes/lec12.html#normality",
    "href": "notes/lec12.html#normality",
    "title": "Checking assumptions",
    "section": "Normality",
    "text": "Normality\n\nLook at the distribution of the residuals\nNormality is satisfied if the distribution is approximately unimodal and symmetric. Inference robust to violations if \\(n &gt; 30\\)\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n. . .\n\nDistribution approximately unimodal and symmetric, aside from the outlier. There are 62 observations, so inference robust to departures."
  },
  {
    "objectID": "notes/lec12.html#independence",
    "href": "notes/lec12.html#independence",
    "title": "Checking assumptions",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf data has spatial element, plot residuals on a map to examine potential spatial correlation.\n\n. . .\n\nThe independence condition is satisfied. The lemurs could reasonably be treated as independent."
  },
  {
    "objectID": "notes/lec12.html#model-diagnostics-1",
    "href": "notes/lec12.html#model-diagnostics-1",
    "title": "Checking assumptions",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nlemurs_aug &lt;- augment(lemurs_fit)\n\nlemurs_aug |&gt; slice(1:10)\n\n# A tibble: 10 √ó 8\n   weight_g age_at_wt_mo .fitted .resid   .hat .sigma  .cooksd .std.resid\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1     3400         32.0   3557. -157.  0.0302   494. 0.00164      -0.324\n 2     3620         33.0   4063. -443.  0.0399   491. 0.0176       -0.922\n 3     3720         32.4   3800.  -80.0 0.0163   495. 0.000224     -0.164\n 4     4440         32.6   3850.  590.  0.0177   489. 0.0132        1.21 \n 5     3770         31.8   3457.  313.  0.0458   493. 0.0102        0.652\n 6     3920         31.9   3522.  398.  0.0350   492. 0.0124        0.826\n 7     4520         32.8   3979.  541.  0.0279   490. 0.0180        1.12 \n 8     3700         33.2   4177. -477.  0.0626   491. 0.0337       -1.01 \n 9     3690         31.9   3537.  153.  0.0329   494. 0.00172       0.318\n10     3790         32.8   3949. -159.  0.0247   494. 0.00136      -0.328"
  },
  {
    "objectID": "notes/lec12.html#model-diagnostics-in-r",
    "href": "notes/lec12.html#model-diagnostics-in-r",
    "title": "Checking assumptions",
    "section": "Model diagnostics in R",
    "text": "Model diagnostics in R\nUse the augment() function in the broom package to output the model diagnostics (along with the predicted values and residuals)\n\nresponse and predictor variables in the model\n.fitted: predicted values\n.se.fit: standard errors of predicted values\n.resid: residuals\n.hat: leverage\n.sigma: estimate of residual standard deviation when the corresponding observation is dropped from model\n.cooksd: Cook‚Äôs distance\n.std.resid: standardized residuals"
  },
  {
    "objectID": "notes/lec12.html#influential-point",
    "href": "notes/lec12.html#influential-point",
    "title": "Checking assumptions",
    "section": "Influential Point",
    "text": "Influential Point\nAn observation is influential if removing has a noticeable impact on the regression coefficients\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "notes/lec12.html#influential-points",
    "href": "notes/lec12.html#influential-points",
    "title": "Checking assumptions",
    "section": "Influential points",
    "text": "Influential points\n\n\nInfluential points have a noticeable impact on the coefficients and standard errors used for inference\nThese points can sometimes be identified in a scatterplot if there is only one predictor variable\n\nThis is often not the case when there are multiple predictors\n\nWe will use measures to quantify an individual observation‚Äôs influence on the regression model\n\nleverage, standardized & studentized residuals, and Cook‚Äôs distance"
  },
  {
    "objectID": "notes/lec12.html#motivating-cooks-distance",
    "href": "notes/lec12.html#motivating-cooks-distance",
    "title": "Checking assumptions",
    "section": "Motivating Cook‚Äôs Distance",
    "text": "Motivating Cook‚Äôs Distance\n\nAn observation‚Äôs influence on the regression line depends on\n\nHow close it lies to the general trend of the data\nIts leverage\n\nCook‚Äôs Distance is a statistic that includes both of these components to measure an observation‚Äôs overall impact on the model"
  },
  {
    "objectID": "notes/lec12.html#cooks-distance-1",
    "href": "notes/lec12.html#cooks-distance-1",
    "title": "Checking assumptions",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance for the \\(i^{th}\\) observation is\n\\[\nD_i = \\frac{r^2_i}{p + 1}\\Big(\\frac{h_{ii}}{1 - h_{ii}}\\Big)\n\\]"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html",
    "href": "notes/lec12-check-assumptions.html",
    "title": "Checking assumptions",
    "section": "",
    "text": "View libraries and data sets used in these notes\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\nlemurs &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/lemurs-pcoq.csv\") |&gt;\n  filter(age_at_wt_mo &lt; 34)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#computing-set-up",
    "href": "notes/lec12-check-assumptions.html#computing-set-up",
    "title": "Checking assumptions",
    "section": "",
    "text": "Code\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)   \nlibrary(viridis)\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#topics",
    "href": "notes/lec12-check-assumptions.html#topics",
    "title": "Checking assumptions",
    "section": "Topics",
    "text": "Topics\n\nModel conditions\nInfluential points\nModel diagnostics\n\nLeverage\nStudentized residuals\nCook‚Äôs Distance"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#data-duke-lemurs",
    "href": "notes/lec12-check-assumptions.html#data-duke-lemurs",
    "title": "Checking assumptions",
    "section": "Data: Duke lemurs",
    "text": "Data: Duke lemurs\nToday‚Äôs data contains a subset of the original Duke Lemur data set available in the TidyTuesday GitHub repo. This data includes information on ‚Äúyoung adult‚Äù lemurs from the Coquerel‚Äôs sifaka species (PCOQ), the largest species at the Duke Lemur Center. The analysis will focus on the following variables:\n\nage_at_wt_mo: Age in months: Age of the animal when the weight was taken, in months (((Weight_Date-DOB)/365)*12)\nweight_g: Weight: Animal weight, in grams. Weights under 500g generally to nearest 0.1-1g; Weights &gt;500g generally to the nearest 1-20g.\n\nThe goal of the analysis is to use the age of the lemurs to understand variability in the weight."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#eda",
    "href": "notes/lec12-check-assumptions.html#eda",
    "title": "Checking assumptions",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#eda-1",
    "href": "notes/lec12-check-assumptions.html#eda-1",
    "title": "Checking assumptions",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#fit-model",
    "href": "notes/lec12-check-assumptions.html#fit-model",
    "title": "Checking assumptions",
    "section": "Fit model",
    "text": "Fit model\n\nlemurs_fit &lt;- lm(weight_g ~ age_at_wt_mo, data = lemurs)\n\ntidy(lemurs_fit) |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-12314.360\n4252.696\n-2.896\n0.005\n\n\nage_at_wt_mo\n496.591\n131.225\n3.784\n0.000"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#assumptions-for-regression",
    "href": "notes/lec12-check-assumptions.html#assumptions-for-regression",
    "title": "Checking assumptions",
    "section": "Assumptions for regression",
    "text": "Assumptions for regression\n\\[\n\\mathbf{y}|\\mathbf{X} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma_\\epsilon^2\\mathbf{I})\n\\]\n\nLinearity: \\(E[\\varepsilon|X] = 0\\), i.e.¬†\\(E[Y|X] = X\\beta\\). There is a linear relationship between the response and predictor variables.\nConstant variance and independence: \\(Var[\\varepsilon] = I\\sigma^2\\). The variability about the least squares line is generally constant and the residuals are independent.\nNormality: \\(\\varepsilon_i \\sim Normal\\). The distribution of the errors (residuals) is approximately normal.\n\n\nHow do we know if these assumptions hold in our data?"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#linearity",
    "href": "notes/lec12-check-assumptions.html#linearity",
    "title": "Checking assumptions",
    "section": "Linearity",
    "text": "Linearity\n\nLook at plot of residuals versus fitted (predicted) values.\nLinearity is satisfied if there is no discernible pattern in the plot (i.e., points randomly scattered around \\(residuals = 0\\)\n\n\n\n\n\n\n\n\n\n\n\nLinearity is satisfied"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#example-linearity-not-satisfied",
    "href": "notes/lec12-check-assumptions.html#example-linearity-not-satisfied",
    "title": "Checking assumptions",
    "section": "Example: Linearity not satisfied",
    "text": "Example: Linearity not satisfied\n\n\n\n\n\n\n\n\n\n\nIf linearity is not satisfied, examine the plots of residuals versus each predictor.\nAdd higher order term(s), as needed."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#constant-variance",
    "href": "notes/lec12-check-assumptions.html#constant-variance",
    "title": "Checking assumptions",
    "section": "Constant variance",
    "text": "Constant variance\n\nLook at plot of residuals versus fitted (predicted) values.\nConstant variance is satisfied if the vertical spread of the points is approximately equal for all fitted values\n\n\n\n\n\n\n\n\n\n\n\nConstant variance is satisfied"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#example-constant-variance-not-satisfied",
    "href": "notes/lec12-check-assumptions.html#example-constant-variance-not-satisfied",
    "title": "Checking assumptions",
    "section": "Example: Constant variance not satisfied",
    "text": "Example: Constant variance not satisfied\n\n\n\n\n\n\n\n\n\n\nConstant variance is critical for reliable inference\nAddress violations by applying transformation on the response"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#normality",
    "href": "notes/lec12-check-assumptions.html#normality",
    "title": "Checking assumptions",
    "section": "Normality",
    "text": "Normality\n\nLook at the distribution of the residuals\nNormality is satisfied if the distribution is approximately unimodal and symmetric. Inference robust to violations if \\(n &gt; 30\\)\n\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nDistribution approximately unimodal and symmetric, aside from the outlier. There are 62 observations, so inference robust to departures."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#independence",
    "href": "notes/lec12-check-assumptions.html#independence",
    "title": "Checking assumptions",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf data has spatial element, plot residuals on a map to examine potential spatial correlation.\n\n\nThe independence condition is satisfied. The lemurs could reasonably be treated as independent."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#model-diagnostics-1",
    "href": "notes/lec12-check-assumptions.html#model-diagnostics-1",
    "title": "Checking assumptions",
    "section": "Model diagnostics",
    "text": "Model diagnostics\n\nlemurs_aug &lt;- augment(lemurs_fit)\n\nlemurs_aug |&gt; slice(1:10)\n\n# A tibble: 10 √ó 8\n   weight_g age_at_wt_mo .fitted .resid   .hat .sigma  .cooksd .std.resid\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1     3400         32.0   3557. -157.  0.0302   494. 0.00164      -0.324\n 2     3620         33.0   4063. -443.  0.0399   491. 0.0176       -0.922\n 3     3720         32.4   3800.  -80.0 0.0163   495. 0.000224     -0.164\n 4     4440         32.6   3850.  590.  0.0177   489. 0.0132        1.21 \n 5     3770         31.8   3457.  313.  0.0458   493. 0.0102        0.652\n 6     3920         31.9   3522.  398.  0.0350   492. 0.0124        0.826\n 7     4520         32.8   3979.  541.  0.0279   490. 0.0180        1.12 \n 8     3700         33.2   4177. -477.  0.0626   491. 0.0337       -1.01 \n 9     3690         31.9   3537.  153.  0.0329   494. 0.00172       0.318\n10     3790         32.8   3949. -159.  0.0247   494. 0.00136      -0.328"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#model-diagnostics-in-r",
    "href": "notes/lec12-check-assumptions.html#model-diagnostics-in-r",
    "title": "Checking assumptions",
    "section": "Model diagnostics in R",
    "text": "Model diagnostics in R\nUse the augment() function in the broom package to output the model diagnostics (along with the predicted values and residuals)\n\nresponse and predictor variables in the model\n.fitted: predicted values\n.se.fit: standard errors of predicted values\n.resid: residuals\n.hat: leverage\n.sigma: estimate of residual standard deviation when the corresponding observation is dropped from model\n.cooksd: Cook‚Äôs distance\n.std.resid: standardized residuals"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#influential-point",
    "href": "notes/lec12-check-assumptions.html#influential-point",
    "title": "Checking assumptions",
    "section": "Influential Point",
    "text": "Influential Point\nAn observation is influential if removing has a noticeable impact on the regression coefficients"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#influential-points",
    "href": "notes/lec12-check-assumptions.html#influential-points",
    "title": "Checking assumptions",
    "section": "Influential points",
    "text": "Influential points\n\n\nInfluential points have a noticeable impact on the coefficients and standard errors used for inference\nThese points can sometimes be identified in a scatterplot if there is only one predictor variable\n\nThis is often not the case when there are multiple predictors\n\nWe will use measures to quantify an individual observation‚Äôs influence on the regression model\n\nleverage, standardized & studentized residuals, and Cook‚Äôs distance"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#motivating-cooks-distance",
    "href": "notes/lec12-check-assumptions.html#motivating-cooks-distance",
    "title": "Checking assumptions",
    "section": "Motivating Cook‚Äôs Distance",
    "text": "Motivating Cook‚Äôs Distance\n\nAn observation‚Äôs influence on the regression line depends on\n\nHow close it lies to the general trend of the data\nIts leverage\n\nCook‚Äôs Distance is a statistic that includes both of these components to measure an observation‚Äôs overall impact on the model"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#cooks-distance-1",
    "href": "notes/lec12-check-assumptions.html#cooks-distance-1",
    "title": "Checking assumptions",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs distance for the \\(i^{th}\\) observation is\n\\[\nD_i = \\frac{r^2_i}{p}\\Big(\\frac{h_{ii}}{1 - h_{ii}}\\Big)\n\\]\nwhere \\(r_i\\) is the studentized residual and \\(h_{ii}\\) is the leverage for the \\(i^{th}\\) observation\nThis measure is a combination of\n\nHow well the model fits the \\(i^{th}\\) observation (magnitude of residuals)\nHow far the \\(i^{th}\\) observation is from the rest of the data (where the point is in the \\(x\\) space)"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#using-cooks-distance",
    "href": "notes/lec12-check-assumptions.html#using-cooks-distance",
    "title": "Checking assumptions",
    "section": "Using Cook‚Äôs Distance",
    "text": "Using Cook‚Äôs Distance\n\nAn observation with large value of \\(D_i\\) is said to have a strong influence on the predicted values\nGeneral thresholds .An observation with\n\n\\(D_i &gt; 0.5\\) is moderately influential\n\\(D_i &gt; 1\\) is very influential"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#cooks-distance-2",
    "href": "notes/lec12-check-assumptions.html#cooks-distance-2",
    "title": "Checking assumptions",
    "section": "Cook‚Äôs Distance",
    "text": "Cook‚Äôs Distance\nCook‚Äôs Distance is in the column .cooksd in the output from the augment() function"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#comparing-models",
    "href": "notes/lec12-check-assumptions.html#comparing-models",
    "title": "Checking assumptions",
    "section": "Comparing models",
    "text": "Comparing models\nWith influential point\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-12314.360\n4252.696\n-2.896\n0.005\n\n\nage_at_wt_mo\n496.591\n131.225\n3.784\n0.000\n\n\n\n\n\n\nWithout influential point\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-6670.958\n3495.136\n-1.909\n0.061\n\n\nage_at_wt_mo\n321.209\n107.904\n2.977\n0.004\n\n\n\n\n\n\nLet‚Äôs better understand the influential point."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#leverage-1",
    "href": "notes/lec12-check-assumptions.html#leverage-1",
    "title": "Checking assumptions",
    "section": "Leverage",
    "text": "Leverage\n\n\nRecall the hat matrix \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\)\nWe focus on the diagonal elements\n\\[\nh_{ii} = \\mathbf{x}_i^\\mathsf{T}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{x}_i\n\\]such that \\(\\mathbf{x}^\\mathsf{T}_i\\) is the \\(i^{th}\\) row of \\(\\mathbf{X}\\)\n\\(h_{ii}\\) is the leverage: a measure of the distance of the \\(i^{th}\\) observation from the center (or centroid) of the \\(x\\) space\nObservations with large values of \\(h_{ii}\\) are far away from the typical value (or combination of values) of the predictors in the data"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#large-leverage",
    "href": "notes/lec12-check-assumptions.html#large-leverage",
    "title": "Checking assumptions",
    "section": "Large leverage",
    "text": "Large leverage\n\n\nThe sum of the leverages for all points is \\(p\\), where \\(p\\) is the number of predictors (+ intercept) in the model . More specifically\n\\[\n\\sum_{i=1}^n h_{ii} = \\text{rank}(\\mathbf{H}) = \\text{rank}(\\mathbf{X}) = p\n\\]\nThe average value of leverage, \\(h_{ii}\\), is \\(\\bar{h} =  \\frac{(p)}{n}\\)\nAn observation has large leverage if \\[h_{ii} &gt; \\frac{2(p)}{n}\\]"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#lemurs-leverage",
    "href": "notes/lec12-check-assumptions.html#lemurs-leverage",
    "title": "Checking assumptions",
    "section": "Lemurs: Leverage",
    "text": "Lemurs: Leverage\n\nh_threshold &lt;- 2 * 2 / nrow(lemurs)\nh_threshold\n\n[1] 0.06451613\n\n\n\nlemurs_aug |&gt;\n  filter(.hat &gt; h_threshold)\n\n# A tibble: 2 √ó 8\n  weight_g age_at_wt_mo .fitted .resid   .hat .sigma .cooksd .std.resid\n     &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n1     4040         33.5   4336.  -296. 0.107    493.  0.0244     -0.639\n2     6519         33.4   4272.  2247. 0.0871   389.  1.10        4.79 \n\n\n\n\nWhy do you think these points have large leverage?"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#lets-look-at-the-data",
    "href": "notes/lec12-check-assumptions.html#lets-look-at-the-data",
    "title": "Checking assumptions",
    "section": "Let‚Äôs look at the data",
    "text": "Let‚Äôs look at the data"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#large-leverage-1",
    "href": "notes/lec12-check-assumptions.html#large-leverage-1",
    "title": "Checking assumptions",
    "section": "Large leverage",
    "text": "Large leverage\nIf there is point with high leverage, ask\n\nIs there a data entry error?\nIs this observation within the scope of individuals for which you want to make predictions and draw conclusions?\nIs this observation impacting the estimates of the model coefficients? (Need more information!)\n\nJust because a point has high leverage does not necessarily mean it will have a substantial impact on the regression. Therefore we need to check other measures."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#scaled-residuals-1",
    "href": "notes/lec12-check-assumptions.html#scaled-residuals-1",
    "title": "Checking assumptions",
    "section": "Scaled residuals",
    "text": "Scaled residuals\n\n\nWhat is the best way to identify outlier points that don‚Äôt fit the pattern from the regression line?\n\nLook for points that have large residuals\n\nWe can rescale residuals and put them on a common scale to more easily identify ‚Äúlarge‚Äù residuals\nWe will consider two types of scaled residuals: standardized residuals and studentized residuals"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#standardized-residuals",
    "href": "notes/lec12-check-assumptions.html#standardized-residuals",
    "title": "Checking assumptions",
    "section": "Standardized residuals",
    "text": "Standardized residuals\n\n\nThe variance of the residuals can be estimated by the mean squared residuals (MSR) \\(= \\frac{SSR}{n - p} = \\hat{\\sigma}^2_{\\varepsilon}\\)\nWe can use MSR to compute standardized residuals\n\\[\nstd.res_i = \\frac{\\hat{\\varepsilon}_i}{\\sqrt{MSR}}\n\\]\nStandardized residuals are produced by augment() in the column .std.resid"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#using-standardized-residuals",
    "href": "notes/lec12-check-assumptions.html#using-standardized-residuals",
    "title": "Checking assumptions",
    "section": "Using standardized residuals",
    "text": "Using standardized residuals\nWe can examine the standardized residuals directly from the output from the augment() function\n\n\n\n\n\n\n\n\n\n\nAn observation is a potential outlier if its standardized residual is beyond \\(\\pm 3\\)"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#digging-in-to-the-data",
    "href": "notes/lec12-check-assumptions.html#digging-in-to-the-data",
    "title": "Checking assumptions",
    "section": "Digging in to the data",
    "text": "Digging in to the data\nLet‚Äôs look at the value of the response variable to better understand potential outliers"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#studentized-residuals",
    "href": "notes/lec12-check-assumptions.html#studentized-residuals",
    "title": "Checking assumptions",
    "section": "Studentized residuals",
    "text": "Studentized residuals\n\n\nMSR is an approximation of the variance of the residuals.\nThe variance of the residuals is \\(Var(\\mathbf{e}) = \\sigma^2_{\\varepsilon}(\\mathbf{I} - \\mathbf{H})\\)\n\nThe variance of the \\(i^{th}\\) residual is \\(Var(e_i) = \\sigma^2_{\\varepsilon}(1 - h_{ii})\\)\n\nThe studentized residual is the residual rescaled by the more exact calculation for variance\n\n\n\\[\nr_i = \\frac{\\hat{\\varepsilon}_{i}}{\\sqrt{\\hat{\\sigma}^2_{\\varepsilon}(1 - h_{ii})}}\n\\]\n\nStandardized and studentized residuals provide similar information about which points are outliers in the response.\n\nStudentized residuals are used to compute Cook‚Äôs Distance."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#using-these-measures",
    "href": "notes/lec12-check-assumptions.html#using-these-measures",
    "title": "Checking assumptions",
    "section": "Using these measures",
    "text": "Using these measures\n\nStandardized residuals, leverage, and Cook‚Äôs Distance should all be examined together\nExamine plots of the measures to identify observations that are outliers, high leverage, and may potentially impact the model."
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#back-to-the-influential-point",
    "href": "notes/lec12-check-assumptions.html#back-to-the-influential-point",
    "title": "Checking assumptions",
    "section": "Back to the influential point",
    "text": "Back to the influential point"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#what-to-do-with-outliersinfluential-points",
    "href": "notes/lec12-check-assumptions.html#what-to-do-with-outliersinfluential-points",
    "title": "Checking assumptions",
    "section": "What to do with outliers/influential points?",
    "text": "What to do with outliers/influential points?\n\n\nFirst consider if the outlier is a result of a data entry error.\nIf not, you may consider dropping an observation if it‚Äôs an outlier in the predictor variables if‚Ä¶\n\nIt is meaningful to drop the observation given the context of the problem\nYou intended to build a model on a smaller range of the predictor variables. Mention this in the write up of the results and be careful to avoid extrapolation when making predictions"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#what-to-do-with-outliersinfluential-points-1",
    "href": "notes/lec12-check-assumptions.html#what-to-do-with-outliersinfluential-points-1",
    "title": "Checking assumptions",
    "section": "What to do with outliers/influential points?",
    "text": "What to do with outliers/influential points?\n\n\nIt is generally not good practice to drop observations that are outliers in the value of the response variable\n\nThese are legitimate observations and should be in the model\nYou can try transformations or increasing the sample size by collecting more data\n\nA general strategy when there are influential points is to fit the model with and without the influential points and compare the outcomes"
  },
  {
    "objectID": "notes/lec12-check-assumptions.html#recap",
    "href": "notes/lec12-check-assumptions.html#recap",
    "title": "Checking assumptions",
    "section": "Recap",
    "text": "Recap\n\nModel conditions\nInfluential points\nModel diagnostics\n\nLeverage\nStudentized residuals\nCook‚Äôs Distance"
  },
  {
    "objectID": "notes/lec13.html",
    "href": "notes/lec13.html",
    "title": "Multicollinearity",
    "section": "",
    "text": "View libraries and data sets used in these notes\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \nlibrary(patchwork)\nlibrary(GGally)   # for pairwise plot matrix\nlibrary(corrplot) # for correlation matrix\n\n# load data\nrail_trail &lt;- read_csv(\"https://sta221-fa25.github.io/data/rail_trail.csv\")\n\n# set default theme in ggplot2\nggplot2::theme_set(ggplot2::theme_bw())"
  },
  {
    "objectID": "notes/lec13.html#topics",
    "href": "notes/lec13.html#topics",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\nMulticollinearity\n\nDefinition\nHow it impacts the model\nHow to detect it\nWhat to do about it"
  },
  {
    "objectID": "notes/lec13.html#data-trail-users",
    "href": "notes/lec13.html#data-trail-users",
    "title": "Multicollinearity",
    "section": "Data: Trail users",
    "text": "Data: Trail users\n\nThe Pioneer Valley Planning Commission (PVPC) collected data at the beginning a trail in Florence, MA for ninety days from April 5, 2005 to November 15, 2005 to\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "notes/lec13.html#variables",
    "href": "notes/lec13.html#variables",
    "title": "Multicollinearity",
    "section": "Variables",
    "text": "Variables\nOutcome:\n\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of ‚ÄúFall‚Äù, ‚ÄúSpring‚Äù, or ‚ÄúSummer‚Äù\nprecip measure of precipitation (in inches)"
  },
  {
    "objectID": "notes/lec13.html#eda-relationship-between-predictors",
    "href": "notes/lec13.html#eda-relationship-between-predictors",
    "title": "Multicollinearity",
    "section": "EDA: Relationship between predictors",
    "text": "EDA: Relationship between predictors\nWe can create a pairwise plot matrix using the ggpairs function from the GGally R package\n\nrail_trail |&gt;\n  select(hightemp, avgtemp, season, precip) |&gt;\n  ggpairs()"
  },
  {
    "objectID": "notes/lec13.html#eda-relationship-between-predictors-1",
    "href": "notes/lec13.html#eda-relationship-between-predictors-1",
    "title": "Multicollinearity",
    "section": "EDA: Relationship between predictors",
    "text": "EDA: Relationship between predictors"
  },
  {
    "objectID": "notes/lec13.html#eda-correlation-matrix",
    "href": "notes/lec13.html#eda-correlation-matrix",
    "title": "Multicollinearity",
    "section": "EDA: Correlation matrix",
    "text": "EDA: Correlation matrix\nWe can. use corrplot() in the corrplot R package to make a matrix of pairwise correlations between quantitative predictors\n\ncorrelations &lt;- rail_trail |&gt;\n  select(hightemp, avgtemp, precip) |&gt;\n  cor()\n\ncorrplot(correlations, method = \"number\")"
  },
  {
    "objectID": "notes/lec13.html#eda-correlation-matrix-1",
    "href": "notes/lec13.html#eda-correlation-matrix-1",
    "title": "Multicollinearity",
    "section": "EDA: Correlation matrix",
    "text": "EDA: Correlation matrix\n\n\n\n\n\n\n\n\n\n\nWhat might be a potential concern with a model that uses high temperature, average temperature, season, and precipitation to predict volume?"
  },
  {
    "objectID": "notes/lec13.html#multicollinearity-1",
    "href": "notes/lec13.html#multicollinearity-1",
    "title": "Multicollinearity",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\n\nIdeally the predictors are orthogonal, meaning they are completely independent of one another\nIn practice, there is typically some dependence between predictors but it is often not a major issue in the model\nIf there is linear dependence among (a subset of) the predictors, we cannot find estimate \\(\\hat{\\boldsymbol{\\beta}}\\)\nIf there are near-linear dependencies, we can find \\(\\hat{\\boldsymbol{\\beta}}\\) but there may be other issues with the model\nMulticollinearity: near-linear dependence among predictors"
  },
  {
    "objectID": "notes/lec13.html#sources-of-multicollinearity",
    "href": "notes/lec13.html#sources-of-multicollinearity",
    "title": "Multicollinearity",
    "section": "Sources of multicollinearity",
    "text": "Sources of multicollinearity\n\n\nData collection method - only sample from a subspace of the region of predictors\nConstraints in the population - e.g., predictors family income and size of house\nChoice of model - e.g., adding high order terms to the model\nOverdefined model - have more predictors than observations"
  },
  {
    "objectID": "notes/lec13.html#detecting-multicollinearity",
    "href": "notes/lec13.html#detecting-multicollinearity",
    "title": "Multicollinearity",
    "section": "Detecting multicollinearity",
    "text": "Detecting multicollinearity\n\nExercise\n\n\nHow could you detect multicollinearity?"
  },
  {
    "objectID": "notes/lec13.html#variance-inflation-factor",
    "href": "notes/lec13.html#variance-inflation-factor",
    "title": "Multicollinearity",
    "section": "Variance inflation factor",
    "text": "Variance inflation factor\n\nThe variance inflation factor (VIF) measures how much the linear dependencies impact the variance of the predictors\n\n\\[\nVIF_{j} = \\frac{1}{1 - R^2_j}\n\\]\nwhere \\(R^2_j\\) is the proportion of variation in \\(x_j\\) that is explained by a linear combination of all the other predictors\n\nWhen the response and predictors are scaled in a particular way, \\(C_{jj} = VIF_{j}\\). Click here to see how."
  },
  {
    "objectID": "notes/lec13.html#detecting-multicollinearity-1",
    "href": "notes/lec13.html#detecting-multicollinearity-1",
    "title": "Multicollinearity",
    "section": "Detecting multicollinearity",
    "text": "Detecting multicollinearity\n\nCommon practice uses threshold \\(VIF &gt; 10\\) as indication of concerning multicollinearity (some say VIF &gt; 5 is worth investigation)\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF\n\n\nlibrary(rms)\n\ntrail_fit &lt;- lm(volume ~ hightemp + avgtemp + precip, data = rail_trail)\n\nvif(trail_fit)\n\nhightemp  avgtemp   precip \n7.161882 7.597154 1.193431 \n\n\n\nExercise\n\n\nLoad the data using the code below:\n\nrail_trail &lt;- read_csv(\"https://sta221-fa25.github.io/data/rail_trail.csv\")\n\nand compute VIF manually (without using the function vif())."
  },
  {
    "objectID": "notes/lec13.html#how-multicollinearity-impacts-model",
    "href": "notes/lec13.html#how-multicollinearity-impacts-model",
    "title": "Multicollinearity",
    "section": "How multicollinearity impacts model",
    "text": "How multicollinearity impacts model\n\n\nLarge variance for the model coefficients that are collinear\n\nDifferent combinations of coefficient estimates produce equally good model fits\n\nUnreliable statistical inference results\n\nMay conclude coefficients are not statistically significant when there is, in fact, a relationship between the predictors and response\n\nInterpretation of coefficient is no longer ‚Äúholding all other variables constant‚Äù, since this would be impossible for correlated predictors"
  },
  {
    "objectID": "notes/lec13.html#dealing-with-multicollinearity",
    "href": "notes/lec13.html#dealing-with-multicollinearity",
    "title": "Multicollinearity",
    "section": "Dealing with multicollinearity",
    "text": "Dealing with multicollinearity\n\n\nCollect more data (often not feasible given practical constraints)\nRedefine the correlated predictors to keep the information from predictors but eliminate collinearity\n\ne.g., if \\(x_1, x_2, x_3\\) are correlated, use a new variable \\((x_1 + x_2) / x_3\\) in the model\n\nFor categorical predictors, avoid using levels with very few observations as the baseline\nRemove one of the correlated variables\n\nBe careful about substantially reducing predictive power of the model"
  },
  {
    "objectID": "notes/lec13.html#recap",
    "href": "notes/lec13.html#recap",
    "title": "Multicollinearity",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multicollinearity\n\nDefinition\nHow it impacts the model\nHow to detect it\nWhat to do about it\n\n\nEnd of class notes\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nrail_trail &lt;- read_csv(\"https://sta221-fa25.github.io/data/rail_trail.csv\")\n\nmodel1 &lt;- lm(hightemp ~ avgtemp + precip, data = rail_trail)\n1 / (1 - glance(model1)$r.squared)\n\nset.seed(221)\nrt_split = initial_split(rail_trail, prop = 3/4)\nrt_train = training(rt_split)\nrt_test = testing(rt_split)\n\nmodel_full &lt;- lm(volume ~ hightemp + avgtemp + precip, data = rt_train)\nmodel_minus_temp &lt;- lm(volume ~ avgtemp + precip, data = rt_train)\n\nresidual_full &lt;- rt_test$volume - predict(model_full, newdata = rt_test)\nresidual_minus_temp &lt;- rt_test$volume -\n  predict(model_minus_temp, newdata = rt_test)\n\n## Compare root mean squared error (RMSE)\nsqrt(sum(residual_full^2))\nsqrt(sum(residual_minus_temp^2))\n\n## Examine coefficients for each model\nmodel_full\nmodel_minus_temp"
  },
  {
    "objectID": "notes/lec13.html#relationship-with-variance",
    "href": "notes/lec13.html#relationship-with-variance",
    "title": "Multicollinearity",
    "section": "Relationship with variance",
    "text": "Relationship with variance\nRecall that \\(Var(\\hat{\\beta}) = \\sigma^2 (X^TX)^{-1}\\). If we call the matrix \\((X^TX)^{-1}\\) ‚ÄúC‚Äù then the variance of \\(\\hat{\\beta}_j\\) is \\(\\sigma^2 C_{jj}\\). Specifically,\n\\[\nC_{jj} = VIF \\times \\frac{1}{\\sum (X_{ij} - \\bar{X_j})^2}\n\\]"
  },
  {
    "objectID": "notes/variance-inflation-factors.html",
    "href": "notes/variance-inflation-factors.html",
    "title": "Variance Inflation Factors",
    "section": "",
    "text": "These notes come from Prof.¬†Maria Tackett‚Äôs course notes for STA221.\n# load packages\nlibrary(tidyverse)  \nlibrary(tidymodels)  \nlibrary(knitr)       \n\nrail_trail &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/rail_trail.csv\")\nHere we explain the connection between the Variance Inflation Factor (VIF) and \\(\\mathbf{C} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\). This explanation is motivated by Chapter 3 of Montgomery, Peck, and Vining (2021)."
  },
  {
    "objectID": "notes/variance-inflation-factors.html#unit-length-scaling",
    "href": "notes/variance-inflation-factors.html#unit-length-scaling",
    "title": "Variance Inflation Factors",
    "section": "Unit length scaling",
    "text": "Unit length scaling\nWe have talked about standardizing predictors, such that\n\\[x_{ij_{std}} = \\frac{x_{ij} - \\bar{x_j}}{s_{x_{j}}}\\] such that \\(\\bar{x}_j\\) is the mean and \\(s_{x_{j}}\\) is the standard deviation of the predictor \\(x_j\\).\nThe standardized predictors have a mean of 0 and variance of 1.\nAnother common type of scaling is unit length scaling. We will denote these scaled predictors as \\(w_j\\). We apply this scaling on both the predictor and response variable in the following way.\n\\[w_{ij} = \\frac{x_{ij} - \\bar{x_j}}{\\sqrt{s_{jj}}}\\] where \\(s_{jj} = \\sum_{i = 1}^{n}(x_{ij} - \\bar{x}_j)^2\\)\nThe scaled response variable, denoted \\(y^0\\) is\n\\[y_i^0 = \\frac{y_i - \\bar{y}}{\\sqrt{SST}}\\]\nwhere \\(SST\\) is the sum of squares total, \\(\\sum_{i=1}^n(y_i - \\bar{y})^2\\)\nWe will use the scaled predictor and response variable to show the relationship between \\(\\mathbf{C} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\) and the formula for VIF. More specifically, that\n\\[\nC_{jj} = VIF_{j} = \\frac{1}{1 - R^2_j}\n\\]\nWhen we use the scaled predictors and response."
  },
  {
    "objectID": "notes/variance-inflation-factors.html#variance-inflation-factor",
    "href": "notes/variance-inflation-factors.html#variance-inflation-factor",
    "title": "Variance Inflation Factors",
    "section": "Variance inflation factor",
    "text": "Variance inflation factor\nWe will use the rail_trail data from the notes to illustrate this connection. We will focus on the predictors hightemp, avgtemp, and precip.\nWe begin by creating unit length scaled versions of the variables.\n\nhightemp_norm &lt;- sum((rail_trail$hightemp - mean(rail_trail$hightemp))^2)\navgtemp_norm &lt;- sum((rail_trail$avgtemp - mean(rail_trail$avgtemp))^2)\nprecip_norm &lt;- sum((rail_trail$precip - mean(rail_trail$precip))^2)\nvolume_norm &lt;- sum((rail_trail$volume - mean(rail_trail$volume))^2)\n\nrail_trail &lt;- rail_trail |&gt;\n  mutate(hightemp_scaled = (hightemp - mean(hightemp)) / hightemp_norm^.5,\n         avgtemp_scaled = (avgtemp - mean(avgtemp)) / avgtemp_norm^.5, \n         precip_scaled = (precip - mean(precip)) / precip_norm^.5,\n         volume_scaled = (volume - mean(volume)) / volume_norm^.5\n         )\n\nThe matrix \\(\\mathbf{W}^\\mathsf{T}\\mathbf{W}\\) is equivalent to the correlation matrix for these predictors.\n\n# use -1 to remove the intercept for the correlation matrix\nW &lt;- model.matrix(volume_scaled ~ hightemp_scaled + avgtemp_scaled + precip_scaled - 1, data = rail_trail)\n\nt(W)%*%W\n\n                hightemp_scaled avgtemp_scaled precip_scaled\nhightemp_scaled       1.0000000      0.9196439     0.1343172\navgtemp_scaled        0.9196439      1.0000000     0.2725832\nprecip_scaled         0.1343172      0.2725832     1.0000000\n\n\nWhen we fit a model using the unit length scaling for the response and predictor variables, we would expect \\(Var(\\hat{\\beta}_j) / \\hat{\\sigma}^2_{\\epsilon} \\approx 1\\). As we see below, however, these values are greater than 1.\n\ntrail_model_scaled &lt;- lm(volume_scaled ~ hightemp_scaled + avgtemp_scaled + precip_scaled , data = rail_trail)\n\nbeta_se &lt;- tidy(trail_model_scaled)$std.error\nsigma &lt;- glance(trail_model_scaled)$sigma\n\nbeta_se^2 / sigma^2\n\n[1] 0.01111111 7.16188175 7.59715405 1.19343051\n\n\n(You can ignore the first element, which represents the intercept).\nThese values show how much the standard errors of the coefficients are inflated given the correlation between the predictors (the off diagonal elements of \\(\\mathbf{W}^\\mathsf{T}\\mathbf{W}\\)). The amount by which the standard errors are inflated are called the variance inflation factors (VIF).\nUnder this model using the unit-length-scaled predictors and response, we see these variance inflation factors are equal to the diagonal elements of \\(\\mathbf{C} = (\\mathbf{W}^\\mathsf{T}\\mathbf{W})^{1}\\).\n\nC &lt;- solve(t(W) %*% W)\ndiag(C)\n\nhightemp_scaled  avgtemp_scaled   precip_scaled \n       7.161882        7.597154        1.193431 \n\n\nThus,\n\\[C_{jj} = VIF_{j} = \\frac{1}{1 - R^2_j}\\]"
  },
  {
    "objectID": "project.html#exploratory-data-analysis-eda",
    "href": "project.html#exploratory-data-analysis-eda",
    "title": "Final project",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\nThe purpose of this milestone is to explore the data early and get feedback on your data and analyses. You will submit a draft of the beginning of your report that includes the introduction and exploratory data analysis, with an emphasis on the EDA. It will also help you prepare for the presentation of the exploratory data analysis results.\nBelow is a brief description of the sections to include in this step:\n\nIntroduction\nThis section includes an introduction to the project motivation, background, data, and research question.\n\n\nExploratory Data Analysis\nThis section includes the following:\n\nDescription of the data set and key variables.\nExploratory data analysis of the response variable and key predictor variables. This includes visualizations, summary statistics, and narrative. Include:\n\nUnivariate EDA of the response and key predictor variables.\nBivariate EDA of the response and key predictor variables\nPotential interaction effects\n\n\nTurn-in: Write your draft introduction and exploratory data analysis in the report.qmd file in your team‚Äôs GitHub repo. Push the qmd and rendered pdf documents to GitHub by the deadline.\nBecause this question is often asked: how long should this section of the report be? Usually, the anticipated length of the EDA section, including all graphs, tables, narrative, etc. with code, warnings, and messages suppressed is about 3-5 pages. If you exceed the limit, format your figures to be smaller."
  },
  {
    "objectID": "project.html#presentations",
    "href": "project.html#presentations",
    "title": "Final project",
    "section": "Presentations",
    "text": "Presentations\nYour team will do an in-person presentation that summarizes and showcases the work you‚Äôve done on the project thus far. Because the presentations will take place while you‚Äôre still working on the project, it will also be an opportunity to receive feedback and suggestions as well as provide feedback to other teams. The presentation will focus on introducing the subject matter and research question, showcase key results from the exploratory data analysis, and discuss primary modeling strategies and/or results. The presentation should be supported by slides that serve as a brief visual addition to the presentation. The presentation and slides will be graded for content and clarity.\nYou can create your slides with any software you like (e.g., Keynote, PowerPoint, Google Slides, etc.). You can also use Quarto to make your slides! While we won‚Äôt be covering making slides with Quarto in the class, we would be happy to help you with it in office hours. It‚Äôs no different than writing other documents with Quarto, so the learning curve will not be steep!\nThe presentation is expected to be between 5 to 6 minutes. It may not exceed 6 minutes.\nEvery team member is expected to speak in the presentation. Part of the grade will be whether every team member had a meaningful speaking role in the presentation.\nSuggested template:\nSlides\nThe slide deck should have no more than 5 content slides + 1 title slide to ensure you have enough time to discuss each slide. Here is a suggested outline as you think through the slides; you do not have to use this exact format for the 6 slides.\n\nTitle Slide\nSlide 1: Introduce the subject, motivation, and research question\nSlide 2: Introduce the data set\nSlide 3 - 4: Highlights from the EDA (be sure to include EDA for the response variable!)\nSlide 5: Initial modeling strategies / results (if applicable) / next steps and anything you‚Äôd like feedback on\n\nTurn-in: Put a PDF of the slides in a folder titled ‚Äúpresentation‚Äù in your team‚Äôs GitHub repo. Push the slides before your lab section on the presentation day."
  },
  {
    "objectID": "labs/lab05.html",
    "href": "labs/lab05.html",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "",
    "text": "Due date\n\n\n\nThis lab is due on Tuesday, October 28 at 5:00pm to Gradescope."
  },
  {
    "objectID": "labs/lab05.html#exercise-1",
    "href": "labs/lab05.html#exercise-1",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWe‚Äôll begin by rescaling the response and some of the predictor variables.\n\nRescale the response variable total_rev_menwomen and the primary predictor variable total_exp_menwomen, so that they are in terms of $100K (100 thousand dollars). Name the new variables rev100k and exp100k, respectively.\nRescale the predictor variable ef_total_count , so it is in terms of thousands of students. Name the new variable students_1k.\nBriefly explain why we might we rescale these variables instead of using them in the original units."
  },
  {
    "objectID": "labs/lab05.html#exercise-2",
    "href": "labs/lab05.html#exercise-2",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nBefore modeling, let‚Äôs do some exploratory data analysis.\n\nMake a visualization of the relationship between revenue and expenditures. Use the plot to describe the relationship between the two variables.\nHigher values of both expenditures and revenues are associated with greater variability. Transform both variables using a log transformation, to deal with the potential violation of an assumption of linear regression. Name the variables log_exp and log_rev, respectively.\nLarger values of expenditures and revenues seem to follow a slightly different trend and are associated with two sports - football and basketball. Create an indicator variable that takes value 1 if the sport is basketball or football and 0 otherwise. Name the variable bball_football."
  },
  {
    "objectID": "labs/lab05.html#exercise-3",
    "href": "labs/lab05.html#exercise-3",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nVisualize the relationship between log_rev and log_exp.\nFrom the visualization, you may notice a lot of observations form a straight diagonal line, indicating a perfect one-to-one relationship between expenses and revenues. Provide a possible interpretation of this phenomenon.\nDo you think it is reasonable to include observations displaying this exact relationship? Briefly explain.\nCreate a new data frame filtering out the observations for which expenditures and revenues are exactly equal. Call the new data frame sports_nolinear.\n\nYou will use sports_nolinear for the remainder of the assignment."
  },
  {
    "objectID": "labs/lab05.html#exercise-4",
    "href": "labs/lab05.html#exercise-4",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse sports_nolinear to fit a regression model with the log-transformed revenue as the response variable and the following predictors: log-transformed expenditures, student enrollment, institution type, sports type, participation in athletics for men, participation in athletics for women, the basketball/football indicator you created in a previous exercise, and the interaction between the log-transformed expenditures and the basketball/football indicator.\nNeatly display the model using 3 digits."
  },
  {
    "objectID": "labs/lab05.html#exercise-5",
    "href": "labs/lab05.html#exercise-5",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nConsider the regression model from the previous exercise.\n\nWhat type of institution and sport correspond to the intercept?\nYou‚Äôll notice that one coefficient has a missing value. Why is the coefficient missing? What is the technical name of this phenomenon?"
  },
  {
    "objectID": "labs/lab05.html#exercise-6",
    "href": "labs/lab05.html#exercise-6",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nFor the sake of interpretability, it is useful to have a regression model in which no coefficients are missing, and the coefficients for each sport indicator represent the baseline level for such sport. To address this issue, use the code below to fit another regression model that uses the same predictors as before, making sure to drop the unnecessary variables and the intercept (by the the -1 in the formula) to achieve this.\n\nsports_fit_2 &lt;- lm(log_rev ~ -1 + sports + students_1k + sector_name +\n             sum_partic_men + sum_partic_women + log_exp + \n             log_exp*bball_football - bball_football,\n          data = sports_nolinear)\n\n\nWhy is there only one coefficient for institution type, even after the intercept was removed?\nWhich type of institution was chosen to be the baseline in this model?"
  },
  {
    "objectID": "labs/lab05.html#exercise-7",
    "href": "labs/lab05.html#exercise-7",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nNow that we have an interpretable model, let us assess the model fit and perform diagnostics to verify whether our linear regression assumptions are reasonable for this data.\nAs a first step, provide some overall measures of model fit and comment on whether it seems to have an acceptable predictive power on the response of interest."
  },
  {
    "objectID": "labs/lab05.html#exercise-8",
    "href": "labs/lab05.html#exercise-8",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nNext, let‚Äôs take a look at the residuals for the model.\nBecause there are multiple sports at every institution, we may be concerned the residuals within an institution are correlated with each other (thus violating the independence assumption).\nDue to the large number of institutions, we will look at randomly selected subset of 20 institutions to evaluate this.\n\nTake a random sample of 20 institutions. Use set.seed(221) to make your results reproducible.\nPlot the residuals versus fitted values, faceted by institution.\nBased on the faceted plot, do the errors appear to be correlated within institutions? Briefly explain your response.\n\n\n\n\n\n\n\nTip\n\n\n\nClick here for more detail, code, and examples of faceting in ggplot2.\nYou may need to change the size of the figure so that the faceted lot is fully visible. You can do so using the options #| fig-width and #| fig-height in the code chunk."
  },
  {
    "objectID": "labs/lab05.html#exercise-9",
    "href": "labs/lab05.html#exercise-9",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nAfter an examination of all the residuals, we notice a few things:\n\nThere seems to be two groups of observations.\nInstitutions with larger fitted values correspond to lower variance in the residuals compared to the others.\nThere are institutions with lower fitted values that have large negative residuals, potentially indicating outliers and/or influential points.\n\nWe are concerned that these observations may indicate some model misspecification (i.e., the model does not accurately reflect the trends in the data). Therefore, we take a look at the residuals a different way. We plot the standardized residual versus the fitted values, color the points based Cook‚Äôs distance, and use shape to indicate whether the sport is basketball or football.\n\nDescribe what you observe from the plot and how your observations compare to the list above.\nDo you think this model is an appropriate fit for the data or is the model misspecified? Briefly explain."
  },
  {
    "objectID": "labs/lab05.html#exercise-10",
    "href": "labs/lab05.html#exercise-10",
    "title": "Lab 05: Expanding Multiple Linear Regression",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nBased on this model (regardless of your answer to the previous exercise), which variables seem to be useful in explaining the variability in the revenue from collegiate sports?\nInterpret the coefficient for one useful quantitative predictor in terms of the log-transformed revenue.\nInterpret the coefficient for one useful categorical predictor in terms of the log-transformed revenue."
  },
  {
    "objectID": "notes/lec15.html",
    "href": "notes/lec15.html",
    "title": "MLEs and Model Comparison",
    "section": "",
    "text": "View libraries and data used in these notes\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(tidymodels)\nnba_salaries &lt;- \n  read_csv(\"https://sta221-fa25.github.io/data/nba_salaries_23.csv\") %&gt;%\n  mutate(Salary = Salary / 1E6) %&gt;% # salary in millions\n  select(`Player Name`, Salary, Position, Age, GP, MP, GS, \n         FG, `FG%`, `2PA`, `2P%`, AST, `AST%`) %&gt;%\n  filter(Position %in% c(\"SG\", \"C\", \"SF\", \"PF\", \"PG\" )) %&gt;%\n  drop_na()"
  },
  {
    "objectID": "notes/lec15.html#recap",
    "href": "notes/lec15.html#recap",
    "title": "MLEs and Model Comparison",
    "section": "Recap",
    "text": "Recap\nLinear regression statistical model:\n\\[\n\\boldsymbol{y}| \\boldsymbol{X}\\sim N_n(\\boldsymbol{X}\\beta, \\sigma^2 \\boldsymbol{I})\n\\] where \\(\\boldsymbol{y}\\in \\mathbb{R}^n\\) and \\(\\boldsymbol{X}\\in \\mathbb{R}^{n \\times p}\\). Here \\(N_n\\) means ‚Äúmultivariate normal of dimension n‚Äù.\nTherefore, the likelihood (joint density of the data, viewed as a function of the parameters) is\n\\[\nL(\\beta, \\sigma^2) = (2\\pi \\sigma^2)^{-n/2} \\exp\\{ -\\frac{1}{2\\sigma^2} (\\boldsymbol{y}- \\boldsymbol{X}\\beta)^T (\\boldsymbol{y}- \\boldsymbol{X}\\beta)\\}\n\\]\n\nExerciseSolution 1Solution 2\n\n\nWhere does the likelihood expression come from? Identify which component of the likelihood is the RSS.\n\n\nWe can arrive at this likelihood using the multivariate normal density function. \\(\\boldsymbol{y}\\sim N_n(\\boldsymbol{\\mu}, \\Sigma)\\) means that its density can be written,\n\\[\nf_Y(\\boldsymbol{y}) = (2 \\pi)^{-n/2} |\\Sigma|^{-1/2} \\exp \\{ (\\boldsymbol{y}- \\boldsymbol{\\mu})^T \\Sigma^{-1} (\\boldsymbol{y}- \\boldsymbol{\\mu})\\}\n\\]\n\n\nNotice the diagonal covariance matrix in the multivariate normal representation. We could equivalently write the model as \\(y_i \\sim N(x_i^T \\beta, \\sigma^2)\\) and write the joint density as\n\\[\nf(y_1, \\ldots, y_n | x_1, \\ldots, x_n) = \\prod_{i=1}^n f_y(y_i|x_i)\n\\] where \\(f(y_i | x_i) = (2 \\pi \\sigma^2)^{-1/2} \\exp\\{ - \\frac{1}{2\\sigma^2} (y_i - x_i^T \\beta)^2\\}\\) and arrive at the same likelihood function of \\(\\beta\\) and \\(\\sigma^2\\).\n\n\n\nQuestion: Is a likelihood a density function of the parameters?\nAnswer: NO!\n\nMaximum likelihood estimates\nMaximum likelihood estimates are obtained by calculus: differentiate the likelihood function and set equal to zero.\nDoing so, we find:\n\\[\n\\begin{aligned}\n\\hat{\\beta}_{MLE} &= (\\boldsymbol{X}^T \\boldsymbol{X})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}= \\hat{\\beta}_{OLS}\\\\\n\\hat{\\sigma^2}_{MLE} &= \\frac{RSS}{n}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/lec15.html#properties-of-maximum-likelihood-estimators",
    "href": "notes/lec15.html#properties-of-maximum-likelihood-estimators",
    "title": "MLEs and Model Comparison",
    "section": "Properties of maximum likelihood estimators",
    "text": "Properties of maximum likelihood estimators\nMaximum likelihood estimators have nice properties:\n\nAsymptotic consistency\nAsymptotic efficiency\nAsymptotic normality\n\n\nAsymptotic consistency\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a sequence of estimators, \\(\\hat{\\theta}_1, \\hat{\\theta}_2, \\ldots\\), we say \\(\\hat{\\theta}_n\\) is a consistent estimator of \\(\\theta\\) if for all \\(c&gt;0\\),\n\\[\n\\lim_{n \\rightarrow \\infty} p(|\\hat{\\theta}_n - \\theta| \\geq c) = 0\n\\]\n\n\nMeaning: as sample size \\(n \\rightarrow \\infty\\), the estimator will be arbitrarily close to the parameter with high probability.\nEquivalently, an estimator \\(\\hat{\\theta_n}\\) is consistent if\n\\[\n\\begin{aligned}\n\\lim_{n \\rightarrow \\infty} \\text{var}(\\hat{\\theta}) &= 0\\\\\n\\lim_{n \\rightarrow \\infty} \\text{bias}(\\hat{\\theta}) &= 0\n\\end{aligned}\n\\]\n\n\nAsymptotic efficiency\n\n\n\n\n\n\nDefinition: efficient\n\n\n\nAn estimator is efficient if it has the smallest variance among a class of estimators.\n\n\n\nGauss Markov theorem showed \\(\\hat{\\beta}_{OLS}\\) is the most efficient among all linear unbiased estimators (see homework 2).\nMLEs are asymptotically efficient.\n\n\n\nAsymptotically normal\nAs \\(n \\rightarrow \\infty\\), maximum likelihood estimators are asymptotically normal. This means that the distribution of \\(\\hat{\\beta}_{MLE}\\) is normal when \\(n\\) is large regardless of the distribution of the underlying data."
  },
  {
    "objectID": "notes/lec15.html#topics",
    "href": "notes/lec15.html#topics",
    "title": "MLEs and Model Comparison",
    "section": "Topics",
    "text": "Topics\n\nProperties of MLEs\nModel selection via information criteria (AIC, BIC)"
  },
  {
    "objectID": "notes/lec15.html#model-selection-by-example",
    "href": "notes/lec15.html#model-selection-by-example",
    "title": "MLEs and Model Comparison",
    "section": "Model selection (by example)",
    "text": "Model selection (by example)\n\nData\nData on NBA player salaries and metrics for the 2022-2023 season. Data sourced from kaggle but originally scraped from ‚ÄúHoopshype‚Äù and ‚ÄúBasketball Reference‚Äù.\nWe‚Äôll look at a subset of the data below.\n\nglimpse(nba_salaries)\n\nRows: 456\nColumns: 13\n$ `Player Name` &lt;chr&gt; \"Stephen Curry\", \"John Wall\", \"Russell Westbrook\", \"LeBr‚Ä¶\n$ Salary        &lt;dbl&gt; 48.07001, 47.34576, 47.08018, 44.47499, 44.11984, 43.279‚Ä¶\n$ Position      &lt;chr&gt; \"PG\", \"PG\", \"PG\", \"PF\", \"PF\", \"SG\", \"SF\", \"SF\", \"PF\", \"P‚Ä¶\n$ Age           &lt;dbl&gt; 34, 32, 34, 38, 34, 29, 31, 32, 28, 32, 32, 30, 31, 29, ‚Ä¶\n$ GP            &lt;dbl&gt; 56, 34, 73, 55, 47, 50, 52, 56, 63, 58, 69, 70, 33, 56, ‚Ä¶\n$ MP            &lt;dbl&gt; 34.7, 22.2, 29.1, 35.5, 35.6, 33.5, 33.6, 34.6, 32.1, 36‚Ä¶\n$ GS            &lt;dbl&gt; 56, 3, 24, 54, 47, 50, 50, 56, 63, 58, 69, 70, 19, 54, 6‚Ä¶\n$ FG            &lt;dbl&gt; 10.0, 4.1, 5.9, 11.1, 10.3, 8.9, 8.6, 8.2, 11.2, 9.6, 7.‚Ä¶\n$ `FG%`         &lt;dbl&gt; 0.493, 0.408, 0.436, 0.500, 0.560, 0.506, 0.512, 0.457, ‚Ä¶\n$ `2PA`         &lt;dbl&gt; 8.8, 6.7, 9.7, 15.3, 13.4, 13.2, 11.9, 10.3, 17.6, 9.4, ‚Ä¶\n$ `2P%`         &lt;dbl&gt; 0.579, 0.459, 0.487, 0.580, 0.617, 0.552, 0.551, 0.521, ‚Ä¶\n$ AST           &lt;dbl&gt; 6.3, 5.2, 7.5, 6.8, 5.0, 5.4, 3.9, 5.1, 5.7, 7.3, 2.4, 1‚Ä¶\n$ `AST%`        &lt;dbl&gt; 30.0, 35.3, 38.6, 33.5, 24.5, 26.6, 19.6, 24.2, 33.2, 35‚Ä¶\n\n\n\n\nCodebook\n\nSalary: salary in millions of US dollars\nPosition: player position\nAge: age of player\nGP: total number of games played in the season\nMP: average minutes played per game\nGS: number of games the player is put in at the start of the game\nFG: average number of shots made\nFG%: percentage of shots made\n2PA: average number of 2 point shots attempted\n2P%: average number of 2 point shots made\nAST: average number of assists per game\nAST%: average percentage of teammate field goals a player assisted on while they were on the floor\n\n\nEDA\nDo salaries fluctuate by position?\n\nnba_salaries %&gt;%\n  ggplot(aes(x = Position, y = Salary)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nSimilar median, but different spread.\n\ncorrelations = nba_salaries %&gt;%\n  select(-c(1,3)) %&gt;%\n  cor()\n\ncorrplot(correlations, method = \"number\")\n\n\n\n\n\n\n\n\n\nnba_salaries = nba_salaries %&gt;%\n  mutate(isPG = Position == \"PG\")\n\nnba_salaries %&gt;%\n  ggplot(aes(x = Age, y = Salary, color = isPG)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_bw() +\n  labs(title = \"Salary by age and position\")\n\n\n\n\n\n\n\n\n\n\nFitting models\n\nnba_salaries_mp = nba_salaries[,-1]\nmodel1 = lm(Salary ~ ., data = nba_salaries_mp)\n\nmodel1 %&gt;%\n  summary()\n\n\nCall:\nlm(formula = Salary ~ ., data = nba_salaries_mp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.6096  -3.6025  -0.2372   2.7579  30.6123 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -20.088201   3.076332  -6.530 1.81e-10 ***\nPositionPF   -0.414340   1.031869  -0.402  0.68822    \nPositionPG    1.339458   1.375032   0.974  0.33053    \nPositionSF    0.742350   1.071026   0.693  0.48860    \nPositionSG   -0.372072   1.081694  -0.344  0.73103    \nAge           0.916530   0.074487  12.305  &lt; 2e-16 ***\nGP           -0.007521   0.017365  -0.433  0.66515    \nMP           -0.221064   0.107851  -2.050  0.04098 *  \nGS            0.068864   0.021235   3.243  0.00127 ** \nFG            2.526342   0.537229   4.703 3.44e-06 ***\n`FG%`        -2.385923   4.873721  -0.490  0.62470    \n`2PA`         0.365007   0.301742   1.210  0.22705    \n`2P%`        -2.583581   3.635532  -0.711  0.47768    \nAST           0.627065   0.577601   1.086  0.27823    \n`AST%`       -0.079180   0.104619  -0.757  0.44955    \nisPGTRUE            NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.326 on 441 degrees of freedom\nMultiple R-squared:  0.6599,    Adjusted R-squared:  0.6491 \nF-statistic: 61.11 on 14 and 441 DF,  p-value: &lt; 2.2e-16\n\n\n\nmodel2 = lm(Salary ~ isPG * Age + FG + `2PA` + AST, data = nba_salaries_mp)\n\nmodel2 %&gt;%\n  summary()\n\n\nCall:\nlm(formula = Salary ~ isPG * Age + FG + `2PA` + AST, data = nba_salaries_mp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.5090  -3.7531  -0.1026   3.1225  25.0844 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -22.14837    2.02466 -10.939  &lt; 2e-16 ***\nisPGTRUE     -18.08177    4.85227  -3.726 0.000219 ***\nAge            0.79272    0.07741  10.241  &lt; 2e-16 ***\nFG             2.09741    0.40881   5.131 4.31e-07 ***\n`2PA`          0.60736    0.26694   2.275 0.023362 *  \nAST            0.04536    0.27523   0.165 0.869183    \nisPGTRUE:Age   0.75687    0.18459   4.100 4.90e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.282 on 449 degrees of freedom\nMultiple R-squared:  0.6584,    Adjusted R-squared:  0.6539 \nF-statistic: 144.2 on 6 and 449 DF,  p-value: &lt; 2.2e-16\n\n\n \n\n\nComparing models\n\nWe want a fitted model that has a larger likelihood\nWe want to penalize the existence of more explanatory variables\n\nInformation criterion has the form:\n\\[\n- 2 \\log_e(L(\\hat{\\theta}_{MLE})) +  (p \\times c)\n\\] where \\(L(\\hat{\\theta}_{MLE})\\) is the log-likelihood of the observed data evaluated at the maximum likelihood estimate of the parameters, \\(p\\) is the number of predictors in the model and \\(c\\) is a constant specific to the information criterion.\n\nThe first term decreases as \\(p\\) increases\nThe second term increases as \\(p\\) increases\nLower *IC is better.\n\n\n\n\nAkaike information criterion (AIC)\n\nglance(model1)$AIC\n\n[1] 2993.091\n\nglance(model2)$AIC\n\n[1] 2979.036\n\n\n\nFor AIC, \\(c = 2\\)\n\n\n\nBayesian information criterion\n\nglance(model1)$BIC\n\n[1] 3059.051\n\nglance(model2)$BIC\n\n[1] 3012.016\n\n\n\nFor BIC, \\(c = \\log_e(n)\\), \\(n\\) is the number of observations.\nThe penalty for BIC grows with the sample size.\nThe penalty for BIC is larger than thatfor the AIC when $n $ ___.\n\nWhat about penalizing the existence of additional predictors with \\(R^2\\)?\n\n\nAdjusted \\(R^2\\)\n\nglance(model1)$adj.r.squared\n\n[1] 0.6490785\n\nglance(model2)$adj.r.squared\n\n[1] 0.6538581\n\n\n\nless generalizable (see logistic regression soon)\nless commonly used\n\n\\[\nR^2_{adj} = 1 - \\frac{SSR/(n-p-1)}{SST/(n-1)}\n\\]\nReminder: \\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\).\n\nExercise\n\n\nCompute AIC, BIC and \\(R^2_{adj}\\) ‚Äòmanually‚Äô for model1 above in R."
  },
  {
    "objectID": "notes/lec16.html",
    "href": "notes/lec16.html",
    "title": "Ridge regression",
    "section": "",
    "text": "View libraries and data used in these notes\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(tidymodels)\nlibrary(glmnet) # ridge regression\n\ntrim32 = read_csv(\"https://sta221-fa25.github.io/data/trim32.csv\")"
  },
  {
    "objectID": "notes/logistic-ridge.html",
    "href": "notes/logistic-ridge.html",
    "title": "logistic-ridge",
    "section": "",
    "text": "library(plsgenomics)\ndata(Colon)\n\n# Colon$X is 62 x 2000 matrix, Colon$Y is outcome\ndim(Colon$X)  # 62 samples, 2000 genes\nlength(Colon$Y)\n\n# Make tidy tibble\ncolon_df &lt;- as_tibble(Colon$X) %&gt;%\n  mutate(\n    sample_id = row_number(),\n    outcome   = factor(Colon$Y, labels = c(\"Normal\", \"Tumor\"))\n  ) %&gt;%\n  relocate(sample_id, outcome)\n\ncolon_df"
  },
  {
    "objectID": "notes/lec16.html#fix-the-model",
    "href": "notes/lec16.html#fix-the-model",
    "title": "Ridge regression",
    "section": "Fix the model",
    "text": "Fix the model\n\nRe-formulating the optimization problem\nIntuitively, we have too many predictors, and we want fewer. We could enforce this by shrinking some of the elements of \\(\\beta\\) towards zero. Practically, this would reduce the size of the \\(\\beta\\) vector. We can do this by modifying our objective function:\n\\[\n\\hat{\\beta}_{ridge} = \\underset{\\beta}{\\operatorname{argmin}} \\underbrace{(\\boldsymbol{y}- \\boldsymbol{X}\\beta)^T (\\boldsymbol{y}- \\boldsymbol{X}\\beta)}_{\\text{RSS}} + \\lambda \\underbrace{\\beta^T\\beta}_{\\text{penalty}}\n\\] where \\(\\lambda &gt; 0\\) is a parameter that we, as the statistician, must specify. What does this parameter do? What happens at the boundaries \\(\\lambda \\rightarrow 0\\) and \\(\\lambda \\rightarrow \\infty\\)?\n\n\nThe picture: what‚Äôs happening?\n\n\n\n\n\n\n\n\n\nFor any given \\(\\lambda\\), the set of solutions is constrained to the ball \\(||\\beta||_2^2 \\leq c^2\\), for some constant \\(c\\). Note: \\(||\\beta||_2 = \\sqrt{\\sum_{i=1}^p \\beta_i^2}\\).\n\nWhat happens if \\(c^2 \\geq ||\\hat{\\beta}_{OLS}||_2^2\\)?\nWhat is the ‚Äúlevel curve‚Äù of a function?\nWhy do the level curves look like an ellipse?\nConsider: what if predictors are on very different scales, i.e.¬†\\(x_1 &gt;&gt; x_2\\)?"
  },
  {
    "objectID": "notes/lec16.html#data",
    "href": "notes/lec16.html#data",
    "title": "Ridge regression",
    "section": "Data",
    "text": "Data\n\ndim(trim32)\n\n[1] 120 501\n\n\n\n\n\n\n\n\nThe data come from a gene expression study by Scheetz et al.¬†(2006, Genome Biology, 7:R62), which examined gene regulation in rat eye tissue.\nIn this data set, we have measurements from 120 rats, each with expression levels recorded for 500 genes. These 500 genes serve as our predictor variables. The outcome of interest is the expression level of a specific gene called TRIM32, which plays a role in retinal development. Moreover, TRIM32 is known to be linked with a genetic disorder called Bardet-Biedl Syndrome (BBS): the mutation (P130S) in Trim32 gives rise to BBS. Thus, each observation corresponds to one rat, with TRIM32 expression as a continuous response variable and 500 other gene expression levels as predictors."
  },
  {
    "objectID": "notes/lec16.html#naive-fit-which-genes-matter",
    "href": "notes/lec16.html#naive-fit-which-genes-matter",
    "title": "Ridge regression",
    "section": "Naive fit: which genes matter?",
    "text": "Naive fit: which genes matter?\n\nfitwhat went wrong?simpler example\n\n\n\nfit = lm(y ~ 0 + ., data = trim32)\nsummary(fit)\n\n\n\n\n\n\nCall:\nlm(formula = y ~ 0 + ., data = trim32)\n\nResiduals:\nALL 120 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (380 not defined because of singularities)\n               Estimate Std. Error t value Pr(&gt;|t|)\n`1367539_at`    2.46865        NaN     NaN      NaN\n`1367566_at`    1.99593        NaN     NaN      NaN\n`1368099_at`   -6.18590        NaN     NaN      NaN\n`1368114_at`   -1.56565        NaN     NaN      NaN\n`1368136_at`   -0.10125        NaN     NaN      NaN\n`1368175_at`   -0.96281        NaN     NaN      NaN\n`1368177_at`   -1.62396        NaN     NaN      NaN\n`1368216_at`    3.14747        NaN     NaN      NaN\n`1368225_at`   -0.50621        NaN     NaN      NaN\n`1368229_at`    3.34274        NaN     NaN      NaN\n`1368476_at`    0.69985        NaN     NaN      NaN\n`1368898_at`   -0.42802        NaN     NaN      NaN\n`1369018_at`   -0.63656        NaN     NaN      NaN\n`1369210_at`    1.12077        NaN     NaN      NaN\n`1369237_at`   -0.74250        NaN     NaN      NaN\n`1370205_at`    0.08702        NaN     NaN      NaN\n`1370354_at`    1.67294        NaN     NaN      NaN\n`1370429_at`   -0.03392        NaN     NaN      NaN\n`1370437_at`   -1.70450        NaN     NaN      NaN\n`1370439_a_at`  3.98168        NaN     NaN      NaN\n`1370551_a_at` -0.01096        NaN     NaN      NaN\n`1370555_at`    4.07083        NaN     NaN      NaN\n`1370827_at`    2.42265        NaN     NaN      NaN\n`1371072_at`    2.02812        NaN     NaN      NaN\n`1371133_a_at`  1.59933        NaN     NaN      NaN\n`1371610_at`    1.37769        NaN     NaN      NaN\n`1371614_at`    7.60068        NaN     NaN      NaN\n`1371661_at`   -1.53753        NaN     NaN      NaN\n`1371828_at`    0.99506        NaN     NaN      NaN\n`1371953_at`   -2.81015        NaN     NaN      NaN\n`1371963_at`    2.68767        NaN     NaN      NaN\n`1372022_at`    0.68078        NaN     NaN      NaN\n`1372046_at`   -2.10084        NaN     NaN      NaN\n`1372072_at`    0.85047        NaN     NaN      NaN\n`1372078_at`   -3.77567        NaN     NaN      NaN\n`1372081_at`    4.40179        NaN     NaN      NaN\n`1372104_at`   -2.33684        NaN     NaN      NaN\n`1372121_at`   -3.38501        NaN     NaN      NaN\n`1372143_at`   -0.23154        NaN     NaN      NaN\n`1372189_at`    2.46281        NaN     NaN      NaN\n`1372192_at`   -4.08633        NaN     NaN      NaN\n`1372202_at`    1.25190        NaN     NaN      NaN\n`1372248_at`    2.07958        NaN     NaN      NaN\n`1372266_at`    1.69119        NaN     NaN      NaN\n`1372329_at`    0.53905        NaN     NaN      NaN\n`1372445_at`   -4.53436        NaN     NaN      NaN\n`1372449_at`   -4.86333        NaN     NaN      NaN\n`1372453_at`    3.30659        NaN     NaN      NaN\n`1372463_at`   -4.53623        NaN     NaN      NaN\n`1372497_at`   -8.59019        NaN     NaN      NaN\n`1372538_at`    1.57537        NaN     NaN      NaN\n`1372544_at`   -1.08486        NaN     NaN      NaN\n`1372563_at`    0.16883        NaN     NaN      NaN\n`1372582_at`    6.01114        NaN     NaN      NaN\n`1372594_at`   -3.02689        NaN     NaN      NaN\n`1372633_at`    0.56998        NaN     NaN      NaN\n`1372674_at`   -2.07716        NaN     NaN      NaN\n`1372675_at`    3.87690        NaN     NaN      NaN\n`1372710_at`    1.13940        NaN     NaN      NaN\n`1372713_at`    3.95023        NaN     NaN      NaN\n`1372715_at`   -4.67805        NaN     NaN      NaN\n`1372733_at`    1.11764        NaN     NaN      NaN\n`1372768_at`    2.46896        NaN     NaN      NaN\n`1372816_at`    2.07744        NaN     NaN      NaN\n`1372817_at`    5.15885        NaN     NaN      NaN\n`1372830_at`    2.48010        NaN     NaN      NaN\n`1372867_at`   -1.38762        NaN     NaN      NaN\n`1372921_at`    5.76248        NaN     NaN      NaN\n`1372928_at`   -0.14972        NaN     NaN      NaN\n`1372953_at`   -4.46764        NaN     NaN      NaN\n`1373039_at`   -4.84993        NaN     NaN      NaN\n`1373083_at`   -0.33033        NaN     NaN      NaN\n`1373136_at`   -4.77394        NaN     NaN      NaN\n`1373163_at`    0.14398        NaN     NaN      NaN\n`1373165_at`   -1.33642        NaN     NaN      NaN\n`1373194_at`   -2.98023        NaN     NaN      NaN\n`1373200_at`    2.76554        NaN     NaN      NaN\n`1373272_at`   -0.09018        NaN     NaN      NaN\n`1373326_at`   -0.71469        NaN     NaN      NaN\n`1373327_at`    2.05675        NaN     NaN      NaN\n`1373333_at`   -1.84414        NaN     NaN      NaN\n`1373336_at`    3.76057        NaN     NaN      NaN\n`1373381_at`   -2.16455        NaN     NaN      NaN\n`1373398_at`    0.90460        NaN     NaN      NaN\n`1373431_at`   -2.24258        NaN     NaN      NaN\n`1373441_at`   -0.10277        NaN     NaN      NaN\n`1373450_at`    2.91785        NaN     NaN      NaN\n`1373472_at`   -0.57352        NaN     NaN      NaN\n`1373519_at`   -1.61598        NaN     NaN      NaN\n`1373534_at`   -4.05563        NaN     NaN      NaN\n`1373545_at`   -2.35288        NaN     NaN      NaN\n`1373569_at`   -1.33591        NaN     NaN      NaN\n`1373621_at`    3.04496        NaN     NaN      NaN\n`1373643_at`   -0.20441        NaN     NaN      NaN\n`1373677_at`    3.91286        NaN     NaN      NaN\n`1373683_at`    0.64733        NaN     NaN      NaN\n`1373707_at`    1.43368        NaN     NaN      NaN\n`1373771_at`   -0.33101        NaN     NaN      NaN\n`1373784_at`   -1.24218        NaN     NaN      NaN\n`1373813_at`    0.61754        NaN     NaN      NaN\n`1373835_at`   -1.31505        NaN     NaN      NaN\n`1373843_at`   -1.45044        NaN     NaN      NaN\n`1373859_at`   -0.85743        NaN     NaN      NaN\n`1373887_at`    2.74729        NaN     NaN      NaN\n`1373912_at`    0.66126        NaN     NaN      NaN\n`1373941_at`    1.52398        NaN     NaN      NaN\n`1373944_at`    0.40778        NaN     NaN      NaN\n`1374010_at`    2.04752        NaN     NaN      NaN\n`1374027_at`   -0.32516        NaN     NaN      NaN\n`1374074_at`   -2.22232        NaN     NaN      NaN\n`1374094_at`    0.33973        NaN     NaN      NaN\n`1374106_at`   -0.68325        NaN     NaN      NaN\n`1374150_at`    0.90613        NaN     NaN      NaN\n`1374151_at`    1.33677        NaN     NaN      NaN\n`1374187_at`    2.00123        NaN     NaN      NaN\n`1374233_at`   -4.76009        NaN     NaN      NaN\n`1374401_at`    0.02641        NaN     NaN      NaN\n`1374423_at`   -3.22747        NaN     NaN      NaN\n`1374519_at`   -2.31308        NaN     NaN      NaN\n`1374542_at`    0.76899        NaN     NaN      NaN\n`1374548_at`         NA         NA      NA       NA\n`1374574_at`         NA         NA      NA       NA\n`1374581_at`         NA         NA      NA       NA\n`1374628_at`         NA         NA      NA       NA\n`1374645_at`         NA         NA      NA       NA\n`1374647_at`         NA         NA      NA       NA\n`1374669_at`         NA         NA      NA       NA\n`1374766_at`         NA         NA      NA       NA\n`1374792_at`         NA         NA      NA       NA\n`1374809_at`         NA         NA      NA       NA\n`1374890_at`         NA         NA      NA       NA\n`1375035_at`         NA         NA      NA       NA\n`1375036_at`         NA         NA      NA       NA\n`1375056_at`         NA         NA      NA       NA\n`1375129_at`         NA         NA      NA       NA\n`1375389_at`         NA         NA      NA       NA\n`1375431_at`         NA         NA      NA       NA\n`1375510_x_at`       NA         NA      NA       NA\n`1375638_at`         NA         NA      NA       NA\n`1375743_at`         NA         NA      NA       NA\n`1375832_at`         NA         NA      NA       NA\n`1375833_at`         NA         NA      NA       NA\n`1375841_at`         NA         NA      NA       NA\n`1375894_at`         NA         NA      NA       NA\n`1375965_at`         NA         NA      NA       NA\n`1376004_at`         NA         NA      NA       NA\n`1376086_at`         NA         NA      NA       NA\n`1376121_at`         NA         NA      NA       NA\n`1376125_at`         NA         NA      NA       NA\n`1376159_at`         NA         NA      NA       NA\n`1376261_at`         NA         NA      NA       NA\n`1376306_at`         NA         NA      NA       NA\n`1376336_at`         NA         NA      NA       NA\n`1376374_at`         NA         NA      NA       NA\n`1376376_at`         NA         NA      NA       NA\n`1376551_at`         NA         NA      NA       NA\n`1376584_at`         NA         NA      NA       NA\n`1376587_at`         NA         NA      NA       NA\n`1376637_at`         NA         NA      NA       NA\n`1376641_at`         NA         NA      NA       NA\n`1376683_at`         NA         NA      NA       NA\n`1376686_at`         NA         NA      NA       NA\n`1376687_at`         NA         NA      NA       NA\n`1376714_at`         NA         NA      NA       NA\n`1376747_at`         NA         NA      NA       NA\n`1376773_at`         NA         NA      NA       NA\n`1376832_at`         NA         NA      NA       NA\n`1376842_at`         NA         NA      NA       NA\n`1376945_at`         NA         NA      NA       NA\n`1376984_at`         NA         NA      NA       NA\n`1377005_at`         NA         NA      NA       NA\n`1377043_at`         NA         NA      NA       NA\n`1377127_at`         NA         NA      NA       NA\n`1377128_at`         NA         NA      NA       NA\n`1377141_at`         NA         NA      NA       NA\n`1377157_at`         NA         NA      NA       NA\n`1377162_at`         NA         NA      NA       NA\n`1377194_a_at`       NA         NA      NA       NA\n`1377236_at`         NA         NA      NA       NA\n`1377254_a_at`       NA         NA      NA       NA\n`1377273_at`         NA         NA      NA       NA\n`1377281_at`         NA         NA      NA       NA\n`1377327_at`         NA         NA      NA       NA\n`1377469_at`         NA         NA      NA       NA\n`1377836_at`         NA         NA      NA       NA\n`1379451_at`         NA         NA      NA       NA\n`1379580_at`         NA         NA      NA       NA\n`1380466_at`         NA         NA      NA       NA\n`1382223_at`         NA         NA      NA       NA\n`1382345_at`         NA         NA      NA       NA\n`1383192_at`         NA         NA      NA       NA\n`1383383_at`         NA         NA      NA       NA\n`1383561_at`         NA         NA      NA       NA\n`1383649_a_at`       NA         NA      NA       NA\n`1384065_at`         NA         NA      NA       NA\n`1384584_at`         NA         NA      NA       NA\n`1385697_at`         NA         NA      NA       NA\n`1385826_at`         NA         NA      NA       NA\n`1386358_at`         NA         NA      NA       NA\n`1387142_at`         NA         NA      NA       NA\n`1387262_at`         NA         NA      NA       NA\n`1387501_at`         NA         NA      NA       NA\n`1387527_at`         NA         NA      NA       NA\n`1387781_at`         NA         NA      NA       NA\n`1387977_at`         NA         NA      NA       NA\n`1388087_at`         NA         NA      NA       NA\n`1388135_at`         NA         NA      NA       NA\n`1388383_at`         NA         NA      NA       NA\n`1388491_at`         NA         NA      NA       NA\n`1388656_at`         NA         NA      NA       NA\n`1388743_at`         NA         NA      NA       NA\n`1388796_at`         NA         NA      NA       NA\n`1388799_at`         NA         NA      NA       NA\n`1388830_at`         NA         NA      NA       NA\n`1388929_at`         NA         NA      NA       NA\n`1388996_at`         NA         NA      NA       NA\n`1388999_at`         NA         NA      NA       NA\n`1389049_at`         NA         NA      NA       NA\n`1389057_at`         NA         NA      NA       NA\n`1389082_at`         NA         NA      NA       NA\n`1389133_at`         NA         NA      NA       NA\n`1389153_at`         NA         NA      NA       NA\n`1389205_at`         NA         NA      NA       NA\n`1389225_at`         NA         NA      NA       NA\n`1389226_at`         NA         NA      NA       NA\n`1389229_at`         NA         NA      NA       NA\n`1389241_at`         NA         NA      NA       NA\n`1389254_at`         NA         NA      NA       NA\n`1389275_at`         NA         NA      NA       NA\n`1389329_at`         NA         NA      NA       NA\n`1389357_at`         NA         NA      NA       NA\n`1389388_at`         NA         NA      NA       NA\n`1389428_at`         NA         NA      NA       NA\n`1389457_at`         NA         NA      NA       NA\n`1389460_at`         NA         NA      NA       NA\n`1389539_at`         NA         NA      NA       NA\n`1389549_at`         NA         NA      NA       NA\n`1389584_at`         NA         NA      NA       NA\n`1389607_at`         NA         NA      NA       NA\n`1389730_at`         NA         NA      NA       NA\n`1389763_at`         NA         NA      NA       NA\n`1389781_at`         NA         NA      NA       NA\n`1389910_at`         NA         NA      NA       NA\n`1389915_at`         NA         NA      NA       NA\n`1389968_at`         NA         NA      NA       NA\n`1389978_at`         NA         NA      NA       NA\n`1389981_at`         NA         NA      NA       NA\n`1390142_at`         NA         NA      NA       NA\n`1390272_at`         NA         NA      NA       NA\n`1390301_at`         NA         NA      NA       NA\n`1390323_at`         NA         NA      NA       NA\n`1390394_at`         NA         NA      NA       NA\n`1390427_at`         NA         NA      NA       NA\n`1390439_at`         NA         NA      NA       NA\n`1390456_at`         NA         NA      NA       NA\n`1390539_at`         NA         NA      NA       NA\n`1390574_at`         NA         NA      NA       NA\n`1390599_at`         NA         NA      NA       NA\n`1390709_at`         NA         NA      NA       NA\n`1391096_at`         NA         NA      NA       NA\n`1391408_a_at`       NA         NA      NA       NA\n`1391412_at`         NA         NA      NA       NA\n`1391454_at`         NA         NA      NA       NA\n`1391484_at`         NA         NA      NA       NA\n`1392749_at`         NA         NA      NA       NA\n`1392958_at`         NA         NA      NA       NA\n`1393251_at`         NA         NA      NA       NA\n`1398308_at`         NA         NA      NA       NA\n`1398337_at`         NA         NA      NA       NA\n`1398340_at`         NA         NA      NA       NA\n`1398370_at`         NA         NA      NA       NA\n`1398384_at`         NA         NA      NA       NA\n`1398395_at`         NA         NA      NA       NA\n`1398396_at`         NA         NA      NA       NA\n`1398405_at`         NA         NA      NA       NA\n`1398409_at`         NA         NA      NA       NA\n`1398447_at`         NA         NA      NA       NA\n`1398448_at`         NA         NA      NA       NA\n`1398988_at`         NA         NA      NA       NA\n`1399061_at`         NA         NA      NA       NA\n`1399114_at`         NA         NA      NA       NA\n`1399134_at`         NA         NA      NA       NA\n`1399154_at`         NA         NA      NA       NA\n`1377569_at`         NA         NA      NA       NA\n`1377596_a_at`       NA         NA      NA       NA\n`1377637_at`         NA         NA      NA       NA\n`1377644_at`         NA         NA      NA       NA\n`1377681_at`         NA         NA      NA       NA\n`1377701_at`         NA         NA      NA       NA\n`1377745_at`         NA         NA      NA       NA\n`1377755_at`         NA         NA      NA       NA\n`1377757_at`         NA         NA      NA       NA\n`1377786_at`         NA         NA      NA       NA\n`1377791_at`         NA         NA      NA       NA\n`1377880_at`         NA         NA      NA       NA\n`1377883_at`         NA         NA      NA       NA\n`1377919_at`         NA         NA      NA       NA\n`1378022_at`         NA         NA      NA       NA\n`1378036_at`         NA         NA      NA       NA\n`1378125_at`         NA         NA      NA       NA\n`1378185_at`         NA         NA      NA       NA\n`1378316_at`         NA         NA      NA       NA\n`1378578_at`         NA         NA      NA       NA\n`1378590_at`         NA         NA      NA       NA\n`1378620_at`         NA         NA      NA       NA\n`1378680_at`         NA         NA      NA       NA\n`1378741_at`         NA         NA      NA       NA\n`1378819_at`         NA         NA      NA       NA\n`1378928_at`         NA         NA      NA       NA\n`1379029_at`         NA         NA      NA       NA\n`1379094_at`         NA         NA      NA       NA\n`1379121_at`         NA         NA      NA       NA\n`1379169_at`         NA         NA      NA       NA\n`1379187_at`         NA         NA      NA       NA\n`1379279_at`         NA         NA      NA       NA\n`1379283_at`         NA         NA      NA       NA\n`1379290_at`         NA         NA      NA       NA\n`1379476_at`         NA         NA      NA       NA\n`1379495_at`         NA         NA      NA       NA\n`1379600_at`         NA         NA      NA       NA\n`1379625_at`         NA         NA      NA       NA\n`1379693_at`         NA         NA      NA       NA\n`1379699_at`         NA         NA      NA       NA\n`1379728_at`         NA         NA      NA       NA\n`1379971_at`         NA         NA      NA       NA\n`1379982_at`         NA         NA      NA       NA\n`1380058_at`         NA         NA      NA       NA\n`1380174_at`         NA         NA      NA       NA\n`1380185_at`         NA         NA      NA       NA\n`1380214_at`         NA         NA      NA       NA\n`1380233_x_at`       NA         NA      NA       NA\n`1380402_at`         NA         NA      NA       NA\n`1380463_at`         NA         NA      NA       NA\n`1380477_at`         NA         NA      NA       NA\n`1380553_at`         NA         NA      NA       NA\n`1380794_at`         NA         NA      NA       NA\n`1380811_at`         NA         NA      NA       NA\n`1380815_at`         NA         NA      NA       NA\n`1380852_at`         NA         NA      NA       NA\n`1380890_at`         NA         NA      NA       NA\n`1380932_at`         NA         NA      NA       NA\n`1381077_at`         NA         NA      NA       NA\n`1381481_at`         NA         NA      NA       NA\n`1381755_x_at`       NA         NA      NA       NA\n`1381849_at`         NA         NA      NA       NA\n`1381902_at`         NA         NA      NA       NA\n`1381914_at`         NA         NA      NA       NA\n`1381926_at`         NA         NA      NA       NA\n`1381930_at`         NA         NA      NA       NA\n`1381963_at`         NA         NA      NA       NA\n`1381976_at`         NA         NA      NA       NA\n`1381998_at`         NA         NA      NA       NA\n`1382045_at`         NA         NA      NA       NA\n`1382065_at`         NA         NA      NA       NA\n`1382114_at`         NA         NA      NA       NA\n`1382132_at`         NA         NA      NA       NA\n`1382139_at`         NA         NA      NA       NA\n`1382154_at`         NA         NA      NA       NA\n`1382271_at`         NA         NA      NA       NA\n`1382290_at`         NA         NA      NA       NA\n`1382379_at`         NA         NA      NA       NA\n`1382517_at`         NA         NA      NA       NA\n`1382579_at`         NA         NA      NA       NA\n`1382651_at`         NA         NA      NA       NA\n`1382779_at`         NA         NA      NA       NA\n`1383081_at`         NA         NA      NA       NA\n`1383084_at`         NA         NA      NA       NA\n`1383110_at`         NA         NA      NA       NA\n`1383167_at`         NA         NA      NA       NA\n`1383172_at`         NA         NA      NA       NA\n`1383226_at`         NA         NA      NA       NA\n`1383236_at`         NA         NA      NA       NA\n`1383249_at`         NA         NA      NA       NA\n`1383254_at`         NA         NA      NA       NA\n`1383257_at`         NA         NA      NA       NA\n`1383299_at`         NA         NA      NA       NA\n`1383374_at`         NA         NA      NA       NA\n`1383431_at`         NA         NA      NA       NA\n`1383433_at`         NA         NA      NA       NA\n`1383504_at`         NA         NA      NA       NA\n`1383533_at`         NA         NA      NA       NA\n`1383607_at`         NA         NA      NA       NA\n`1383611_at`         NA         NA      NA       NA\n`1383673_at`         NA         NA      NA       NA\n`1383720_at`         NA         NA      NA       NA\n`1383731_at`         NA         NA      NA       NA\n`1383783_at`         NA         NA      NA       NA\n`1383998_at`         NA         NA      NA       NA\n`1384007_at`         NA         NA      NA       NA\n`1384035_at`         NA         NA      NA       NA\n`1384150_at`         NA         NA      NA       NA\n`1384205_at`         NA         NA      NA       NA\n`1384222_at`         NA         NA      NA       NA\n`1384247_at`         NA         NA      NA       NA\n`1384296_at`         NA         NA      NA       NA\n`1384320_at`         NA         NA      NA       NA\n`1384352_at`         NA         NA      NA       NA\n`1384368_at`         NA         NA      NA       NA\n`1384382_at`         NA         NA      NA       NA\n`1384387_at`         NA         NA      NA       NA\n`1384542_at`         NA         NA      NA       NA\n`1384565_at`         NA         NA      NA       NA\n`1384860_at`         NA         NA      NA       NA\n`1384940_at`         NA         NA      NA       NA\n`1385030_at`         NA         NA      NA       NA\n`1385106_at`         NA         NA      NA       NA\n`1385168_at`         NA         NA      NA       NA\n`1385261_s_at`       NA         NA      NA       NA\n`1385501_at`         NA         NA      NA       NA\n`1385559_at`         NA         NA      NA       NA\n`1385597_at`         NA         NA      NA       NA\n`1385776_at`         NA         NA      NA       NA\n`1385798_at`         NA         NA      NA       NA\n`1385883_at`         NA         NA      NA       NA\n`1386248_at`         NA         NA      NA       NA\n`1386683_at`         NA         NA      NA       NA\n`1390779_at`         NA         NA      NA       NA\n`1390799_at`         NA         NA      NA       NA\n`1390814_at`         NA         NA      NA       NA\n`1390836_at`         NA         NA      NA       NA\n`1390856_at`         NA         NA      NA       NA\n`1390864_at`         NA         NA      NA       NA\n`1390868_at`         NA         NA      NA       NA\n`1390874_at`         NA         NA      NA       NA\n`1390875_a_at`       NA         NA      NA       NA\n`1390945_at`         NA         NA      NA       NA\n`1391005_at`         NA         NA      NA       NA\n`1391013_at`         NA         NA      NA       NA\n`1391085_at`         NA         NA      NA       NA\n`1391092_at`         NA         NA      NA       NA\n`1391101_at`         NA         NA      NA       NA\n`1391155_at`         NA         NA      NA       NA\n`1391183_at`         NA         NA      NA       NA\n`1391212_at`         NA         NA      NA       NA\n`1391271_at`         NA         NA      NA       NA\n`1391308_at`         NA         NA      NA       NA\n`1391415_at`         NA         NA      NA       NA\n`1391443_at`         NA         NA      NA       NA\n`1391464_at`         NA         NA      NA       NA\n`1391475_at`         NA         NA      NA       NA\n`1391564_at`         NA         NA      NA       NA\n`1391727_at`         NA         NA      NA       NA\n`1391928_at`         NA         NA      NA       NA\n`1392199_at`         NA         NA      NA       NA\n`1392313_at`         NA         NA      NA       NA\n`1392470_at`         NA         NA      NA       NA\n`1392573_at`         NA         NA      NA       NA\n`1392599_at`         NA         NA      NA       NA\n`1392601_at`         NA         NA      NA       NA\n`1392610_at`         NA         NA      NA       NA\n`1392698_a_at`       NA         NA      NA       NA\n`1392772_at`         NA         NA      NA       NA\n`1392911_at`         NA         NA      NA       NA\n`1392918_at`         NA         NA      NA       NA\n`1392982_at`         NA         NA      NA       NA\n`1392997_at`         NA         NA      NA       NA\n`1393019_at`         NA         NA      NA       NA\n`1393027_at`         NA         NA      NA       NA\n`1393051_at`         NA         NA      NA       NA\n`1393149_at`         NA         NA      NA       NA\n`1393218_at`         NA         NA      NA       NA\n`1393258_at`         NA         NA      NA       NA\n`1393366_at`         NA         NA      NA       NA\n`1393510_at`         NA         NA      NA       NA\n`1393666_at`         NA         NA      NA       NA\n`1393736_at`         NA         NA      NA       NA\n`1393794_at`         NA         NA      NA       NA\n`1393803_at`         NA         NA      NA       NA\n`1393817_at`         NA         NA      NA       NA\n`1393977_at`         NA         NA      NA       NA\n`1394095_at`         NA         NA      NA       NA\n`1394192_at`         NA         NA      NA       NA\n`1394455_at`         NA         NA      NA       NA\n`1394505_at`         NA         NA      NA       NA\n`1394884_s_at`       NA         NA      NA       NA\n`1394897_at`         NA         NA      NA       NA\n`1395285_at`         NA         NA      NA       NA\n`1395455_at`         NA         NA      NA       NA\n`1395469_at`         NA         NA      NA       NA\n`1395479_at`         NA         NA      NA       NA\n`1395504_at`         NA         NA      NA       NA\n`1395647_at`         NA         NA      NA       NA\n`1395779_at`         NA         NA      NA       NA\n`1395791_at`         NA         NA      NA       NA\n`1396123_at`         NA         NA      NA       NA\n`1396476_at`         NA         NA      NA       NA\n`1396678_at`         NA         NA      NA       NA\n`1396853_at`         NA         NA      NA       NA\n`1396920_at`         NA         NA      NA       NA\n`1397227_at`         NA         NA      NA       NA\n`1397325_at`         NA         NA      NA       NA\n`1397363_at`         NA         NA      NA       NA\n`1397438_at`         NA         NA      NA       NA\n`1397554_at`         NA         NA      NA       NA\n`1397932_at`         NA         NA      NA       NA\n`1397946_at`         NA         NA      NA       NA\n`1397995_at`         NA         NA      NA       NA\n`1398601_at`         NA         NA      NA       NA\n`1398639_at`         NA         NA      NA       NA\n`1398663_at`         NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 120 and 0 DF,  p-value: NA\n\n\n\n\n\nsmall_subset = trim32 %&gt;%\n  select(y,`1397932_at`, `1394095_at`,`1393803_at`) %&gt;%\n  .[1:2,]\n\nsmall_subset\n\n# A tibble: 2 √ó 4\n      y `1397932_at` `1394095_at` `1393803_at`\n  &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1  8.38         4.80         5.84         6.49\n2  8.27         4.72         6.19         6.68\n\nsmall_subset %&gt;%\n  lm(y ~ 0 + `1397932_at` + `1394095_at` + `1393803_at`, data = .) %&gt;%\n  summary()\n\n\nCall:\nlm(formula = y ~ 0 + `1397932_at` + `1394095_at` + `1393803_at`, \n    data = .)\n\nResiduals:\nALL 2 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)\n`1397932_at`  1.70996        NaN     NaN      NaN\n`1394095_at`  0.03045        NaN     NaN      NaN\n`1393803_at`       NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 2 and 0 DF,  p-value: NA\n\n\n\\[\n\\begin{aligned}\ny_1 &= \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\\\\ny_2 &= \\beta_1 x_1 + \\beta2 x_2 + \\beta_3 x_3\n\\end{aligned}\n\\] - \\(p &gt; n\\) \\(\\implies\\) linearly dependent columns \\(\\implies\\) infinitely many solutions!\n\nPut another way, the rank of \\(X^TX\\) is a \\(p \\times p\\) matrix of rank \\(n &lt; p\\), which means we cannot invert it and find a unique least-squares solution.\n\n\n\n\nQuestion: What would be nice to do, to fix this, intuitively?\nQuestion: What would be nice to do, to fix this, mathematically?"
  },
  {
    "objectID": "notes/lec16.html#what-is-the-ridge-regression-solution",
    "href": "notes/lec16.html#what-is-the-ridge-regression-solution",
    "title": "Ridge regression",
    "section": "What is the ridge regression solution?",
    "text": "What is the ridge regression solution?\n\nExerciseSolution\n\n\nDerive the ridge-regression solution.\n\n\n\\[\n\\hat{\\beta}_{ridge} = (\\boldsymbol{X}^T\\boldsymbol{X}) + \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n\\]\n\n\n\n\nAlgebraically, we have ‚Äúfixed‚Äù the troublesome singular matrix \\((\\boldsymbol{X}^T \\boldsymbol{X})\\) by adding a positive constant \\(\\lambda\\) to its diagonal.\nWhat is \\(E[\\hat{\\beta}_{ridge}]\\)? Is it a biased or unbiased estimator of \\(\\beta\\)?\n\n\nComputing manually\n\nCodeOutput\n\n\n\n\n\n\n\n\n\n# get X matrix (no intercept)\nX = model.matrix(y ~ 0 + ., data = trim32)\nXs = scale(X)\n\nXTX = t(Xs) %*% Xs\ny = trim32$y %&gt;%\n  as.matrix(ncol = 1)\n\np = nrow(XTX)\nlambda = 1\n\nbetahat_ridge = solve(XTX + diag(lambda, nrow = p)) %*% t(Xs) %*% y \n\n\n\n\nbetahat_ridge %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(V1))) %&gt;%\n  head(n = 5)\n\n                      V1\n`1389910_at`  0.01344827\n`1374423_at` -0.01276357\n`1388491_at`  0.01199265\n`1372453_at`  0.01178389\n`1389241_at` -0.01169062\n\n\n\n\n\n\n\nUsing a package\nlm.ridge() within the MASS package\n\n\n\n\n\n\nWarning\n\n\n\nMASS has a function select() that will cause issues with dplyr‚Äôs select. For this reason it is preferable to not load both libraries simultaneously.\n\n\n\n\n\n\n\n\n\n\n\n# pass standardized data in: \nridge_fit = MASS::lm.ridge(y ~ 0 + ., data = data.frame(y, Xs),\n                           lambda = 1)\n\nbeta_scaled_lmridge = ridge_fit$coef # scaled space coefficients\nbeta_unscaled_lmridge = coef(ridge_fit) # original scale coefficients \n# (different than computed above if y and X are not standardized inputs)\n\nbeta_unscaled_lmridge[1:4]\n\nX.1367539_at. X.1367566_at. X.1368099_at. X.1368114_at. \n 0.0015121170 -0.0027627847 -0.0026806299  0.0005469267 \n\nbeta_scaled_lmridge[1:4]\n\nX.1367539_at. X.1367566_at. X.1368099_at. X.1368114_at. \n 0.0015058034 -0.0027512491 -0.0026694372  0.0005446431 \n\nbetahat_ridge[1:4,]\n\n`1367539_at` `1367566_at` `1368099_at` `1368114_at` \n 0.001511725 -0.002762699 -0.002679906  0.000546758 \n\n\n\n\n\n\n\n\nWarning\n\n\n\ncoef(ridge_fit) and ridge_fit$coef return unscaled and scaled coefficients respectively.\n\n\n\n\nUsing a different package\nAlternatively, could use glmnet() within the glmnet package. The function glmnet is not just for ridge regression but can perform other forms of penalized regression. Setting alpha = 0 maps the objective to a ridge regression problem.\n\n\n\n\n\n\nWarning\n\n\n\nThe argument \\(\\lambda\\) in glmnet() is different. Specifically, the objective function used in glmnet is\n\\[\n\\arg \\min_{\\beta} ~\\frac{1}{n} RSS + \\lambda \\beta^T\\beta\n\\]\nSo, \\(\\lambda \\times n\\) where \\(n\\) is the number of observations, nrow(X), will match the same objective function.\n\n\n\nX = scale(model.matrix(y ~ 0 + ., data = trim32))\n\nXTX = t(X) %*% X\ny = trim32$y %&gt;%\n  as.matrix(ncol = 1) %&gt;%\n  scale()\n\np = nrow(XTX)\nlambda = 100\n\nbetahat_ridge = solve(XTX + diag(lambda * nrow(X), nrow = p)) %*% t(X) %*% y \nbetahat_ridge[1:4,]\n\n`1367539_at` `1367566_at` `1368099_at` `1368114_at` \n 0.001605039 -0.002256027  0.001665459  0.001521466 \n\nfit = glmnet::glmnet(X, y, alpha = 0, # alpha = 0 defines ridge reg.\n                     lambda = lambda, \n                     intercept = FALSE) \nfit$beta[1:4,]\n\n`1367539_at` `1367566_at` `1368099_at` `1368114_at` \n 0.001637818 -0.002290652  0.001700664  0.001555549"
  },
  {
    "objectID": "notes/lec16.html#computing-manually",
    "href": "notes/lec16.html#computing-manually",
    "title": "MLEs and Model Comparison",
    "section": "Computing manually",
    "text": "Computing manually"
  },
  {
    "objectID": "notes/lec16.html#using-a-package",
    "href": "notes/lec16.html#using-a-package",
    "title": "MLEs and Model Comparison",
    "section": "Using a package",
    "text": "Using a package"
  },
  {
    "objectID": "notes/lec16.html#pseudo-inverse",
    "href": "notes/lec16.html#pseudo-inverse",
    "title": "Ridge regression",
    "section": "Pseudo-inverse",
    "text": "Pseudo-inverse"
  },
  {
    "objectID": "notes/lec16.html#ridge-plots",
    "href": "notes/lec16.html#ridge-plots",
    "title": "Ridge regression",
    "section": "Ridge plots",
    "text": "Ridge plots"
  },
  {
    "objectID": "notes/lec16.html#vocabulary-and-further-reading",
    "href": "notes/lec16.html#vocabulary-and-further-reading",
    "title": "Ridge regression",
    "section": "Vocabulary and further reading",
    "text": "Vocabulary and further reading\n\nBroadly ridge regression is a form of ‚Äúpenalized regression‚Äù and is often called ‚Äúregularization‚Äù. The other two most common forms of regularization are ‚Äúlasso regularization‚Äù and best subset selection. These correspond to an L-1 and L-0 penalty respectively.\nSince the expected value of the ridge regression estimator is not equal to \\(\\beta\\), it is sometimes referred to as a method of biased estimation. Often, we trade a small amount of bias for reduced variability in the estimator. This trade-off is called the bias-variance trade-off.\nThe seminal ridge regression paper: Hoerl and Kennard 1970 and a better format."
  },
  {
    "objectID": "notes/lec16.html#ridge-plots-how-to-choose-lambda",
    "href": "notes/lec16.html#ridge-plots-how-to-choose-lambda",
    "title": "Ridge regression",
    "section": "Ridge plots (how to choose \\(\\lambda\\)?)",
    "text": "Ridge plots (how to choose \\(\\lambda\\)?)\n\n## 1. Fit for a sequence of lambdas \nlambdas = seq(0, 5, length = 100)\nridge_fit = MASS::lm.ridge(y ~ 0 + ., data = trim32, lambda = lambdas)\n\n# 2. Plot ridge trace\nplot(ridge_fit, main = \"Ridge Trace Plot\")\n\n\n\n\n\n\n\nMASS::select(ridge_fit) # find lambda that minimizes GCV\n\nmodified HKB estimator is -1.460229e-27 \nmodified L-W estimator is -2.215401e-28 \nsmallest value of GCV  at 0.2525253 \n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized cross validation (GCV) is an approximation to leave 1 out cross-validation, but does not require re-computing the model \\(n\\) times for each \\(\\lambda\\).\n\n\nWhat is GCV and why does it work?\nIn ridge regression the fitted values can be written as\n\\[\n\\hat{y} = H_\\lambda y,\n\\]\nwhere \\(H_\\lambda\\) is the hat matrix depending on \\(\\lambda\\).\nIn order to compute the average leave-one-out cross-validation error for a given \\(\\lambda\\) requires computing the residual for each observation \\(i\\) when the model is refit without that point:\n\\[\n\\text{CV}(\\lambda) = \\frac{1}{n}\\sum_{i=1}^n\n\\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}(\\lambda)} \\right)^2,\n\\]\nwhere \\(h_{ii}\\) is the \\(i\\)th diagonal element of \\(H_\\lambda\\).\nGeneralized cross-validation (GCV) replaces the individual \\(h_{ii}\\) values with their average \\(\\mathrm{tr}(H_\\lambda)/n\\):\n\\[\n\\text{GCV}(\\lambda) =\n\\frac{\\tfrac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n{\\left(1 - \\tfrac{\\mathrm{tr}(H_\\lambda)}{n}\\right)^2}.\n\\]"
  },
  {
    "objectID": "hw/hw03.html",
    "href": "hw/hw03.html",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "The conceptual exercises are focused on explaining concepts and showing results mathematically. Show your work for each question.\nYou may write the answers and associated work for conceptual exercises by hand or type them in a Quarto document.\nIn all exercises below, you may assume \\(y \\sim N_n(X\\beta, \\sigma^2 I)\\)\n\n\n\n\nThe singular value decomposition of an \\(n \\times p\\) matrix \\(A\\), is a factorization of the form \\(A = U\\Sigma V^T\\) where\n\n\\(U\\) is an \\(n \\times n\\) orthogonal matrix,\n\\(\\Sigma\\) is an \\(n \\times p\\) diagonal matrix,\n\\(V\\) is a \\(p \\times p\\) orthogonal matrix.\n\nRecall that if a matrix \\(B\\) is orthogonal, \\(B^T B = I\\).\nFacts about SVD:\n\nthe diagonal entries of \\(\\Sigma\\), \\(\\sigma_{ii}\\) are called ‚Äúsingular values‚Äù. The number of non-zero singular values is equal to the rank of \\(A\\).\nthe SVD is not unique, however you can always choose a decomposition such that the singular values are ordered in descending order, i.e.¬†\\(\\sigma_{11} &gt; \\sigma_{22} &gt; \\ldots \\geq 0\\). In this case, \\(\\Sigma\\) is unique but \\(U\\) and \\(V\\) are not.\nat least one of the singular values is zero iff the matrix is non-invertible\nthe SVD can always be computed for any1 matrix \\(A\\).\n\n\n\n\n\nThe goal of this exercise is to re-write known quantities in terms of the SVD.\nSuppose we decompose \\(X\\) according to the singular value decomposition:\n\\[\nX = U\\Sigma V^T\n\\]\n\nWrite \\(\\hat{\\beta}_{OLS}\\) in terms of the SVD. Hint: to write \\((X^TX)^{-1}\\) implies \\(n \\geq p\\). Simplify as much as possible.\nCompute \\(Var[\\hat{\\beta}_{OLS}]\\) in terms of the SVD. Simplify as much as possible.\n\n\n\n\nRidge regression: show that even if \\(X^TX\\) is singular (not invertible), the matrix \\(A = (X^TX + \\lambda I)\\), where \\(\\lambda &gt; 0\\), is invertible using the singular value decomposition. Hint: \\(I = VV^T\\).\n\n\n\nRidge regression: show that when \\(X^TX\\) is of full rank, \\(Var(\\hat{\\beta}_{ridge}) \\leq Var(\\hat{\\beta}_{OLS})\\) using the SVD. Hint: it suffices to show that the matrix difference \\(Var(\\hat{\\beta}_{OLS}) - Var(\\hat{\\beta}_{ridge})\\) is positive semi-definite.\n\n\n\nExplain, in your own words, why it may be problematic to not standardize each predictor variable before performing ridge regression.\n\n\n\nExplain in words, math, or a combination of both why the row sums of the hat matrix formed from X1 (as defined below) are guaranteed to sum to 1 while the rows of the hat matrix formed by X2 are not. Additionally, validate this empirically using the matrices provided by the code below.\n\nset.seed(221)\nn = 10\nintercept = rep(1, n)\nx1 = runif(n)\nx2 = runif(n)\nX1 = cbind(intercept, x1, x2)\nX2 = cbind(x1, x2)"
  },
  {
    "objectID": "hw/hw03.html#exercise-1",
    "href": "hw/hw03.html#exercise-1",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "The goal of this exercise is to re-write known quantities in terms of the SVD.\nSuppose we decompose \\(X\\) according to the singular value decomposition:\n\\[\nX = U\\Sigma V^T\n\\]\n\nWrite \\(\\hat{\\beta}_{OLS}\\) in terms of the SVD. Hint: to write \\((X^TX)^{-1}\\) implies \\(n \\geq p\\). Simplify as much as possible.\nCompute \\(Var[\\hat{\\beta}_{OLS}]\\) in terms of the SVD. Simplify as much as possible."
  },
  {
    "objectID": "hw/hw03.html#footnotes",
    "href": "hw/hw03.html#footnotes",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfinite dimensional, real or complex‚Ü©Ô∏é"
  },
  {
    "objectID": "hw/hw03.html#preliminary-linear-algebra-background",
    "href": "hw/hw03.html#preliminary-linear-algebra-background",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "The singular value decomposition of an \\(n \\times p\\) matrix \\(A\\), is a factorization of the form \\(A = U\\Sigma V^T\\) where\n\n\\(U\\) is an \\(n \\times n\\) orthogonal matrix,\n\\(\\Sigma\\) is an \\(n \\times p\\) diagonal matrix,\n\\(V\\) is a \\(p \\times p\\) orthogonal matrix.\n\nRecall that if a matrix \\(B\\) is orthogonal, \\(B^T B = I\\).\nFacts about SVD:\n\nthe diagonal entries of \\(\\Sigma\\), \\(\\sigma_{ii}\\) are called ‚Äúsingular values‚Äù. The number of non-zero singular values is equal to the rank of \\(A\\).\nthe SVD is not unique, however you can always choose a decomposition such that the singular values are ordered in descending order, i.e.¬†\\(\\sigma_{11} &gt; \\sigma_{22} &gt; \\ldots \\geq 0\\). In this case, \\(\\Sigma\\) is unique but \\(U\\) and \\(V\\) are not.\nat least one of the singular values is zero iff the matrix is non-invertible\nthe SVD can always be computed for any1 matrix \\(A\\)."
  },
  {
    "objectID": "hw/hw03.html#exercise-2",
    "href": "hw/hw03.html#exercise-2",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "Ridge regression: show that even if \\(X^TX\\) is singular (not invertible), the matrix \\(A = (X^TX + \\lambda I)\\), where \\(\\lambda &gt; 0\\), is invertible using the singular value decomposition. Hint: \\(I = VV^T\\)."
  },
  {
    "objectID": "hw/hw03.html#exercise-3",
    "href": "hw/hw03.html#exercise-3",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "Ridge regression: show that when \\(X^TX\\) is of full rank, \\(Var(\\hat{\\beta}_{ridge}) \\leq Var(\\hat{\\beta}_{OLS})\\) using the SVD. Hint: it suffices to show that the matrix difference \\(Var(\\hat{\\beta}_{OLS}) - Var(\\hat{\\beta}_{ridge})\\) is positive semi-definite."
  },
  {
    "objectID": "hw/hw03.html#exercise-4",
    "href": "hw/hw03.html#exercise-4",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "Explain, in your own words, why it may be problematic to not standardize each predictor variable before performing ridge regression."
  },
  {
    "objectID": "hw/hw03.html#exercise-5",
    "href": "hw/hw03.html#exercise-5",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "",
    "text": "Explain in words, math, or a combination of both why the row sums of the hat matrix formed from X1 (as defined below) are guaranteed to sum to 1 while the rows of the hat matrix formed by X2 are not. Additionally, validate this empirically using the matrices provided by the code below.\n\nset.seed(221)\nn = 10\nintercept = rep(1, n)\nx1 = runif(n)\nx2 = runif(n)\nX1 = cbind(intercept, x1, x2)\nX2 = cbind(x1, x2)"
  },
  {
    "objectID": "hw/hw03.html#exercise-6",
    "href": "hw/hw03.html#exercise-6",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the hat matrix formed by X1 in exercise 5. Verify that the hat matrix \\(H = UU^T\\) as in exercise 1(c). Hint: use all.equal(H, U %*% t(U)) to avoid floating point rounding issues. Verify empirically that the columns of \\(U\\) are indeed orthonormal. The function svd() will compute the SVD in R. Use ?svd to read more."
  },
  {
    "objectID": "hw/hw03.html#data-2000-u.s.-presidential-election2",
    "href": "hw/hw03.html#data-2000-u.s.-presidential-election2",
    "title": "Homework 03",
    "section": "Data: 2000 U.S. Presidential Election3",
    "text": "Data: 2000 U.S. Presidential Election3\n\nWe will examine data about the 2000 U.S. presidential election between George W. Bush and Al Gore. It was one of the closest elections in history that ultimately came down to the state of Florida. One county in particular, Palm Beach County, was at the center of the controversy due to the design of their ballots - the infamous butterfly ballots. It is believed that many people who intended to vote for Al Gore accidentally voted for Pat Buchanan due to how the spots to mark the candidate were arranged next to the names.\nThe variables in the data are\n\nCounty: County name\nBush2000: Number of votes for George W. Bush\nBuchanan2000: Number of votes for Pat Buchanan\n\nThe data are available in the file florida-votes-2000.csv in the data folder of your repo."
  },
  {
    "objectID": "hw/hw03.html#exercise-7",
    "href": "hw/hw03.html#exercise-7",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Exercise 7",
    "text": "Exercise 7\nThe goal is to fit a model that uses the number of votes for Bush to predict the number of votes for Buchanan. Using this model, we‚Äôll investigate whether the data support the claim that votes for Gore may have accidentally gone to Buchanan.\n\nVisualize the relationship between the number of votes for Buchanan versus the number of votes for Bush. Describe what you observe in the visualization, including a description of the relationship between the votes for Buchanan and votes for Bush.\nWhat is the county with the extreme outlier number of votes for Buchanan? Create a new data frame that doesn‚Äôt include the outlying county. You will use this updated data frame for the remainder of this exercise and Exercise 8."
  },
  {
    "objectID": "hw/hw03.html#exercise-8",
    "href": "hw/hw03.html#exercise-8",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let‚Äôs consider potential models with transformations on the response and/or predictor variables. The four candidate models are the following:\n\n\n\nModel\nResponse variable\nPredictor variable\n\n\n\n\n1\nBuchanan2000\nBush2000\n\n\n2\nlog(Buchanan2000)\nBush2000\n\n\n3\nBuchanan2000\nlog(Bush2000)\n\n\n4\nlog(Buchanan2000)\nlog(Bush2000)\n\n\n\nWhich model best fits the data? Briefly explain, showing any work and output used to determine the response. (Note: Use the data set without the outlying county to find the candidate models.)"
  },
  {
    "objectID": "hw/hw03.html#exercise-9",
    "href": "hw/hw03.html#exercise-9",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Exercise 9",
    "text": "Exercise 9\nNow we will use the model to predict the expected number of Buchanan votes for the outlier county.\nSuppose the observed value of the predictor for this county (a new observation) is \\(x_0\\). We define \\(\\mathbf{x}_0^\\mathsf{T} = [1, x_0]\\)\nThen the predicted response is\n\\[\n\\hat{y}_0 = \\mathbf{x}_0^\\mathsf{T}\\hat{\\boldsymbol{\\beta}}\n\\]\nWhere \\(\\hat{\\boldsymbol{\\beta}}\\) is the vector of estimated model coefficients.\nJust as there is uncertainty in our model coefficients, there is uncertainty in our predictions as well. We use a confidence interval to quantify the uncertainty for a model coefficient, and we can use a prediction interval to quantify the uncertainty in the prediction for a new observation.\nThe \\(C\\%\\) prediction interval for the new observation is\n\\[\n\\hat{y}_0 \\pm t^*_{n - p - 1}\\sqrt{\\hat{\\sigma}^2_\\epsilon(1 + \\mathbf{x}_0^\\mathsf{T}(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{x}_0)}\n\\]\nwhere \\(t^*_{n-p-1}\\) is the critical value obtained from the \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom, \\(\\mathbf{X}\\) is the design matrix for the model, and \\(\\hat{\\sigma}^2_\\epsilon\\) is the estimated variability about the regression line.\n\nUse the model you chose in the previous exercise to compute the predicted number of votes for Buchanan in the outlying county identified in Exercise 7. If you selected a model with a transformation, be sure to report your answer in terms of votes, not log(votes).\nUse the formula above to ‚Äúmanually‚Äù compute the 95% prediction interval for this county (do not obtain the interval using the predict function) . If you selected a model with a transformation, be sure to report your answer in terms of votes, not log(votes).\nIt is assumed that some of the votes for Buchanan in that county were actually intended to be for Gore. Based on your results in the previous question, does your model support this claim?\n\nIf no, briefly explain.\nIf yes, about how many votes were possibly intended for Gore? Show any calculations and output used to determine your answer. If you selected a model with a transformation, be sure to report your answer in terms of votes, not log(votes)."
  },
  {
    "objectID": "hw/hw03.html#data-2000-u.s.-presidential-election",
    "href": "hw/hw03.html#data-2000-u.s.-presidential-election",
    "title": "Homework 03: Ridge regression and variable transformations",
    "section": "Data: 2000 U.S. Presidential Election",
    "text": "Data: 2000 U.S. Presidential Election\nWe will examine data about the 2000 U.S. presidential election between George W. Bush and Al Gore. It was one of the closest elections in history that ultimately came down to the state of Florida. One county in particular, Palm Beach County, was at the center of the controversy due to the design of their ballots - the infamous butterfly ballots. It is believed that many people who intended to vote for Al Gore accidentally voted for Pat Buchanan due to how the spots to mark the candidate were arranged next to the names.\nThe variables in the data are\n\nCounty: County name\nBush2000: Number of votes for George W. Bush\nBuchanan2000: Number of votes for Pat Buchanan\n\nThe data are available in the file florida-votes-2000.csv in the data folder of your repo."
  }
]