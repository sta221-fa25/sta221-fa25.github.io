---
title: "Introduction to inference"
author: "Dr. Alexander Fisher"
format: html
---

\newcommand{\by}{\boldsymbol{y}}
\newcommand{\hby}{\hat{\boldsymbol{y}}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\be}{\boldsymbol{\varepsilon}}
\newcommand{\hbe}{\hat{\be}}
\newcommand{\bzero}{\boldsymbol{0}}

```{r}
#| label: load-packages
#| code-fold: true
#| code-summary: "View libraries and data sets used in these notes"
#| warning: false
#| message: false
library(tidyverse)
library(tidymodels)
library(DT)
library(latex2exp)
library(patchwork)

pokemon <- read_csv("https://sta221-fa25.github.io/data/pokemon_data.csv") # complete, population data
```

## The data 

As of 2025, there are 1025 pokemon in existence.

In all statistical inference tasks, we only have a *sample* from the population. Let's consider a random sample of the pokemon data, given below:

```{r}
#| echo: false
#| eval: false
for(i in 1:1000) {
  set.seed(i)
  sample = pokemon %>% 
  slice_sample(n = 15) %>%
    filter(name == "Pikachu") %>%
    nrow()
  if(sample != 0) {
    print(i)
  }
}
```


```{r}
set.seed(48) 
pokemon_sample <- pokemon |>
  slice_sample(n = 15) |>
  select(dexnum, name, height_m, weight_kg) |>
  arrange(dexnum)
  

datatable(pokemon_sample, rownames = FALSE, options = list(pageLength = 5),
           caption = "sample of 15 pokemon")
```

Let's investigate the question: are heavier pokemon taller?

::: panel-tabset
## plot

```{r}
#| echo: false
#| warning: false
pokemon_sample |>
  ggplot(aes(x = weight_kg, y = height_m)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE, color = 'steelblue') +
  theme_bw() +
  labs(title = "Pokemon weight vs height", y = "height (m)", x = "weight (kg)")
```

## linear model

```{r}
lm(height_m ~ weight_kg, data = pokemon_sample) |>
  tidy()
```

## plot code 

```{r}
#| eval: false
pokemon_sample |>
  ggplot(aes(x = weight_kg, y = height_m)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE, color = 'steelblue') +
  theme_bw() +
  labs(title = "Pokemon weight vs height", y = "height (m)", x = "weight (kg)")
```

:::


## Question

How can we tell if our estimates $\hat{\beta}$ are any good?

:::panel-tabset

## repeated sampling

```{r}
pokemon_hw = pokemon |>
  select(height_m, weight_kg)

BETA_HAT <- NULL
for(i in 1:1000) {
  fit <- pokemon_hw |>
    slice_sample(n = 15) |> 
    lm(height_m ~ weight_kg, data = _) 
  BETA_HAT <- rbind(BETA_HAT, fit$coefficients)
}

BETA_HAT <- data.frame(BETA_HAT)
colnames(BETA_HAT) <- c("beta0", "beta1")

glimpse(BETA_HAT)
```

## plots

```{r}
#| warning: false
#| echo: false
set.seed(221)

p1 <- BETA_HAT |>
  ggplot(aes(x = beta0, y = beta1)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  xlim(c(0, 2)) +
  ylim(c(0, 0.025)) +
  theme_bw() +
  labs(title = "Joint distribution", x = TeX("$\\hat{\\beta}_0$"),
       y = TeX("$\\hat{\\beta}_1$"), fill = "density")

p2 <- BETA_HAT |> 
  ggplot(aes(x = beta0)) +
  geom_density(aes(y = ..density..)) +
  theme_bw() +
  labs(title = "Marginal densities", 
       x = TeX("$\\hat{\\beta}_0$"))

p3 <- BETA_HAT |> 
  ggplot(aes(x = beta1)) +
  geom_density(aes(y = ..density..)) +
  theme_bw() +
  labs(x = TeX("$\\hat{\\beta}_1$"))

(p1) / (p2 + p3) + plot_annotation(title = "Sampling variability in OLS estimates")
```
## population-level annotation

```{r}
popn_model <- lm(height_m ~ weight_kg, data = pokemon)
popn_model$coefficients
```


```{r}
#| warning: false
#| echo: false
set.seed(221)

p1 <- BETA_HAT |>
  ggplot(aes(x = beta0, y = beta1)) +
  stat_density_2d(aes(fill = ..level..), geom = "polygon") +
  xlim(c(0, 2)) +
  ylim(c(0, 0.025)) +
  theme_bw() +
  labs(title = "Joint distribution", x = TeX("$\\hat{\\beta}_0$"),
       y = TeX("$\\hat{\\beta}_1$"), fill = "density") +
  annotate(geom = "point", x = popn_model$coefficients[[1]],
           y = popn_model$coefficients[[2]], color = "darkgoldenrod2") + annotate(geom = "text", x = .65, y = 0.009, 
           label = TeX("$\\beta_0, \\beta_1$"), size = 5,
           color = "darkgoldenrod2")

p2 <- BETA_HAT |> 
  ggplot(aes(x = beta0)) +
  geom_density(aes(y = ..density..)) +
  theme_bw() +
  labs(title = "Marginal densities", 
       x = TeX("$\\hat{\\beta}_0$")) +
  geom_vline(xintercept = popn_model$coefficients[[1]],
             color = "darkgoldenrod2")

p3 <- BETA_HAT |> 
  ggplot(aes(x = beta1)) +
  geom_density(aes(y = ..density..)) +
  theme_bw() +
  labs(x = TeX("$\\hat{\\beta}_1$")) +
  geom_vline(xintercept = popn_model$coefficients[[2]], color = "darkgoldenrod2")

(p1) / (p2 + p3) + plot_annotation(title = "Sampling variability in OLS estimates")
```

:::

## Framework

Our **objective** is to infer properties about a *population* using data from an experiment or survey (in this case, a survey/sample of pokemon).

**Post-experiment**: after collecting the data, $\hat{\beta}_0, \hat{\beta}_1$ are fixed and known.

**Pre-experiment**: before collecting the data, the data are unknown and random. $\hat{\beta}_0, \hat{\beta}_1$, which are functions of the data, are also unknown and random. 

In all cases, the true population parameters, $\beta_0, \beta_1$ are *fixed* but *unknown*.

**Pre-experimental question**: is the probability distribution of $\hat{\beta}_0, \hat{\beta}_1$ a meaningful representation of the population?

**Answer**: this depends on certain assumptions we make about population.

::: callout-important
## Assumption 1

$E[\be|\bx] = \bzero$, or equivalently, $E[\varepsilon_i|\bx] = 0$ for all $i$.

:::

**Implications**: in simple linear regression, assumption 1 implies that $E[y_i|\bx] = E[\beta_0 + \beta_1 x_i + \epsilon_i|\bx] = E[\beta_0 |\bx] + E[\beta_1 x_i|\bx] + E[\epsilon_i|\bx] = \beta_0 + \beta_1 x_i$. 


::: panel-tabset
## Exercise
Show that assumption 1 implies that $E[\hat{\beta}|\bX] = \beta$.

## Solution

$$
\begin{aligned}
E[\hat{\beta}|\bX] &= E[(\bX^T\bX)^{-1}\bX^T\by | \bX]\\
&= (\bX^T\bX)^{-1}\bX^T E[\by | \bX]\\
&= (\bX^T\bX)^{-1}\bX^T \bX \beta\\
&= \beta
\end{aligned}
$$

:::

Since $E[\hat{\beta}|\bX] = \beta$, we say $\hat{\beta}$ is an **unbiased** estimator of $\beta$.

Notice that this result (unbiasedness) does **not** depend *independence* of errors, *normality*, or *constant variance*.

However, the result doesn't tell us anything about *how close* $\hat{\beta}$ will be to $\beta$. The estimator may be unbiased, but by itself, this doesn't tell us much. See examples of "unbiased" distributions of an estimator  below.

```{r}
#| echo: false

set.seed(221)

beta1 = 2
df = data.frame(x = -3:8)
lims = c(-3, 8)

p1 = df %>%
ggplot(aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 2, sd = 2)) +
  geom_vline(xintercept = beta1, color = "firebrick") +
  theme_bw() + 
  labs(x = TeX("$\\hat{\\beta}_1$"), y = TeX("$p(\\hat{\\beta}_1)$")) +
 scale_x_continuous(breaks = c(seq(floor(lims[1]), ceiling(lims[2]), by = 2), beta1),
                       labels = function(b) ifelse(b == beta1, expression(beta[1]), b))

p2 = df %>%
ggplot(aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 2, sd = .2)) +
  geom_vline(xintercept = beta1, color = "firebrick") +
  theme_bw() + 
  labs(x = TeX("$\\hat{\\beta}_1$"), y = TeX("$p(\\hat{\\beta}_1)$")) +
scale_x_continuous(breaks = c(seq(floor(lims[1]), ceiling(lims[2]), by = 2), beta1),
                       labels = function(b) ifelse(b == beta1, expression(beta[1]), b))

p3 = df %>%
ggplot(aes(x = x)) +
  stat_function(fun = dgamma, args = list(1.5, .75)) +
  geom_vline(xintercept = beta1, color = "firebrick") +
  theme_bw() + 
  labs(x = TeX("$\\hat{\\beta}_1$"), y = TeX("$p(\\hat{\\beta}_1)$")) +
scale_x_continuous(breaks = c(seq(floor(lims[1]), ceiling(lims[2]), by = 2), beta1),
                       labels = function(b) ifelse(b == beta1, expression(beta[1]), b))

p1 / p2 / p3 +
  plot_annotation(title = "Various densities with the same mean", tag_levels = c("A"))
```

