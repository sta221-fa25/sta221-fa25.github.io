---
title: "Multicollinearity"
author: "Dr. Alexander Fisher"
format: html
execute: 
  warning: false
  message: false
---

\newcommand{\by}{\boldsymbol{y}}
\newcommand{\hby}{\hat{\boldsymbol{y}}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\be}{\boldsymbol{\varepsilon}}
\newcommand{\hbe}{\hat{\be}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\identity}{\boldsymbol{I}}


```{r}
#| label: load-packages
#| code-fold: true
#| code-summary: "View libraries and data sets used in these notes"
#| echo: true
#| message: false

# load packages
library(tidyverse)  
library(tidymodels)  
library(knitr)       
library(patchwork)
library(GGally)   # for pairwise plot matrix
library(corrplot) # for correlation matrix

# load data
rail_trail <- read_csv("https://sta221-fa25.github.io/data/rail_trail.csv")

# set default theme in ggplot2
ggplot2::theme_set(ggplot2::theme_bw())
```


## Topics

-   Multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

## Data: Trail users {.midi}

-   The Pioneer Valley Planning Commission (PVPC) collected data at the beginning a trail in Florence, MA for ninety days from April 5, 2005 to November 15, 2005 to
-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.


Source: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.

## Variables

**Outcome**:

-   `volume` estimated number of trail users that day (number of breaks recorded)

**Predictors**

-   `hightemp` daily high temperature (in degrees Fahrenheit)

-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)

-   `season` one of “Fall”, “Spring”, or “Summer”

-   `precip` measure of precipitation (in inches)

## EDA: Relationship between predictors

We can create a pairwise plot matrix using the `ggpairs` function from the **GGally** R package

```{r}
#| eval: false
rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

## EDA: Relationship between predictors {.midi}

```{r}
#| echo: false
#| fig-align: center

rail_trail |>
  select(hightemp, avgtemp, season, precip) |>
  ggpairs()
```

## EDA: Correlation matrix

We can. use `corrplot()` in the **corrplot** R package to make a matrix of pairwise correlations between [quantitative]{.underline} predictors

```{r}
#| eval: false
correlations <- rail_trail |>
  select(hightemp, avgtemp, precip) |>
  cor()

corrplot(correlations, method = "number")
```

## EDA: Correlation matrix

```{r}
#| echo: false
#| fig-align: center

correlations <- rail_trail |>
  select(hightemp, avgtemp, precip) |>
  cor()


corrplot(correlations, method = "number")
```

::: question
What might be a potential concern with a model that uses high temperature, average temperature, season, and precipitation to predict volume?
:::

# Multicollinearity

## Multicollinearity

::: incremental
-   Ideally the predictors are orthogonal, meaning they are completely independent of one another

-   In practice, there is typically some dependence between predictors but it is often not a major issue in the model

-   If there is linear dependence among (a subset of) the predictors, we cannot find estimate $\hat{\boldsymbol{\beta}}$

-   If there are near-linear dependencies, we can find $\hat{\boldsymbol{\beta}}$ but there may be other issues with the model

-   **Multicollinearity**: near-linear dependence among predictors
:::

## Sources of multicollinearity

::: incremental
-   Data collection method - only sample from a subspace of the region of predictors

-   Constraints in the population - e.g., predictors family income and size of house

-   Choice of model - e.g., adding high order terms to the model

-   Overdefined model - have more predictors than observations
:::


## Detecting multicollinearity

::: panel-tabset
## Exercise

How could you detect multicollinearity?
:::

<!-- ::: incremental -->
<!-- -   Recall $Var(\hat{\boldsymbol{\beta}}) = \sigma^2_{\epsilon}(\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}$ -->
<!-- -   Let $\mathbf{C} = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}$. Then $Var(\hat{\beta}_j) = \sigma^2_{\epsilon}C_{jj}$ -->
<!-- -   When there are near-linear dependencies, $C_{jj}$ increases and thus $Var(\hat{\beta}_j)$ becomes inflated -->
<!-- -   $C_{jj}$ is associated with how much $Var(\hat{\beta}_j)$ is inflated due to $x_j$ dependencies with other predictors -->
<!-- ::: -->

## Variance inflation factor {.midi}

-   The **variance inflation factor (VIF)** measures how much the linear dependencies impact the variance of the predictors

$$
VIF_{j} = \frac{1}{1 - R^2_j}
$$

where $R^2_j$ is the proportion of variation in $x_j$ that is explained by a linear combination of all the other predictors

-   When the response and predictors are scaled in a particular way, $C_{jj} = VIF_{j}$. 

<!-- [Click here](variance-inflation-factors.html) to see how. -->

## Detecting multicollinearity

-   Common practice uses threshold $VIF > 10$ as indication of concerning multicollinearity (some say VIF \> 5 is worth investigation)

-   Variables with similar values of VIF are typically the ones correlated with each other

-   Use the `vif()` function in the **rms** R package to calculate VIF

```{r}
library(rms)

trail_fit <- lm(volume ~ hightemp + avgtemp + precip, data = rail_trail)

vif(trail_fit)
```

::: panel-tabset
## Exercise

Load the data using the code below:

```{r}
#| eval: false
rail_trail <- read_csv("https://sta221-fa25.github.io/data/rail_trail.csv")
```

and compute VIF manually (without using the function `vif()`).

:::

```{r}
#| echo: false
#| eval: false

X <- model.matrix(volume ~ hightemp + avgtemp + precip, data = rail_trail)
x1_model <- lm(hightemp ~ avgtemp + precip, data = rail_trail)
x2_model <- lm(avgtemp ~ hightemp + precip, data = rail_trail)
x3_model <- lm(precip ~ hightemp + avgtemp, data = rail_trail)

r21 <- glance(x1_model)$r.squared
r22 <- glance(x2_model)$r.squared
r23 <- glance(x2_model)$r.squared

1 / (1 - c(r21, r22, r23))

## manually:
# solve(t(X) %*% X)[2,2] * sum((X[,2] - mean(X[,2]))^2)




```


## Relationship with variance 

Recall that $Var(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$. If we call the matrix $(X^TX)^{-1}$ "C" then the variance of $\hat{\beta}_j$ is $\sigma^2 C_{jj}$. Specifically, 

$$
C_{jj} = VIF \times \frac{1}{\sum (X_{ij} - \bar{X_j})^2}
$$

## How multicollinearity impacts model

::: incremental
-   Large variance for the model coefficients that are collinear

    -   Different combinations of coefficient estimates produce equally good model fits

-   Unreliable statistical inference results

    -   May conclude coefficients are not statistically significant when there is, in fact, a relationship between the predictors and response

-   Interpretation of coefficient is no longer "holding all other variables constant", since this would be impossible for correlated predictors
:::

## Dealing with multicollinearity

::: incremental
-   Collect more data (often not feasible given practical constraints)

-   Redefine the correlated predictors to keep the information from predictors but eliminate collinearity

    -   e.g., if $x_1, x_2, x_3$ are correlated, use a new variable $(x_1 + x_2) / x_3$ in the model

-   For categorical predictors, avoid using levels with very few observations as the baseline

-   Remove one of the correlated variables

    -   Be careful about substantially reducing predictive power of the model
:::



```{r}
#| echo: false
#| eval: false
rail_trail_split <- initial_split(rail_trail, prop = 4/5)
rt_train <- training(rail_trail_split)
rt_test <- testing(rail_trail_split)

model1 <- lm(volume ~ hightemp + avgtemp + season + precip, data = rt_train)
residuals1 <- rt_test$volume - predict(model1, newdata = rt_test)
sum(residuals1^2)

model2 <- lm(volume ~ hightemp + season + precip, data = rt_train)
residuals2 <- rt_test$volume - predict(model2, newdata = rt_test)
sum(residuals2^2)

rt_train <- 
  rt_train %>%
  mutate(newTemp = hightemp - avgtemp)
model3 <- lm(volume ~ newTemp + season + precip, data = rt_train)
residuals3 <- rt_test$volume - predict(model2, newdata = rt_test)
sum(residuals3^2)
```


## Recap

-   Introduced multicollinearity

    -   Definition

    -   How it impacts the model

    -   How to detect it

    -   What to do about it

