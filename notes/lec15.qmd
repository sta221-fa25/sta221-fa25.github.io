---
title: "MLEs and Model Comparison"
author: "Dr. Alexander Fisher"
format: html
---

## Topics

- Properties of MLEs
- Model selection via information criteria (AIC, BIC)

\newcommand{\by}{\boldsymbol{y}}
\newcommand{\hby}{\hat{\boldsymbol{y}}}
\newcommand{\bone}{\boldsymbol{1}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\be}{\boldsymbol{\varepsilon}}
\newcommand{\hbe}{\hat{\be}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\identity}{\boldsymbol{I}}
\newcommand{\bmu}{\boldsymbol{\mu}}

## Recap

Linear regression statistical model: 

$$
\by | \bX \sim N_n(\bX \beta, \sigma^2 \identity)
$$
where $\by \in \mathbb{R}^n$ and $\bX \in \mathbb{R}^{n \times p}$. Here $N_n$ means "multivariate normal of dimension n".

Therefore, the likelihood (joint density of the data, viewed as a function of the parameters) is

$$
L(\beta, \sigma^2) = (2\pi \sigma^2)^{-n/2} \exp\{ -\frac{1}{2\sigma^2} (\by - \bX \beta)^T (\by - \bX \beta)\}
$$

::: panel-tabset
## Exercise

Where does the likelihood expression come from? Identify which component of the likelihood is the RSS.

## Solution 1

We can arrive at this likelihood using the multivariate normal density function. $\by \sim N_n(\bmu, \Sigma)$ means that its density can be written,

$$
f_Y(\by) = (2 \pi)^{-n/2} |\Sigma|^{-1/2} \exp \{ (\by - \bmu)^T \Sigma^{-1} (\by - \bmu)\}
$$

## Solution 2

Notice the diagonal covariance matrix in the multivariate normal representation. We could equivalently write the model as $y_i \sim N(x_i^T \beta, \sigma^2)$ and write the joint density as

$$
f(y_1, \ldots, y_n | x_1, \ldots, x_n) = \prod_{i=1}^n f_y(y_i|x_i)
$$
where $f(y_i | x_i) = (2 \pi \sigma^2)^{-1/2} \exp\{ - \frac{1}{2\sigma^2} (y_i - x_i^T \beta)^2\}$ and arrive at the same likelihood function of $\beta$ and $\sigma^2$.

:::

**Question**: Is a likelihood a density function of the parameters? 

**Answer**: NO!

### Maximum likelihood estimates 

Maximum likelihood estimates are obtained by calculus: differentiate the likelihood function and set equal to zero. 

Doing so, we find:

$$
\begin{aligned}
\hat{\beta}_{MLE} &= (\bX^T \bX)^{-1} \bX^T \by = \hat{\beta}_{OLS}\\
\hat{\sigma^2}_{MLE} &= \frac{RSS}{n}
\end{aligned}
$$

## Properties of maximum likelihood estimators

Maximum likelihood estimators have nice properties:

1. Asymptotic consistency
2. Asymptotic efficiency
3. Asymptotic normality

### Asymptotic consistency

::: callout-note
## Definition
Given a sequence of estimators, $\hat{\theta}_1, \hat{\theta}_2, \ldots$, we say $\hat{\theta}_n$ is a consistent estimator of $\theta$ if for all $c>0$, 

$$
\lim_{n \rightarrow \infty} p(|\hat{\theta}_n - \theta| \geq c) = 0
$$
:::

Meaning: as sample size $n \rightarrow \infty$, the estimator will be arbitrarily close to the parameter with high probability.

Equivalently, an estimator $\hat{\theta_n}$ is consistent if 

$$
\begin{aligned}
\lim_{n \rightarrow \infty} \text{var}(\hat{\theta}) &= 0\\
\lim_{n \rightarrow \infty} \text{bias}(\hat{\theta}) &= 0
\end{aligned}
$$
 
### Asymptotic efficiency

::: callout-note
## Definition: efficient

An estimator is **efficient** if it has the smallest variance among a class of estimators. 
:::

- Gauss Markov theorem showed $\hat{\beta}_{OLS}$ is the most efficient among all linear unbiased estimators (see homework 2).

- MLEs are asymptotically efficient.

### Asymptotically normal

As $n \rightarrow \infty$, maximum likelihood estimators are asymptotically normal. This means that the distribution of $\hat{\beta}_{MLE}$ is normal when $n$ is large *regardless* of the distribution of the underlying data.

## Model selection (by example)

### Data

Data on NBA player salaries and metrics for the 2022-2023 season. Data sourced from [kaggle](https://www.kaggle.com/datasets/jamiewelsh2/nba-player-salaries-2022-23-season/data) but originally scraped from "Hoopshype" and "Basketball Reference".

We'll look at a subset of the data below. 

```{r}
#| warning: false
#| message: false
library(tidyverse)
library(GGally)
library(corrplot)
nba_salaries <- read_csv("~/Downloads/nba_salaries_23.csv") %>%
  mutate(Salary = Salary / 1E6) %>% # salary in millions
  select(`Player Name`, Salary, Position, Age, GP, MP, GS, 
         FG, `FG%`, `2PA`, `2P%`, AST, `AST%`) %>%
  filter(Position %in% c("SG", "C", "SF", "PF", "PG" )) 
glimpse(nba_salaries)
```

### Codebook

- `Salary`: salary in millions of US dollars
- `Position`: player position
- `Age`: age of player
- `GP`: total number of games played in the season
- `MP`: average minutes played per game
- `GS`: number of games the player is put in at the start of the game
- 

#### EDA

Do salaries fluctuate by position?
```{r}
nba_salaries %>%
  ggplot(aes(x = Position, y = Salary)) +
  geom_boxplot()
```

Similar median, but different spread.

```{r}
correlations = nba_salaries %>%
  select(-c(1,3)) %>%
  cor()

corrplot(correlations, method = "number")
```


```{r}
nba_salaries_augmented = nba_salaries %>%
  mutate(isPGSG = (Position %in% c("PG-SG", "SG-PG")))
nba_salaries_augmented %>%
  count(Position)
```



```{r}
# nba_salaries %>%
#   select(-c(`Player Name`, Position)) %>%
#   ggpairs()
```


#### Fitting models 

```{r}
nba_salaries_mp = nba_salaries[,-1]
lm(Salary ~ ., data = nba_salaries_mp) %>%
  summary()
```

```{r}
nba_salaries_mp = nba_salaries_mp %>%
  mutate(isPG = Position == "PG")
lm(Salary ~ isPG + Age + GS  , data = nba_salaries_mp) %>%
  summary()
```


#### Comparing models





